\documentclass[11pt]{book}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\graphicspath{ {./images/} }
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{bbold}
\usepackage{CJKutf8}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{bbold}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\graphicspath{ {./images/} }
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan,}
\urlstyle{same}
\usepackage{fixltx2e}
\usepackage{mathrsfs}
\usepackage{pgfplots}
\usepackage{enumitem}
\usepackage{varwidth}
\usepackage{tasks}
\usepackage{amsthm}
\usepackage{float} % For H float placement specifier
\usepackage{placeins} % For the \FloatBarrier macro
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\usepackage[a4paper, margin= 3.5cm]{ geometry }
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{titling}
\usepackage{authblk}

\begin{document}



\begin{center}
	{\Huge \textsc{Numerical Methods of Partial Differential Equations}}\\
	\vspace*{1cm}
	{\large From lessons by Alfio Quarteroni}\\
	\vspace*{0.1cm}
	{\large {Luigi Pagani}}\\
	\vspace*{1cm}
	Politechnic University of Milan\\
	A.Y. 2023/2024
\end{center}




\vspace*{\fill}
\textbf{Disclaimer.} \\
This document contains the lecture notes for the course on numerical methods for partial differential equations, taught by Professor Alfio Quarteroni at the Polytechnic University of Milan during the 2023/24 academic year, with minor additions and modifications.
The intellectual property remains with the aforementioned professor, who has not reviewed this document. It is intended solely as a supplementary resource for the lectures, created by students for students, without any claims to replace official textbooks or attendance in the lectures. These notes are taken from the lectures and most of the illustrations are based on the professor's drawings and slides, to whom the intellectual property also belongs. \\

Revised on \today \\ 

The most up-to-date version of these notes is always available at this \href{https://www.overleaf.com/read/yqysrsbvccht#946a7e2}{\textcolor{blue}{link}}.

\vspace*{\fill}



\tableofcontents

\chapter{Elliptic Equations}
\section{Boundary value problems}
We are considering a second order differential equation of the form:
\begin{equation}
\begin{cases}
\mathcal{L} u=f & \text { in } \Omega \\
\text { Boundary Conditions (B.C.) } & \text { on } \partial \Omega
\end{cases}
\end{equation}
Where:
\begin{itemize}
\item $\Omega$ represents an open bounded domain in $\mathbb{R}^{d}$, with $d=2,3$.
\item $\partial \Omega$ is the boundary of $\Omega$.
\item $f$ is a given function.
\item The Boundary Conditions (B.C.) are to be prescribed according to $\mathcal{L}$.
\item $\mathcal{L}$ is a 2nd order differential operator.
\end{itemize}
Examples of $\mathcal{L}$ include:
\begin{itemize}
\item Non-conservative form: $\mathcal{L} u=-\operatorname{div}(\mu \nabla u)+\mathbf{b} \cdot \nabla u+\sigma u$.
\item Conservative form: $\mathcal{L} u=-\operatorname{div}(\mu \nabla u)+\operatorname{div}(\mathbf{b} u)+\sigma u$.
\end{itemize}
\subsection*{Example}

\begin{equation}
\begin{aligned}
& \begin{cases}\mathcal{L} u=-\operatorname{div}(\mu \nabla u)+\mathbf{b} \cdot \nabla u+\sigma u=f & \text { in } \Omega \\
u=0 & \text { on } \Gamma_{\mathrm{D}} \\
\mu \nabla u \cdot \mathbf{n}=g & \text { on } \Gamma_{\mathrm{N}}\end{cases} \\
& g \in L^{2}\left(\Gamma_{\mathrm{N}}\right), \quad \partial \Omega=\Gamma_{\mathrm{D}} \cup \Gamma_{\mathrm{N}}, \quad \Gamma_{\mathrm{D}} \cap \Gamma_{\mathrm{N}}=\emptyset
\end{aligned}
\end{equation}


\begin{center}
\includegraphics[scale = 0.3,max width=\textwidth]{2023_12_08_da72293789f2516e991ag-03}
\end{center}

$\Gamma_{N}$

\section{Weak formulation}
Weak form of Eq. (1.2):\\
Idea: $v=$ suitable test function $\rightarrow$ multiply (1.2) by $v:(\mathcal{L} u) v=f v$

$\rightarrow$ integrate in $\Omega \rightarrow$ apply integration by parts, product rule of divergence and divergence theorem to obtain:

$$
\begin{aligned}
& \underbrace{\int_{\Omega} \mu \nabla u \cdot \nabla v+\int_{\Omega} \mathbf{b} \cdot \nabla u v+\int_{\Omega} \sigma u v}_{=: a(u, v)} \\
& =\int_{\Omega} f v+\underbrace{\int_{\Gamma_{\mathrm{D}}} \mu \nabla u \cdot \mathbf{n} v}_{=0 \text { if }\left.v\right|_{\Gamma_{\mathrm{D}}}=0}+\int_{\Gamma_{\mathrm{N}}} \underbrace{\mu \nabla u \cdot \mathbf{n}}_{=g} v
\end{aligned}
$$

whence:

$$
\left\{\begin{array}{l}
\text { Find } u \in V=\left\{v \in H^{1}(\Omega),\left.v\right|_{\Gamma_{\mathrm{D}}}=0\right\}=: H_{\Gamma_{\mathrm{D}}}^{1}(\Omega) \\
a(u, v)=\langle F, v\rangle \quad \forall v \in V
\end{array}\right.
$$
\begin{itemize}
\item  a: $V \times V \rightarrow \mathbb{R}$ bilinear form.
  \item $F: V \rightarrow \mathbb{R}$ linear form s.t. $\langle F, v\rangle \equiv F(v)=\int_{\Omega} f v+\int_{\Gamma_{N}} g v$.
\end{itemize}

\section{Theorem (Lax-Milgram)}
Assume that:
\begin{enumerate}
\item $V$ is a Hilbert space with norm $\|\cdot\|$ and inner product $(\cdot, \cdot)$.
\item $F$ is bounded, i.e., $F \in V^{\prime}$ such that $|F(v)| \leq\|F\|_{V^{\prime}}\|v\| \quad \forall v \in V$.
\item $a$ is continuous, i.e., $\exists M>0:|a(u, v)| \leq M\|u\|\|v\| \quad \forall u, v \in V$.
\item $a$ is coercive, i.e., $\exists \alpha>0: a(v, v) \geq \alpha\|v\|^{2} \quad \forall v \in V$.
\end{enumerate}
Then, there exists a unique solution $u$ of (1.2).\\ \\

\textbf{Remark:} $V^{\prime}$ is the dual space of $V$, which consists of linear and bounded (i.e., continuous) maps from $V$ to $\mathbb{R}$ with norm:
$$
\|F\|_{V^{\prime}}=\sup _{v \in V \backslash\{0\}} \frac{|F(v)|}{\|v\|_{V}}
$$
Moreover, we have:
$$
\alpha\|u\|^{2} \leq a(u, u)=F(u) \leq\|F\|_{V^{\prime}}\|u\|
$$
Hence, we can conclude that:
$$
\|u\| \leq \frac{\|F\|_{V^{\prime}}}{\alpha}
$$
This leads to stability or continuous dependence on data. 

\section{Problem 1.3 - Generalization of problem 1.2}
To formulate Nečas Theorem, consider a slightly more general problem than (1.2):

\begin{equation}
\left\{\begin{array}{l}
\text { Find } u \in V \\
a(u, w)=\langle F, w\rangle \quad \forall w \in W
\end{array}\right.
\end{equation}


Note: $\langle F, w\rangle$ is a different (equivalent) notation for $F(w)$.
$a: V \times W \rightarrow \mathbb{R}$ bilinear form,
$F: W \rightarrow \mathbb{R}$ linear and continuous (that is $F \in W^{\prime}$ )

\section{Nečas Theorem}
Assume that $F \in W^{\prime}$. Consider the following conditions.\\
\begin{enumerate}[label=\roman*)]

\item  a continuous: $\exists M>0:|a(u, w)| \leq M\|u\|_V\|w\|_w \forall u \in V, w \in W$.\\
\item inf-sup condition: $\exists \alpha>0: \forall v \in V \sup _{w \in W \backslash\{0\}} \frac{a(v, w)}{\|w\|_w} \geq \alpha\|v\|_V$. \\
\item $\forall w \in W, w \neq 0, \exists v \in V: a(v, w) \neq 0$. \\ 
\end{enumerate}
These three conditions are necessary and sufficient for the existence and uniqueness of a solution of (1.3), for any $F \in W^{\prime}$.
Moreover (continuous dependence on data):
$$
\|u\|_V \leq \frac{1}{\alpha}\|F\|_{w^{\prime}}
$$

% The $2nd$ condition can be restated as: 
% $$\exists \alpha>0: \inf _{v \in V} \sup _{w \in W \backslash\{0\}} a(v, w) \geq \alpha\|v\| v\|w\| w.$$ \\

% Condition (1) is an asymmetric coercivity, while (iii) assures that, for every fixed \( v \in V \), \( a(v, \cdot) \) is positive at some point in \( W \). We have the following theorem, due to Nečas:

% If (i), (ii), (iii) hold, there exists a unique \( u \in W \) such that
% \[
% a(u, v)=F v \quad \forall v \in V .
% \]

% Moreover,
% \[
% \|u\|_W \leq \frac{1}{\alpha}\|F\|_{V^*} .
% \]


\section*{Remark}
If $W=V$, then the previous theorem provides necessary and sufficient conditions for the existence and uniqueness of solutions of the weak formulation of the Example problem(1.2). \\
Note that in this case:
\begin{itemize}
\item Condition i) is equivalent to continuity.
\item Condition ii) yields:
\begin{equation*}
\exists \alpha>0: \forall v \in V \sup _{w \in V \backslash\{0\}} \frac{a(v, w)}{\|w\|_{V}} \geq \alpha\|v\|_{V}
\end{equation*}
\item Condition iii) yields:
\begin{equation*}
\forall w \in V, w \neq 0, \exists v \in V: a(v, w) \neq 0
\end{equation*}
\end{itemize}

Conditions ii) and iii are more general and weaker conditions than coercivity. Indeed, by taking $w=v$, Condition iv (coercitivity) of Lax Milgram implies Equations ii) and iii) here.


\section{Approximation}
\subsection*{Galerkin method (for problem (1.2))}
Find $u_{h} \in V_{h}$ such that
\begin{equation*}
a\left(u_{h}, v_{h}\right)=F\left(v_{h}\right) \quad \forall v_{h} \in V_{h}
\end{equation*}

This is the Petrov-Galerkin method for problem (3). We want to find $u_{h} \in V_{h}$ such that
\begin{equation*}
a\left(u_{h}, w_{h}\right)=F\left(w_{h}\right) \quad \forall w_{h} \in W_{h}
\end{equation*}

where:
\begin{itemize}
\item $\left\{V_{h}, h>0\right\}$ are finite dimensional subspaces of $V$,
\item $\left\{W_{h}, h>0\right\}$ are finite dimensional subspaces of $W$,
\item $\operatorname{dim} V_{h}=\operatorname{dim} W_{h}=N_{h}<+\infty$.
\end{itemize}

\section*{Analysis of the Galerkin Problem}
\subsection*{Existence and Uniqueness}
This is a corollary of the Lax-Milgram Lemma, with $V_{h}$ being a closed subspace of $V$.

\subsection*{Stability}
We have a uniform bound with respect to $h$:
\begin{equation*}
\left\|u_{h}\right\| \leq \frac{\|F\|_{V^{\prime}}}{\alpha}
\end{equation*}

\subsection*{Consistency = Galerkin Orthogonality}
This is equivalent to Galerkin orthogonality. By subtracting $a\left(u_{h}, v_{h}\right)=F\left(v_{h}\right)$ from $a\left(u, v_{h}\right)=F\left(v_{h}\right)$, we obtain:
\begin{equation*}
a\left(u-u_{h}, v_{h}\right)=0 \quad \forall v_{h} \in V_{h}
\end{equation*}
The equation \( a(u - u_h, v_h) = 0 \) means that the error \( u - u_h \) is orthogonal to the subspace \( V_h \) with respect to the bilinear form \( a(\cdot, \cdot) \).

\subsection*{Convergence (Céa Lemma)}
We have:
\begin{align}
\alpha\left\|u-u_{h}\right\|^{2} & \leq a\left(u-u_{h}, u-u_{h}\right) \\
& =a\left(u-u_{h}, u-v_{h}\right)+\underbrace{a\left(u-u_{h}, v_{h}-u_{h}\right)}_{=0, \text{ since } (v_h - u_h) \in V_h} \\
& \leq M\left\|u-u_{h}\right\|\left\|u-v_{h}\right\| \quad \forall v_{h} \in V_{h}
\end{align}

Hence:
\begin{equation*}
\left\|u-u_{h}\right\| \leq \frac{M}{\alpha}\left\|u-v_{h}\right\| \quad \forall v_{h} \in V_{h}
\end{equation*}

Finally:
\begin{equation*}
\left\|u-u_{h}\right\| \leq \frac{M}{\alpha} \inf _{v_{h} \in V_{h}}\left\|u-v_{h}\right\|
\end{equation*}

\subsection*{Rate of Convergence}
We start with the assumption of space saturation:
\begin{equation*}
\forall v \in V \quad \lim _{h \rightarrow 0} \inf _{v_{h} \in V_{h}}\left\|v-v_{h}\right\|=0
\end{equation*}
Then, we have convergence:
\begin{equation*}
\lim _{h \rightarrow 0}\left\|u-u_{h}\right\|=0
\end{equation*}

We consider the rate of convergence (for example, in Finite Elements). Let $\mathcal{T}_{h}=\bigcup K$ be a triangulation of $\Omega$ (actually of $\Omega_{h}$). We define $v_{h}$ as follows:
\begin{equation*}
v_{h}=\left\{v_{h} \in \mathcal{C}^{0}(\bar{\Omega}):\left.v_{h}\right|_{K} \in \mathbb{P}^{r}(K) \ \ \forall K \in \mathcal{T}_{h},\left.v_{h}\right|_{\Gamma_{\mathrm{D}}}=0\right\}, \quad \text{for } r \geq 1
\end{equation*}
We have:
\begin{equation*}
\inf _{v_{h} \in V_{h}}\left\|u-v_{h}\right\| \leq\left\|u-\bar{u}_{h}\right\|
\end{equation*}
where $\bar{u}_{h}$ is a suitable choice. For example, $\bar{u}_{h}=\Pi_{h}^{r} u$ is the Finite Element interpolant. Then, we have:
\begin{equation*}
\left\|u-\bar{u}_{h}\right\| \leq C h^{r}|u|_{H^{r+1}(\Omega)}
\end{equation*}
provided $u \in V \cap H^{r+1}(\Omega)$. \\ \\ 
We define \( v_h \) as a function that is continuous over the closure of the domain \( \Omega \), denoted as \( \mathcal{C}^{0}(\bar{\Omega}) \). For each element \( K \) in the triangulation \( \mathcal{T}_h \), \( v_h \) takes the form of a polynomial of degree \( r \), expressed as \( v_h|_{K} \in \mathbb{P}^{r}(K) \) for all \( K \in \mathcal{T}_h \). Here, \( r \) is an integer greater than or equal to 1, where a higher \( r \) provides a more refined approximation. Additionally, \( v_h \) satisfies the Dirichlet boundary conditions, as indicated by \( v_h|_{\Gamma_{\mathrm{D}}} = 0 \).

Regarding error estimation, we consider the inequality \( \inf _{v_{h} \in V_{h}}\left\|u-v_{h}\right\| \leq\left\|u-\bar{u}_{h}\right\| \). This compares the error between the true solution \( u \) and any function in the finite element space \( V_h \) with the error between \( u \) and a particular approximation \( \bar{u}_h \), which is often chosen as the finite element interpolant \( \Pi_h^r u \). The selection of \( \bar{u}_h \) is critical for determining the error reduction rate.

Finally, the rate of convergence is described by \( \left\|u-\bar{u}_{h}\right\| \leq C h^{r}|u|_{H^{r+1}(\Omega)} \). This inequality shows that the error is proportional to \( h^r \), where \( h \) relates to the mesh size in the triangulation, and \( C \) is a constant. The condition \( u \in V \cap H^{r+1}(\Omega) \) is essential, as it requires \( u \) to have sufficient smoothness (belonging to the Sobolev space \( H^{r+1}(\Omega) \)) for this error estimate to be valid.
\section*{Examples of Polynomial Spaces}
\textbullet \quad For 1D $\left(\mathbb{P}^{r}\right)$, we have:
\begin{equation*}
p(x)=\sum_{k=0}^{r} a_{k} x^{k}
\end{equation*}
\textbullet \quad For 2D $\left(\mathbb{P}^{r}\right)$, we have:
\begin{equation*}
p\left(x_{1}, x_{2}\right)=\sum_{\substack{k=0, \ldots, r \\ m=0, \ldots, r \\ k+m \leq r}} a_{k m} x_{1}^{k} x_{2}^{m}
\end{equation*}
\textbullet \quad For 3D tetrahedra $\left(\mathbb{P}^{r}\right)$, we have:
\begin{equation*}
p\left(x_{1}, x_{2}, x_{3}\right)=\sum_{\begin{array}{c}
k=0, \ldots, r \\
m=0, \ldots, r \\
n=0, \ldots, r \\
k+m+n \leq r
\end{array}} a_{k m n} x_{1}^{k} x_{2}^{m} x_{3}^{n}
\end{equation*}
\textbullet \quad For 3D hexahedra $\left(\mathbb{Q}^{r}\right)$, we have:
\begin{equation*}
p\left(x_{1}, x_{2}, x_{3}\right)=\sum_{\substack{k=0, \ldots, r \\ m=0, \ldots, r \\ n=0, \ldots, r}} a_{k m n} x_{1}^{k} x_{2}^{m} x_{3}^{n}
\end{equation*}

\section*{Basis functions}
Having defined a basis $\left\{\phi_{j}(\mathbf{x})\right\}_{j=1}^{N_{h}}$ for the space $V_{h}$, each function $v_{h} \in V_{h}$ can be expanded as a linear combination of elements of the basis, suitably weighted by the coefficients $\left\{v_{j}\right\}_{j=1}^{N_{h}}$ :
$$
v_{h}(\mathbf{x})=\sum_{j=1}^{N_{h}} v_{j} \phi_{j}(\mathbf{x})
$$
We will use the notation $\mathbf{v}=\left(v_{1}, \ldots, v_{N_{h}}\right)^{T}$ to denote a vector $\mathbf{v} \in \mathbb{R}^{N_{h}}$ collecting all the basis coefficients (also called degrees of freedom). A basis is called lagrangian if it satisfies the following property:
$$
\phi_{i}\left(\mathbf{x}_{j}\right)=\delta_{i j} \quad \forall \ 1 \leq i, j \leq N_{h}
$$
for a suitable collection of points $\left\{\mathbf{x}_{j}\right\}_{j=1}^{N_{h}}$ called nodes. When the basis is lagrangian, the following property holds:
$$
v_{h}\left(\mathbf{x}_{j}\right)=v_{j} \quad \forall \ 1 \leq j \leq N_{h}
$$
We will now prove the conditions of Theorem 2 (Lax-Milgram).
\section{Lax-Milgram hypothesis for the weak formulation of problem 1.2}
\subsubsection*{Condition I}
We have a Hilbert space, as it is a closed subspace of the Hilbert space $H^{1}(\Omega)$. 
The scalar product is defined as $(u, v)=\int_{\Omega} u v+\int_{\Omega} \nabla u \cdot \nabla v$ and the norm is defined as $\|v\|=\left(\int_{\Omega} v^{2}+\int_{\Omega}|\nabla v|^{2}\right)^{1 / 2}$.
\subsubsection*{Condition II} \vspace{-0.4cm}
We now check the condition $F \in V^{\prime}$:
\begin{align*}
|\langle F, v\rangle| & =\left|\int_{\Omega} f v+\int_{\Gamma_{N}} g v\right| \\
& \leq\|f\|_{L^{2}(\Omega)}\|v\|_{L^{2}(\Omega)}+\|g\|_{L^{2}\left(\Gamma_{N}\right)}\|v\|_{L^{2}\left(\Gamma_{N}\right)} \quad \text { (Cauchy-Schwarz inequality) } \\
& \leq\|f\|_{L^{2}(\Omega)}\|v\|_{H^1}+\|g\|_{L^{2}\left(\Gamma_{N}\right)} C_{\text {trace }}\|v\|_{H^1} \quad \text { (definition of }\|\cdot\| \text { and trace ineq.) } \\
& =\left(\|f\|_{L^{2}(\Omega)}+C_{\text {trace }}\|g\|_{L^{2}\left(\Gamma_{N}\right)}\right)\|v\|_{H^1}
\end{align*}\vspace{-0.5cm}
Therefore, we have $\|F\|_{V^{\prime}}=\sup _{v \neq 0} \frac{|\langle F, v\rangle|}{\|v\|} \leq\|f\|_{L^{2}(\Omega)}+C_{\text {trace }}\|g\|_{L^{2}\left(\Gamma_{N}\right)}<+\infty$.
\subsubsection*{Condition III}
We have:
\begin{align*}
|a(u, v)| & \leq \|\mu\|_{L^{\infty}}\|\nabla u\|_{L^{2}}\|\nabla v\|_{L^{2}}+\|\mathbf{b}\|_{L^{\infty}}\|\nabla u\|_{L^{2}}\|v\|_{L^{2}} \\
& +\|\sigma\|_{L^{2}}\|u\|_{L^{4}}\|v\|_{L^{4}} \\
& \leq \underbrace{\left(\|\mu\|_{L^{\infty}}+\|\mathbf{b}\|_{L^{\infty}}+\|\sigma\|_{L^{2}}\right)}_{M}\|u\|\|v\| \quad \forall u, v \in V
\end{align*}
\textbf{Note}: By Sobolev embedding, we have $\|v\|_{L^{4}} \leq\|v\| \quad \forall v \in H^{1}$. \\ \\
\textbf{Estimation of the Term} $\|\sigma\|_{L^{2}}\|u\|_{L^{4}}\|v\|_{L^{4}}$:
\begin{itemize}
\item \textit{Hölder's Inequality}: Apply Hölder's inequality for $\sigma \in L^2(\Omega)$ and $uv \in L^2(\Omega)$, obtaining $\|\sigma uv\|_{L^1} \leq \|\sigma\|_{L^2} \|uv\|_{L^2}$.
\item \textit{Sobolev Embedding}: Use Sobolev embedding to assert $u, v \in H^1(\Omega) \Rightarrow u, v \in L^4(\Omega)$.
\item \textit{Algebraic Closure in $L^p$ Spaces}: Leverage the closure property of $L^p$ spaces under multiplication for $u, v \in L^4(\Omega)$, yielding $\|uv\|_{L^2} \leq \|u\|_{L^4}\|v\|_{L^4}$.
\item \textit{Final Estimation}: Combine the above to estimate $\|\sigma\|_{L^{2}}\|u\|_{L^{4}}\|v\|_{L^{4}}$ as part of the overall bound for $|a(u, v)|$.
\end{itemize}
\subsubsection*{Condition IV} 
This condition holds under the assumptions:
\[
\begin{aligned}
& \text{Given: } \sigma-\frac{1}{2} \operatorname{div} \mathbf{b} \geq 0 \text{ in } \Omega, \quad \mathbf{b} \cdot \mathbf{n} \geq 0 \text{ on } \Gamma_{\mathrm{N}}, \\
& a(v, v) = \int_{\Omega} \mu|\nabla v|^2 + \underbrace{\int_{\Omega} \mathbf{b} \cdot \frac{1}{2} \nabla(v^2)}_{\text{Target Expression}} + \int_{\Omega} \sigma v^2, \\
& \text{Target Expression}:  \int_{\Omega} \mathbf{b} \cdot \frac{1}{2} \nabla(v^2) =
\quad = \int_{\Omega} \mathbf{b} \cdot v\nabla v \quad \text{(product rule)} \\
& \text{Applying the Divergence Theorem:} \\
& \quad \mathbf{F} = \frac{1}{2} \mathbf{b}(v^2) \text{ and } \nabla \cdot \mathbf{F} = \frac{1}{2} \nabla \cdot (\mathbf{b}(v^2)) = \frac{1}{2}v^2 \nabla \cdot \mathbf{b} + \frac{1}{2} \mathbf{b} \cdot \nabla(v^2), \\
& a(v, v) = \int_{\Omega} \mu|\nabla v|^2 + \int_{\Omega} -\frac{1}{2} \nabla \cdot \mathbf{b}v^2 + \frac{1}{2} \int_{\partial \Omega} \mathbf{b} \cdot \mathbf{n} v^2 + \int_{\Omega} \sigma v^2 \\
& \quad = \int_{\Omega} \mu|\nabla v|^2 + \int_{\Omega}\left(\sigma-\frac{1}{2} \operatorname{div} \mathbf{b}\right) v^2 + \frac{1}{2} \int_{\Gamma_{\mathrm{N}}} \mathbf{b} \cdot \mathbf{n} v^2 \geq \mu_0\|\nabla v\|_{L^2}^2, \\
& \text{If } \mathbf{b} \text{ is constant, } \operatorname{div} \mathbf{b} = 0, \text{ then } \sigma \geq 0.
\end{aligned}
\]
\(\mu_0\) is a positive lower bound for the coefficient \(\mu\) throughout the domain \(\Omega\). This means that for all points in the domain, \(\mu(x) \geq \mu_0 > 0\). \\

Note: if $\mathbf{b}$ is constant, then $\operatorname{div} \mathbf{b}=0$, and the first term $\sigma-\frac{1}{2} \operatorname{div} \mathbf{b} \geq 0$ reduces to $\sigma \geq 0$.

\section{Poincaré Inequality}
The Poincaré inequality is a fundamental result in the theory of partial differential equations and the calculus of variations. It provides a relationship between the norm of a function and the norm of its derivatives.\\   

If $\Gamma_{D}$ is a set of positive measure (in 1D, it is sufficient that $\Gamma_{D}$ contains a single point), then there exists a constant $C_{\mathrm{P}}>0$ such that:
\begin{equation*}
\|v\|_{L^{2}(\Omega)} \leq C_{\mathrm{P}}\|\nabla v\|_{L^{2}(\Omega)} \quad \forall v \in H_{\Gamma_{\mathrm{D}}}^{1}(\Omega)
\end{equation*}

From this, we have:
\begin{equation*}
\|v\|_V^{2}=\|v\|_{L^{2}(\Omega)}^{2}+\|\nabla v\|_{L^{2}(\Omega)}^{2} \leq\left(1+C_{\mathrm{P}}^{2}\right)\|\nabla v\|_{L^{2}(\Omega)}^{2}
\end{equation*}

And hence:
\begin{equation*}
\|\nabla v\|_{L^{2}(\Omega)}^{2} \geq\left(1+C_{\mathrm{P}}^{2}\right)^{-1}\|v\|^{2}
\end{equation*}

In conclusion (coercivity), we have:
\begin{equation*}
a(v, v) \geq \frac{\mu_{0}}{1+C_{\mathrm{P}}^{2}}\|v\|^{2}.
\end{equation*}


\section{Stiffness Matrix}
As a reminder, if \(A\) is symmetric positive definite (spd), then the condition number \(K_{2}(A)\) is given by the ratio of the maximum to the minimum eigenvalues of \(A\):
\[
K_{2}(A)=\frac{\lambda_{\max }(A)}{\lambda_{\min }(A)}
\]
\textbf{Proposition}
If the bilinear form \(a(\cdot, \cdot)\) is symmetric and coercive, then the matrix \(A\) is symmetric positive definite. \\ \\
\textbf{Proof}
The symmetry of \(A\) is given by \(A_{i j}=a\left(\phi_{j}, \phi_{i}\right)=a\left(\phi_{i}, \phi_{j}\right)=A_{j i}\).\\ \\
For any vector \(\mathbf{v} \in \mathbb{R}^{N_{h}}\), we have: \\
\begin{align*}
\mathbf{v}^{T} A \mathbf{v} & =\sum_{i, j} A_{i j} v_{i} v_{j}=\sum_{i, j} a\left(\phi_{j}, \phi_{i}\right) v_{i} v_{j} \\
& =a\left(\sum_{j} v_{j} \phi_{j}, \sum_{i} v_{i} \phi_{i}\right)=a\left(v_{h}, v_{h}\right) \geq \alpha\left\|v_{h}\right\|^{2}>0
\end{align*}\\
Hence, \(A\) is positive definite. \\

\textbf{A-Norm}
If \(A\) is spd, we define the \(A\)-norm of \(\mathbf{v}\) as
\[
\|\mathbf{v}\|_{A} =\sqrt{(A \mathbf{v}, \mathbf{v})} =\sqrt{\sum_{i, j} a_{i j} v_{i} v_{j}}
\]
\textbf{Remark:}\\
In the case the bilinear form $a$ is symmetric, we have a stronger stability result. Since $a$ is symmetric, the finite element solution can be viewed as the element belonging to $V_h$ minimizing the distance to the exact solution $u$ in the energy norm. Thus:
$$\alpha \|u-u_h\|_V ^2\leq a(u-u_h,u-u_h) \leq a(u-v_h,u-v_h) \leq M \|u-v_h \|_V^2, \quad \forall v_h \in V_h $$
$$\implies \|u-u_h\|_V \leq \sqrt{\frac{M}{\alpha}} \inf_{v_h \in V_h} \|u-vh\|_V.$$

\subsection*{Conditioning of the Stiffness Matrix}
We can prove that there exist constants $C_{1}, C_{2}>0$ such that for all eigenvalues $\lambda_{h}$ of $A$:
$$
\alpha C_{1} h^{d} \leq \lambda_{h} \leq M C_{2} h^{d-2} \quad d=1,2,3
$$
From this, it follows that:
$$
\frac{\lambda_{\max }(A)}{\lambda_{\min }(A)} \leq \frac{M C_{2}}{\alpha C_{1}} h^{-2}
$$
This is indeed an asymptotic estimate (also a lower bound, with a different constant). Then:
$$
K_{2}(A)=\mathcal{O}\left(h^{-2}\right)
$$
Implication: If we use the conjugate gradient method to solve $A \mathbf{u}=\mathbf{f}$, then:
$$
\left\|\mathbf{u}^{(k)}-\mathbf{u}\right\|_{A} \leq 2\left(\frac{\sqrt{K_{2}(A)}-1}{\sqrt{K_{2}(A)}+1}\right)^{k}\left\|\mathbf{u}^{(0)}-\mathbf{u}\right\|_{A}
$$
The same holds for the gradient method, with $\sqrt{K_{2}(A)}$ replaced by $K_{2}(A)$.
This leads to the need for preconditioners.


\section{Interpolation Error Estimate}
\begin{enumerate}
\item \textbf{Estimate Formula}:
   \begin{equation}
   \left|v-\Pi_{h}^{r} v\right|_{H^{m}(\Omega)} \leq C\left(\sum_{K \in \mathcal{T}_{h}} h_{K}^{2(r+1-m)}|v|_{H^{r+1}(K)}^{2}\right)^{\frac{1}{2}}
   \end{equation}
   This formula provides an upper bound on the error between the actual function \( v \) and its interpolant. The error is measured in the \( H^m \) semi-norm. \\ \\

\item \textbf{Interpretation}:
   \begin{itemize} 
   \item The error is influenced by the size of each mesh element (\( h_K \)) and the degree of the polynomial (\( r \)) used for interpolation.
   \item The constant \( C \) is dependent on the polynomial degree \( r \), the semi-norm \( m \), and possibly other characteristics of the mesh (denoted as \( \hat{k} \)).
   \end{itemize}

\item \textbf{Simplified Estimate}:
   \begin{equation}
   \left|v-\Pi_{h}^{r} v\right|_{H^{m}(\Omega)} \leq C h^{r+1-m}|v|_{H^{r+1}(\Omega)}
   \end{equation}
   In this simplified version, the estimate assumes \( h_K \leq h \) for all elements. \\ \\

\item \textbf{Special Case, m = 0}\\ \\
   When \( m=0 \), \( H^{0}(\Omega) = L^{2}(\Omega) \), and the norm becomes the standard \( L^2 \) norm, which is the square root of the integral of the square of the function over the domain.
\end{enumerate}

\section{Finite Element Error Estimate}
Recall that:
\begin{equation}
\left\|u-u_{h}\right\| = \left\|u-u_{h}\right\|_{H^{1}(\Omega)} \\
\leq \frac{M}{\alpha} \inf_{v_{h} \in V_{h}} \left\|u-v_{h}\right\|_{H^{1}(\Omega)} \\
\leq \frac{M}{\alpha} \left\|u-\Pi_{h}^{r} u\right\|_{H^{1}(\Omega)}
\end{equation}
Using (1.7):
\begin{equation*}
\left\|u-u_{h}\right\| \leq C \frac{M}{\alpha}\left(\sum_{K \in \mathcal{T}_{h}} h_{K}^{2 r}|u|_{H^{r+1}(\Omega)}^{2}\right)^{1 / 2}
\end{equation*}
Using (1.8):
\begin{equation*}
\left\|u-u_{h}\right\| \leq C \frac{M}{\alpha} h^{r}|u|_{H^{r+1}(\Omega)}
\end{equation*}

\section{A more general FE error estimate}
Suppose $u \in H^{p+1}(\Omega)$, with $p \geq 0$.\\ 
Convergence rate for $\left\|u-u_{h}\right\|_{v}$ :\\
$$
\begin{array}{l|lllll} 
& p=0 & p=1 & p=2 & p=3 & p>3 \\
\hline r=1 & \text { conv. } & \mathcal{O}(h) & \mathcal{O}(h) & \mathcal{O}(h) & \mathcal{O}(h) \\
r=2 & \text { conv. } & \mathcal{O}(h) & \mathcal{O}\left(h^{2}\right) & \mathcal{O}\left(h^{2}\right) & \mathcal{O}\left(h^{2}\right) \\
r=3 & \text { conv. } & \mathcal{O}(h) & \mathcal{O}\left(h^{2}\right) & \mathcal{O}(h)^{3} & \mathcal{O}\left(h^{3}\right)
\end{array}
$$

\begin{itemize}
  \item Optimal rate of convergence for $p=r$.
  \item Suboptimal rate if $p<r$.
  \item Waste of computational effort if $p>r$ (extra regularity not rewarding in terms of order of accuracy).
\end{itemize}

\section*{Remark}
For $\left\|u-u_{h}\right\|_{L^{2}}$, the convergence rates from the table should be increased by one order. This is due to the less stringent nature of the $L^2$ norm compared to other norms, such as the $H^1$ norm. The $L^2$ norm measures the error in an average sense over the domain and does not consider the derivatives of the function, making it more forgiving and resulting in higher apparent convergence rates.


\chapter{Parabolic equations}

\section*{Parabolic Equations}
We consider parabolic equations of the form
\begin{equation}
\frac{\partial u}{\partial t}+L u=f, \quad \mathbf{x} \in \Omega, t>0
\end{equation}
where:
\begin{itemize}
\item $\Omega$ is a domain of $\mathbb{R}^{d}, \  d=1,2,3$,
\item $f=f(\mathbf{x}, t)$ is a given function,
\item $L=L(\mathbf{x})$ is a generic elliptic operator acting on the unknown $u=u(\mathbf{x}, t)$.
\end{itemize}
When solved only for a bounded temporal interval, say for $0<t<T$, the region $Q_{T}=\Omega \times(0, T)$ is called cylinder in the space $\mathbb{R}^{d} \times \mathbb{R}^{+}$.
\begin{figure}[h]
\centering
\includegraphics[max width=\textwidth]{a.png}
\caption{The cylinder $Q_{T}=\Omega \times(0, T), \Omega \subset \mathbb{R}^{2}$}
\end{figure} \\
In the case where $T=+\infty, Q=\{(\mathbf{x}, t): \mathbf{x} \in \Omega, t>0\}$ will be an infinite cylinder.
Equation (2.1) must be completed by assigning an initial condition
\begin{equation}
u(\mathbf{x}, 0)=u_{0}(\mathbf{x}), \quad \mathbf{x} \in \Omega
\end{equation}
together with boundary conditions, which can take the following form:
\begin{equation}
\left\{
\begin{aligned}
& u(\mathbf{x}, t)=\varphi(\mathbf{x}, t), \quad \mathbf{x} \in \Gamma_{D} \text { and } t>0 \\
& \frac{\partial u(\mathbf{x}, t)}{\partial n}=\psi(\mathbf{x}, t), \quad \mathbf{x} \in \Gamma_{N} \text { and } t>0 \label{eq:second}
\end{aligned}
\right.
\end{equation}
where $u_{0}, \varphi$ and $\psi$ are given functions and $\left\{\Gamma_{D}, \Gamma_{N}\right\}$ provides a boundary partition, that is $\Gamma_{D} \cup \Gamma_{N}=\partial \Omega$, $\stackrel{\circ}{\Gamma}_{D} \cap \stackrel{\circ}{\Gamma}_{N}=\emptyset$. For obvious reasons, $\Gamma_{D}$ is called Dirichlet boundary and $\Gamma_{N}$ Neumann boundary.
In the one-dimensional case, the problem:
\begin{equation}
\left\{
\begin{aligned}
& \frac{\partial u}{\partial t}-\nu \frac{\partial^{2} u}{\partial x^{2}}=f, \quad 0<x<d, \quad t>0 \\
& u(x, 0)=u_{0}(x), \quad 0<x<d \label{eq:central} \\
& u(0, t)=u(d, t)=0, \quad t>0
\end{aligned}
\right.
\end{equation}
describes the evolution of the temperature $u(x, t)$ at point $x$ and time $t$ of a metal bar of length $d$ occupying the interval $[0, d]$, whose thermal conductivity is $\nu$ and whose endpoints are kept at a constant temperature of zero degrees.

The function $u_{0}$ describes the initial temperature, while $f$ represents the heat generated (per unit length) by the bar.

For this reason, (2.4) is called heat equation.

\section*{Weak Formulation and Its Approximation}
We proceed formally, by multiplying for each $t>0$ the differential equation by a test function $v=v(\mathbf{x})$ and integrating on $\Omega$. We set $V=\mathrm{H}_{\Gamma_{D}}^{1}(\Omega)$ and for each $t>0$ we seek $u(t) \in V$ such that
\begin{equation}
\int_{\Omega} \frac{\partial u(t)}{\partial t} v d \Omega+a(u(t), v)=\int_{\Omega} f(t) v d \Omega \quad \forall v \in V
\end{equation}
where
\begin{itemize}
\item $u(0) =u_{0}$;
\item $a(\cdot, \cdot)$ is the bilinear form associated to the elliptic operator $L$;
\item we have supposed for simplicity $\varphi=0$ and $\psi=0$.
\end{itemize}


\section{Bilinear Forms}
\textbf{Weak Coercitivity}
A bilinear form $a(\cdot, \cdot)$ is said weakly coercive if
$$
\exists \lambda \geq 0, \exists \alpha>0: \quad a(v, v)+\lambda\|v\|_{L^2(\Omega)}^2 \geq \alpha\|v\|_v^2 \quad \forall v \in v,
$$
yielding for $\lambda=0$ the standard definition of coercivity. 
\subsubsection{Rationale for weak coercitivity}
Before coming to the existence and uniqueness theorem, let us notice that, if we introduce the change of variable $u_\lambda(t, \mathbf{x}):=e^{-\lambda t} u(t, \mathbf{x})$, where $u$ is the solution of (2.4), the new unknown $u_\lambda$ satisfies
$$
\frac{\partial u_\lambda}{\partial t}+L u_\lambda+\lambda u_\lambda=e^{-\lambda t} f \quad \text { in } Q_T .
$$

If a bilinear form $a(w, v)$, the bilinear form $a_\lambda(w, v):=a(w, v)+\lambda(w, v)$ associated to this last problem is coercive, i.e., it satisfies (11.1.6) with $\lambda=0$. Therefore, if we replace $f$ with $e^{-\lambda t} f$ and $L$ with $L+\lambda I, I$ being the identity operator, without loosing generality we can assume that the bilinear form associated to the initial-boundary value problem (2.4) satisfies the weak coercitivi with $\lambda=0$.

This will be always assumed in the sequel of this Chapter. However, it is worthy to notice that the estimates we will prove are valid for the auxiliary unknown $u_\lambda(t, \mathbf{x})$ (or its approximations), and that the corresponding estimates for the solution $u(t, \mathbf{x})$ show an extra multiplicative factor $e^{\lambda t}$.

Let us now prove the existence theorem. We notice that hereafter all norms refer to the space variables, i.e., $\|\cdot\|_k$ is the norm in the Sobolev space $H^k(\Omega)$ for $k \geq 0$.



\section{Condition on the well-posedness of problem 2.5}
Consider the following parabolic partial differential equation (PDE) which incorporates time-dependency:

\[
\begin{cases}
\partial_t u -\nabla \cdot (\mu \nabla u) + \mathbf{b} \cdot \nabla u + \nabla \cdot (\mathbf{c}u)+ \sigma u = f & \text{in } \Omega \times (0,T], \\
u = g_D & \text{on } \Gamma_D \times (0,T], \\
(\mu \nabla u - \mathbf{b}u) \cdot \mathbf{n} + \gamma u = g_N & \text{on } \Gamma_N \times (0,T], \\
u(\cdot,0) = u_0 & \text{in } \Omega.
\end{cases}
\]

Where \( u_0 \) is the initial condition and \( T \) is the final time.

To ensure the well-posedness of the problem, we assume the following regularity and compatibility conditions:
\begin{itemize}
    \item \( \mu \in \mathrm{L}^{\infty}(\Omega \times (0, T)) \) with \( \mu(\mathbf{x}, t) \geq \mu_0 > 0 \).
    \item \( \sigma \in \mathrm{L}^{\infty}(\Omega \times (0, T)) \).
    \item \( \mathbf{b}, \mathbf{c} \in [\mathrm{L}^{\infty}(\Omega \times (0, T))]^n \).
    \item \( \operatorname{div}(\mathbf{b} - \mathbf{c}) \in \mathrm{L}^{\infty}(\Omega \times (0, T)) \).
    \item \( f \in \mathrm{L}^2(\Omega \times (0, T)) \).
    \item \( g_D \in \mathrm{H}^{1/2}(\partial \Omega \times (0, T)) \).
    \item \( g_N \in \mathrm{L}^2(\partial \Omega \times (0, T)) \).
    \item \( \gamma \in \mathrm{L}^{\infty}(\partial \Omega) \) is a constant.
\end{itemize}

\begin{enumerate}


\item \textbf{Weak Form:} The weak form for the parabolic problem can be written as:
   \[
   \int_{\Omega} \partial_t \tilde{u} v \, d\Omega + a(\tilde{u}, v) = F(v) \quad \forall v \in V, \text{ for a.e. } t \in (0,T],
   \]
   where \( a(\tilde{u}, v) \) is the bilinear form from the elliptic case and \( F(v) \) is adjusted similarly:
   \[
   F(v) = \int_{\Omega} fv \, d\Omega + \int_{\Gamma_N} g_N v \, d\Gamma - a(R_{g_D},v)- \int_{\Omega} \partial_t R_{g_D} v \, d\Omega  .
   \]
we consider the evolution of \( u \) over time. For the parabolic problem, we include a time derivative term and seek \( u \in L^2(0, T; V) \) such that \( \frac{\partial u}{\partial t} \in L^2(0, T; V') \) and the following equation holds for almost every \( t \in (0, T) \) and for all \( v \in V \):
\[
\left(\frac{\partial \tilde{u}}{\partial t}, v\right) \, d\Omega + a(\tilde{u}(t), v) = f(v) - \left(\frac{\partial \tilde{R}_{gD}}{\partial t}, v\right), \quad \forall t \in (0,T)
\]
where \( f(v) \) represents external forces or source terms. 
The last term, is an integration and since we assumed that $g_D \in H^{1/2}(\Gamma_D \times (0,T) \implies R_{gD}\in H^1(\Omega \times (0,T))\implies $ the last term is well defined. The initial condition is given by:
\[
u(x, 0) = u_0(x) \quad \text{for } x \in \Omega.
\]
\end{enumerate}
Consider the bilinear form $a(\cdot, \cdot)$ originating from the previous boundary value problem, given the right conditions such that it is both continuous and weakly coercive. Additionally, we have the following requirements: $u_{0} \in \mathrm{L}^{2}(\Omega)$ and $f \in \mathrm{L}^{2}(Q_T)$. Under these conditions, problem (2.5) has a unique solution $u$ with the following properties:
\begin{itemize}
  \item $u$ belongs to $C^{0}\left(\mathbb{R}^{+} ; \mathrm{L}^{2}(\Omega)\right)$.
  \item $u$ is also an element of $\mathrm{L}^{2}\left(\mathbb{R}^{+} ; V\right)$.
  \item The partial derivative of $u$ with respect to time, $\frac{\partial u}{\partial t}$, is in $L^{2}\left(\mathbb{R}^{+} ; V^{\prime}\right)$, which can be denoted as $u \in H^{1}\left(\mathbb{R}^{+} ; V, V^{\prime}\right)$, following the notation of Prof. Salsa.
\end{itemize}

Now to prove weakly coercitivity of the bilinear forms we have two possible approaches:
\begin{enumerate}
\item We prove it for $\lambda=0$, following the same approach in Chapter 5, imposing thus stricter conditions on the coefficients and functions.
\item We prove the weakly coercitivity: In this case we require $\mu(\mathbf{x}) \geq \mu_{0}>0, \sigma \in \mathrm{L}^{\infty}(\Omega), \gamma \in L^{\infty}(\partial \Omega),\text{and }\mathbf{b}, \mathbf{c} \in\left[\mathrm{L}^{\infty}(\Omega)\right]^{2}, g_N \in L^2(\Gamma_N)$ and getting this weak coercitivity $\lambda$ (QV NAPDE 11.1.1):
$$\lambda > C \left( \frac{1}{a_0}\|\kappa\|^2_{L^\infty(\partial \Omega)} + \frac{1}{a_0} \|\mathbf{b} - \mathbf{c}\|_{L^\infty(\Omega)}^2 + \|a_0\|_{L^\infty(\Omega)} \right), 
$$
\textbf{Proof of weak coercitivity}

$$\alpha(v,v) \geq \mu_0 \|\nabla v\|_0^2 - \|\mathbf{b}-\mathbf{c}\|_{L^\infty(\Omega)}\|\nabla v\|_0\|v\|_0- \| \sigma \|_{L^\infty(\Omega)}^2 \|v\|_0^2 -\|\gamma\|_{L^\infty(\Omega)} \|v\|_{0,\partial \Omega}^2 $$
$$
\alpha(v,v) + \|\mathbf{b}-\mathbf{c}\|_{L^\infty(\Omega)}\|\nabla v\|_0\|v\|_0+ \| \sigma\|_{L^\infty(\Omega)}^2 \|v\|_0^2 +\|\gamma\|_{L^\infty(\Omega)} \|v\|_{0,\partial \Omega}^2\geq \mu_0 \|\nabla v\|_0^2  
$$
Using the following inequalities:
$$
\|\nabla v\|_0\|v\|_0 \leq \epsilon \|\nabla v \|_0^2+ \frac{1}{4\epsilon}\|v\|_0^2
$$
$$
\|v\|_{L^2(\partial \Omega)}^2\leq C'\|\nabla v \|_0\|v\|_0 \leq C' \biggl( \epsilon' \|\nabla v \|_0^2+ \frac{1}{4\epsilon'}\|v\|_0^2\biggl)
$$
\begin{flalign*}
\alpha(v,v) + \|\mathbf{b}-\mathbf{c}\|_{L^\infty(\Omega)} \left( \epsilon \|\nabla v \|_0^2+ \frac{1}{4\epsilon}\|v\|_0^2\right) +\|a_0\|_{L^\infty(\Omega)}^2 \|v\|_0^2 + \\ +C\|\gamma\|_{L^\infty(\Omega)} (\|v\|_0^2+\|\nabla v\|_0^2)+ \|\gamma\|_{L^\infty(\Omega)} C' \biggl( \epsilon' \|\nabla v \|_0^2+ \frac{1}{4\epsilon'}\|v\|_0^2\biggl) \|v\|_0^2
 \geq \mu_0 \|\nabla v\|_0^2 
\end{flalign*}

Next steps, don't get discouraged:

\[
\begin{aligned}
\alpha(v,v) + \left( \|\mathbf{b}-\mathbf{c}\|_{L^\infty(\Omega)}\frac{1}{4\epsilon} + \|a_0\|_{L^\infty(\Omega)} + C' \frac{1}{4\epsilon} \| \gamma \|_{L^\infty(\Omega)}\right) \|v\|_0^2 & \geq \\
\left(\mu_0 -\epsilon  \|\mathbf{b}-\mathbf{c}\|_{L^\infty(\Omega)}-C'\epsilon' \|\gamma\|_{L^\infty(\Omega)}\right)\|\nabla v\|_0^2, \\
\\
a(v,v) + \lambda \|v\|_0^2 \geq \frac{\left(\mu_0 -\epsilon  \|\mathbf{b}-\mathbf{c}\|_{L^\infty(\Omega)} -C'\epsilon' \|\gamma\|_{L^\infty(\Omega)}\right)}{1+ C_\Omega^2}\|v\|_1^2.
\end{aligned}
\]

Where we used in the last step the Poincaré inequality.

\end{enumerate}

\section{Galerkin Approximation}
For each $t>0$, find $u_{h}(t) \in V_{h}$ such that
\begin{equation}
\int_{\Omega} \frac{\partial u_{h}(t)}{\partial t} v_{h} d \Omega+a\left(u_{h}(t), v_{h}\right)=\int_{\Omega} f(t) v_{h} d \Omega \quad \forall v_{h} \in v_{h}
\end{equation}
with $u_{h}(0)=u_{0 h}$, where $V_{h} \subset V$ is a suitable space of finite dimension and $u_{0 h}$ is a convenient approximation of $u_{0}$ in the space $V_{h}$.

Such problem is called semi-discretization of (2.5), as the temporal variable has not yet been discretized.

\section{Algebraic Formulation}
We introduce a basis $\left\{\varphi_{j}\right\}$ for $V_{h}$ and we observe that it suffices that (2.6) is verified for the basis functions in order to be satisfied by all the functions of the subspace.

Moreover, since for each $t>0$ the solution to the Galerkin problem belongs to the subspace as well, we will have
\begin{equation}
u_{h}(\mathbf{x}, t)=\sum_{j=1}^{N_{h}} u_{j}(t) \varphi_{j}(\mathbf{x})
\end{equation}
where the coefficients $\left\{u_{j}(t)\right\}$ represent the unknowns of problem (2.6).

Denoting by $\dot{u}_{j}(t)$ the derivatives of the function $u_{j}(t)$ with respect to time, (2.6) becomes
\begin{equation*}
\int_{\Omega} \sum_{j=1}^{N_{h}} \dot{u}_{j}(t) \varphi_{j} \varphi_{i} d \Omega+a\left(\sum_{j=1}^{N_{h}} u_{j}(t) \varphi_{j}, \varphi_{i}\right)=\int_{\Omega} f(t) \phi_{i} d \Omega, \quad  i=1,2, \ldots, N_{h},
\end{equation*}
that is
\begin{equation}
\begin{array}{r}
\sum_{j=1}^{N_{h}} \dot{u}_{j}(t) \underbrace{\int_{\Omega} \varphi_{j} \varphi_{i} d \Omega}_{m_{i j}}+\sum_{j=1}^{N_{h}} u_{j}(t) \underbrace{a\left(\varphi_{j}, \varphi_{i}\right)}_{a_{i j}}=\underbrace{\int_{\Omega} f(t) \phi_{i} d \Omega}_{f_{i}(t)}, \quad \forall i \leq   N_{h}
\end{array}
\end{equation}
If we define the vector of unknowns $\mathbf{u}=\left(u_{1}(t), u_{2}(t), \ldots, u_{N_{h}}(t)\right)^{T}$, the mass matrix $\mathrm{M}=\left[m_{i j}\right]$, the stiffness matrix $\mathrm{A}=\left[a_{i j}\right]$ and the right-hand side vector $\mathbf{f}=\left(f_{1}(t), f_{2}(t), \ldots, f_{N_{h}}(t)\right)^{T}$, the system (2.7) can be rewritten in matrix form as
\begin{equation*}
M \dot{\mathbf{u}}(t)+\mathrm{A} \mathbf{u}(t)=\mathbf{f}(t)
\end{equation*}
\section{Time Discretization}
For the numerical solution of this ODE system, many finite difference methods are available. Here we limit ourselves to considering the so-called $\theta$-method.

The latter discretizes the temporal derivative by a simple difference quotient and replaces the other terms with a linear combination of the value at time $t^{k}$ and of the value at time $t^{k+1}$, depending on the real parameter $\theta(0 \leq \theta \leq 1)$,
\begin{equation}
\mathrm{M} \frac{\mathbf{u}^{k+1}-\mathbf{u}^{k}}{\Delta t}+\mathrm{A}\left[\theta \mathbf{u}^{k+1}+(1-\theta) \mathbf{u}^{k}\right]=\theta \mathbf{f}^{k+1}+(1-\theta) \mathbf{f}^{k}
\end{equation}
The real positive parameter $\Delta t=t^{k+1}-t^{k}, k=0,1, \ldots$, denotes the discretization step (here assumed to be constant), while the superscript $k$ indicates that the quantity under consideration refers to the time $t^{k}$.
Let us see some particular cases of (2.8):
\begin{itemize}
  \item For $\theta=0$ we obtain the forward Euler (or explicit Euler) method
\end{itemize}
\begin{equation}
\mathrm{M} \frac{\mathbf{u}^{k+1}-\mathbf{u}^{k}}{\Delta t}+\mathrm{A} \mathbf{u}^{k}=\mathbf{f}^{k}
\end{equation}
which is accurate to order one with respect to $\Delta t$;

\begin{itemize}
  \item For $\theta=1$ we have the backward Euler (or implicit Euler) method
\end{itemize}
\begin{equation}
\mathrm{M} \frac{\mathbf{u}^{k+1}-\mathbf{u}^{k}}{\Delta t}+\mathrm{A} \mathbf{u}^{k+1}=\mathbf{f}^{k+1},
\end{equation}
also of first order with respect to $\Delta t$;

\begin{itemize}
  \item For $\theta=1 / 2$ we have the Crank-Nicolson (or trapezoidal) method
\end{itemize}
\begin{equation}
\mathrm{M} \frac{\mathbf{u}^{k+1}-\mathbf{u}^{k}}{\Delta t}+\frac{1}{2} \mathrm{~A}\left(\mathbf{u}^{k+1}+\mathbf{u}^{k}\right)=\frac{1}{2}\left(\mathbf{f}^{k+1}+\mathbf{f}^{k}\right)
\end{equation}
which is of second order in $\Delta t$. (More precisely, $\theta=1 / 2$ is the only value for which we obtain a second-order method.)
Let us consider the two extremal cases, $\theta=0$ and $\theta=1$. For both, we obtain a system of linear equations:
In the $\theta$-method for solving ordinary differential equations (ODEs), the matrix that precedes $u^{k+1}$ in the system of linear equations depends on the choice of $\theta$:
\begin{enumerate}
  \item If $\theta=0$, the system to solve has matrix $\frac{M}{\Delta t}$
  \item If $\theta=1$, the system to solve has matrix $\frac{\mathrm{M}}{\Delta t}+A$
\end{enumerate}

We observe that the $M$ matrix is invertible, being positive definite. \\ \\
When \(\theta = 0\), the scheme isn't unconditionally stable. For a subspace \(V_{h}\) of finite elements, the stability condition is given by: \(\exists c > 0: \Delta t \leq c h^{2} \quad \forall h > 0\). This means that the time step \(\Delta t\) depends on the mesh size \(h\); it can't be chosen independently.\\ \\
 If we make the matrix \(M\) diagonal, we decouple the system. This is achieved through 'lumping' of the mass matrix, which means transforming \(M\) into a diagonal matrix.\\ \\
When \(\theta > 0\), the system equation becomes: \(K \mathbf{u}^{k+1} = \mathbf{g}\). Here, \(\mathbf{g}\) is the source term, and \(K = \frac{M}{\Delta t} + \theta A\). Since the spatial operator \(L\) and hence the matrix \(A\) are time-independent, and assuming the spatial mesh is constant, \(K\) can be factorized once at the start of the process.\\ \\
If both \(M\) and \(A\) are symmetric, then \(K\) is also symmetric. This allows the use of Cholesky factorization: \(K = H H^T\), where \(H\) is a lower triangular matrix. At each time step, two triangular systems need to be solved with \(N_{h}\) unknowns:\\
\begin{itemize}
\item \(H \mathbf{y} = \mathbf{g}\)
\item \(H^T \mathbf{u}^{k+1} = \mathbf{y}\)
\end{itemize}

\section{A priori estimates}
\subsection*{A priori estimates}
Let us consider problem (2.5); since the corresponding equations must hold for each $v \in V$, it will be legitimate to set $v=u(t)$ ($t$ being given), solution of the problem itself, yielding
\begin{equation}
    \int_{\Omega} \frac{\partial u(t)}{\partial t} u(t) d \Omega+a(u(t), u(t))=\int_{\Omega} f(t) u(t) d \Omega \quad \forall t>0
\end{equation}
Considering the individual terms, we have
\begin{equation}
\int_{\Omega} \frac{\partial u(t)}{\partial t} u(t) d \Omega=\frac{1}{2} \frac{d}{ dt} \int_{\Omega}|u(t)|^{2} d \Omega=\frac{1}{2} \frac{d}{d t}\|u(t)\|_{L^{2}(\Omega)}^{2}
\end{equation}
If we assume for simplicity that the bilinear form is coercive (with coercivity constant equal to $\alpha$), we obtain:
$$
a(u(t), u(t)) \geq \alpha\|u(t)\|_{V}^{2}
$$
while thanks to the Cauchy-Schwarz inequality, we find
\begin{equation}
    (f(t), u(t)) \leq\|f(t)\|_{\mathrm{L}^{2}(\Omega)}\|u(t)\|_{\mathrm{L}^{2}(\Omega)}
\end{equation}
In the remainder, we will often use Young's inequality
$$
\forall a, b \in \mathbb{R}, \quad a b \leq \varepsilon a^{2}+\frac{1}{4 \varepsilon} b^{2} \quad \forall \varepsilon>0
$$
which descends from the elementary inequality
$$
\left(\sqrt{\varepsilon} a-\frac{1}{2 \sqrt{\varepsilon}} b\right)^{2} \geq 0
$$
Using first Poincaré inequality and Young's inequality, we obtain
\begin{align}
\frac{1}{2} \frac{d}{d t}\|u(t)\|_{L^{2}(\Omega)}^{2} + \alpha\|\nabla u(t)\|_{L^{2}(\Omega)}^{2} &\leq \|f(t)\|_{L^{2}(\Omega)}\|u(t)\|_{L^{2}(\Omega)} \\
&\leq \frac{C_{\Omega}^{2}}{2 \alpha}\|f(t)\|_{L^{2}(\Omega)}^{2} + \frac{\alpha}{2}\|\nabla u(t)\|_{L^{2}(\Omega)}^{2}. \nonumber
\end{align}
The detailed steps are the following ones:
$$
\begin{aligned}
\frac{1}{2} \frac{d}{d t}\|u(t)\|_{L^2(\Omega)}^2+\alpha\|\nabla u(t)\|_{L^2(\Omega)}^2 
& \overset{\color{red}{\fbox{A}} }=\frac{1}{2} \frac{d}{d t}\|u(t)\|_{L^2(\Omega)}^2+\alpha\|u(t)\|_{V}^2\\ 
& \le \frac{1}{2} \frac{d}{d t}\|u(t)\|_{L^2(\Omega)}^2+a(u(t),u(t))\\
& = \int_{\Omega} f(t) u(t) d\Omega \\
&\leq\|f(t)\|_{L^2(\Omega)}\|u(t)\|_{L^2(\Omega)} \\
&\leq \frac{C_\Omega^2}{2\alpha}\|f(t)\|_{L^2(\Omega)}^2 + \frac1{4C_\Omega^2/(2\alpha)}\|u(t)\|_{L^2(\Omega)}^2 \\
& \overset{\color{blue}{\fbox{B}} }\leq \frac{C_{\Omega}^2}{2 \alpha}\|f(t)\|_{L^2(\Omega)}^2+\frac{\alpha}{2}\|\nabla u(t)\|_{L^2(\Omega)}^2 .
\end{aligned} 
$$
$\color{blue}{\fbox{B}} $  is Poincaré, but $V=H^1_0$, so for $\color{red}{\fbox{A}}$ we take $\|v\|_V := \|\nabla v\|_{L^2(\Omega)}$ which is equivalent to the $H^1$ norm on that space  $H_0^1$(that is, both upper and lower bounds). 
Then, by integrating in time we obtain, for all $t>0$,
\begin{equation}
\|u(t)\|_{L^{2}(\Omega)}^{2}+\alpha \int_{0}^{t}\|\nabla u(s)\|_{L^{2}(\Omega)}^{2} d s \leq\left\|u_{0}\right\|_{L^{2}(\Omega)}^{2}+\frac{C_{\Omega}^{2}}{\alpha} \int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)}^{2} d s .
\end{equation}
This is an a priori energy estimate. Different kinds of a priori estimates can be obtained as follows. Note that:
$$
\frac{1}{2} \frac{d}{d t}\|u(t)\|_{L^{2}(\Omega)}^{2}=\|u(t)\|_{L^{2}(\Omega)} \frac{d}{d t}\|u(t)\|_{L^{2}(\Omega)}
$$
Then from (5.14), using (2.14) and (2.15) we obtain (still using the Poincaré inequality)
$$
\begin{aligned}
\|u(t)\|_{L^{2}(\Omega)} \frac{d}{d t}\|u(t)\|_{L^{2}(\Omega)}+\frac{\alpha}{C_{\Omega}}\|u(t)\|_{L^{2}(\Omega)} & \|\nabla u(t)\|_{L^{2}(\Omega)} \\
& \leq\|f(t)\|_{L^{2}(\Omega)}\|u(t)\|_{L^{2}(\Omega)}, \quad t>0
\end{aligned}
$$
The detailed steps are the following:
Starting with the energy equality, applying the time derivative property, coercivity, the Cauchy-Schwarz inequality, and the Poincaré inequality, we derive the final inequality as follows:
\[
\begin{aligned}
&\|f(t)\|_{L^2(\Omega)} \|u(t)\|_{L^2(\Omega)} \geq \int_{\Omega} f(t)u(t) \, d\Omega   
= \frac{1}{2} \frac{d}{dt} \|u(t)\|^2_{L^2(\Omega)} + a(u(t), u(t))  \\
&\geq \frac{1}{2} \frac{d}{dt} \|u(t)\|^2_{L^2(\Omega)} + \alpha \|u(t)\|^2_{V} 
\geq \frac{1}{2} \frac{d}{dt} \|u(t)\|^2_{L^2(\Omega)} + \frac{\alpha}{C_{\Omega}^2} \|u(t)\|^2_{L^2(\Omega)}  \\
&\Rightarrow \|u(t)\|_{L^2(\Omega)} \frac{d}{dt}\|u(t)\|_{L^2(\Omega)} + \frac{\alpha}{C_{\Omega}^2} \|u(t)\|_{L^2(\Omega)}^2 \leq \|f(t)\|_{L^2(\Omega)} \|u(t)\|_{L^2(\Omega)},
\end{aligned}
\]

Remember that in this step we assume the space $V = H_0^1(\Omega)$, and so $|| v ||_V = || v ||_{H_0^1(\Omega)} = ||\nabla v||_{L^2_{\Omega}} $. \\ \\
If $\|u(t)\|_{L^{2}(\Omega)} \neq 0$ (otherwise we should proceed differently, even though the final result is still true) we can divide by $\|u(t)\|_{L^{2}(\Omega)}$ and integrate in time to obtain:
\begin{equation}
    \|u(t)\|_{L^{2}(\Omega)} \leq\left\|u_{0}\right\|_{L^{2}(\Omega)}+\int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)} d s, \quad t>0
\end{equation}
This is a further a priori estimate.
Let us now use the first inequality in (2.16) and integrate in time to yield:
$$
\begin{aligned}
\|u(t)\|_{L^2(\Omega)}^2 &+ 2\alpha \int_0^t \|\nabla u(s)\|_{L^2(\Omega)}^2 \,ds \\
&\leq \|u_0\|_{L^2(\Omega)}^2 + 2 \int_0^t \|f(s)\|_{L^2(\Omega)}\|u(s)\|_{L^2(\Omega)} \,ds  \\
&\leq_{\text{(equation 2.18)}} \|u_0\|_{L^2(\Omega)}^2 + 2 \int_0^t \|f(s)\|_{L^2(\Omega)} \left( \|u_0\|_{L^2(\Omega)} + \int_0^s \|f(\tau)\|_{L^2(\Omega)} \,d\tau \right) \,ds  \\
&= \|u_0\|_{L^2(\Omega)}^2 + 2 \int_0^t \|f(s)\|_{L^2(\Omega)}\|u_0\|_{L^2(\Omega)} \,ds 
 +  \int_0^t 2\|f(s)\|_{L^2(\Omega)} \int_0^s \|f(\tau)\|_{L^2(\Omega)} \,d\tau \,ds  \\
&= \left( \|u_0\|_{L^2(\Omega)} + \int_0^t \|f(s)\|_{L^2(\Omega)} \,ds \right)^2 \quad 
\end{aligned}
$$

The latter equality follows upon noticing that:
$$
2 \|f(s)\|_{L^{2}(\Omega)} \int_{0}^{s}\|f(\tau)\|_{L^{2}(\Omega)} d \tau=\frac{d}{d s}\left(\int_{0}^{s}\|f(\tau)\|_{L^{2}(\Omega)} d \tau\right)^{2}
$$

Therefore if we define $F(s) = \left(\int_{0}^{s}\|f(\tau)\|_{L^{2}(\Omega)} d \tau\right)^{2} $, from the Fundamental Theorem of Calculus we can derive that:
$$\int_0^t\frac{d}{ds}F(s) ds = F(t)-F(0) = F(0).$$
We finally conclude with the additional a priori estimate:
\begin{equation}
 \left(\|u(t)\|_{L^{2}(\Omega)}^{2}+2 \alpha \int_{0}^{t}\|\nabla u(s)\|_{L^{2}(\Omega)}^{2} d s\right)^{\frac{1}{2}} \\
 \quad \leq\left\|u_{0}\right\|_{L^{2}(\Omega)}+\int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)} d s, \quad t>0 . \\ \\
\end{equation}
We have seen that we can formulate the Galerkin problem (2.6) for problem (2.5) and that the latter, under suitable hypotheses, admits a unique solution. Similarly to what we did for problem (2.5) we can prove the following a priori (stability) estimates for the solution to problem (2.6):
$$
\begin{aligned}
\left\|u_{h}(t)\right\|_{L^{2}(\Omega)}^{2}+\alpha \int_{0}^{t} \| & \nabla u_{h}(s) \|_{L^{2}(\Omega)}^{2} d s \\
& \leq\left\|u_{0 h}\right\|_{L^{2}(\Omega)}^{2}+\frac{C_{\Omega}^{2}}{\alpha} \int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)}^{2} d s, \quad t>0
\end{aligned}
$$
For its proof we can take, for every $t>0, v_{h}=u_{h}(t)$ and proceed as we did to obtain (2.16). Then, by recalling that the initial data is $u_{h}(0)=u_{0 h}$, we can deduce the following discrete counterparts of (2.18 NMDP) and (2.19):
$$
\left\|u_{h}(t)\right\|_{L^{2}(\Omega)} \leq\left\|u_{0 h}(t)\right\|_{L^{2}(\Omega)}+\int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)} d s, \quad t>0
$$
and
$$
\begin{aligned}
&\left(\left\|u_{h}(t)\right\|_{L^{2}(\Omega)}^{2}+2 \alpha \int_{0}^{t}\left\|\nabla u_{h}(s)\right\|_{L^{2}(\Omega)}^{2} d s\right)^{\frac{1}{2}} \\
& \quad \leq\left\|u_{0 h}\right\|_{L^{2}(\Omega)}+\int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)} d s, \quad t>0
\end{aligned}
$$
\section*{Convergence Analysis of the Semi-Discrete Problem}
\begin{theorem}
There exists a constant $C>0$ independent of both $t$ and $h$ such that:
\begin{align*}
    & \left\{\left\|u(t)-u_{h}(t)\right\|_{L^{2}(\Omega)}^{2}+\alpha \int_{0}^{t}\left\|\nabla u(s)-\nabla u_{h}(s)\right\|_{L^{2}(\Omega)}^{2} d s\right\}^{1 / 2} \\
& \quad \leq C h^{r}\left\{\left|u_{0}\right|_{H^{r}(\Omega)}^{2}+\int_{0}^{t}|u(s)|_{H^{r+1}(\Omega)}^{2} d s+\int_{0}^{t}\left|\frac{\partial u(s)}{\partial s}\right|_{H^{r+1}(\Omega)}^{2} d s\right\}^{1 / 2} .
\end{align*}
\end{theorem}

\section*{Stability Analysis of the $\theta$-Method}
We now analyze the stability of the fully discretized problem. Applying the $\theta$-method to the Galerkin problem (2.6) we obtain
\begin{equation}
\begin{aligned}
\left(\frac{u_{h}^{k+1}-u_{h}^{k}}{\Delta t}, v_{h}\right) & +a\left(\theta u_{h}^{k+1}+(1-\theta) u_{h}^{k}, v_{h}\right) \\
& =\theta F^{k+1}\left(v_{h}\right)+(1-\theta) F^{k}\left(v_{h}\right) \quad \forall v_{h} \in v_{h},
\end{aligned}
\end{equation}
for each $k \geq 0$, with $u_{h}^{0}=u_{0 h}$.
$F^{k}$ indicates that the functional is evaluated at time $t^{k}$.
We will limit ourselves to the case where $F=0$ and start to consider the case of the implicit Euler method $(\theta=1)$ that is
\begin{equation}
\left(\frac{u_{h}^{k+1}-u_{h}^{k}}{\Delta t}, v_{h}\right)+a\left(u_{h}^{k+1}, v_{h}\right)=0 \quad \forall v_{h} \in v_{h}
\end{equation}
By choosing $v_{h}=u_{h}^{k+1}$, we obtain:
\begin{equation}
\left(u_{h}^{k+1}, u_{h}^{k+1}\right)+\Delta t \ a\left(u_{h}^{k+1}, u_{h}^{k+1}\right)=\left(u_{h}^{k}, u_{h}^{k+1}\right) .
\end{equation}
By exploiting the following inequalities:
\begin{equation}
a\left(u_{h}^{k+1}, u_{h}^{k+1}\right) \geq \alpha\left\|u_{h}^{k+1}\right\|_{V}^{2}
\end{equation}
\begin{equation}
\left(u_{h}^{k}, u_{h}^{k+1}\right) \leq \frac{1}{2}\left\|u_{h}^{k}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\frac{1}{2}\left\|u_{h}^{k+1}\right\|_{\mathrm{L}^{2}(\Omega)}^{2},
\end{equation}
the former deriving from the coercivity of the bilinear form $a(\cdot, \cdot)$, and the latter from the Cauchy-Schwarz and Young inequalities, we obtain
\begin{equation}
\left\|u_{h}^{k+1}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+2 \alpha \Delta t\left\|u_{h}^{k+1}\right\|_{V}^{2} \leq\left\|u_{h}^{k}\right\|_{\mathrm{L}^{2}(\Omega)}^{2} .
\end{equation}
Observing that $\left\|u_{h}^{k+1}\right\|_{V} \geq\left\|u_{h}^{k+1}\right\|_{\mathrm{L}^{2}(\Omega)}$, we deduce from the last equation that:
\begin{equation}
(1+2 \alpha \Delta t)\left\|u_{h}^{k+1}\right\|_{\mathrm{L}^{2}(\Omega)}^{2} \leq\left\|u_{h}^{k}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}
\end{equation}
hence:
\begin{equation}
\left\|u_{h}^{k+1}\right\|_{\mathrm{L}^{2}(\Omega)} \leq \frac{1}{\sqrt{1+2 \alpha \Delta t}}\left\|u_{h}^{k}\right\|_{\mathrm{L}^{2}(\Omega)}
\end{equation}
which entails:
\begin{equation}
\left\|u_{h}^{k}\right\|_{\mathrm{L}^{2}(\Omega)} \leq\left(\frac{1}{\sqrt{1+2 \alpha \Delta t}}\right)^{k}\left\|u_{0 h}\right\|_{\mathrm{L}^{2}(\Omega)}
\end{equation}
and therefore
\begin{equation}
\lim _{k \rightarrow \infty}\left\|u_{h}^{k}\right\|_{L^{2}(\Omega)}=0
\end{equation}
that is the backward Euler method is absolutely stable without any restriction on the time step $\Delta t$.
Assume now $f \neq 0$. We have
\begin{equation}
\underbrace{\left(\frac{u_{h}^{k+1}-u_{h}^{k}}{\Delta t}, u_{h}^{k+1}\right)}_{(I)}+\underbrace{a\left(u_{h}^{k+1}, u_{h}^{k+1}\right)}_{(II)}=\underbrace{\int_{\Omega} f^{k+1} u_{h}^{k+1}}_{(III)}
\end{equation}
We can derive the following inequalities:
\begin{align}
(I) & \geq \frac{1}{2 \Delta t}\left(\left\|u_{h}^{k+1}\right\|_{L^{2}}^{2}-\left\|u_{h}^{k}\right\|_{L^{2}}^{2}\right)^{1} \\
(II) & \geq \alpha\left\|u_{h}^{k+1}\right\|_{V}^{2} \quad(\text {coercivity of } a(\cdot, \cdot)) \\
(III) & \stackrel{\text { (C.S.) }}{\leq}\left\|f^{k+1}\right\|_{L^{2}}\left\|u_{h}^{k+1}\right\|_{V} \stackrel{\text { (Young) }}{\leq} \frac{1}{2 \alpha}\left\|f^{k+1}\right\|_{L^{2}}^{2}+\frac{\alpha}{2}\left\|u_{h}^{k+1}\right\|_{V}^{2}
\end{align}\\
The first inequality derives from the fact that $(a-b, a) \geq \frac{1}{2}\left(\|a\|^{2}-\|b\|^{2}\right) \ \forall a, b$. \\ 
Then, after summation on $k$, for $k=0, \ldots, n-1$, we obtain:
\begin{equation}
\begin{aligned}
& \left\|u_{h}^{n}\right\|_{L^{2}}^{2}+ \underbrace{\alpha \sum_{k=1}^{n} \Delta t\left\|u_{h}^{k}\right\|_{V}^{2}}_{\simeq \alpha \int_{0}^{t^{n}}\left\|u_{h}(t)\right\|_{V}^{2} d t} \leq\left\|u_{0, h}\right\|_{L^{2}}^{2}+\underbrace{\frac{1}{\alpha} \sum_{k=1}^{n} \Delta t\left\|f^{k}\right\|_{L^{2}}^{2}}_{\simeq \frac{1}{\alpha} \int_{0}^{t^{n}}\|f(t)\|_{L^{2}}^{2} d t} \\
\end{aligned}
\end{equation}
This leads to unconditional stability (no restriction on $\Delta t$).

Before analyzing the general case where $\theta$ is an arbitrary parameter ranging between 0 and 1, we introduce the following definition.

We say that the scalar $\lambda$ is an eigenvalue of the bilinear form $a(\cdot, \cdot): V \times V \mapsto \mathbb{R}$ and that $w \in V$ is its corresponding eigenfunction if it turns out that:
$$
a(w, v)=\lambda(w, v) \quad \forall v \in V
$$
If the bilinear form $a(\cdot, \cdot)$ is symmetric and coercive, it has positive, real eigenvalues forming an infinite sequence; moreover, its eigenfunctions form a basis of the space $V$.
The eigenvalues and eigenfunctions of $a(\cdot, \cdot)$ can be approximated by finding the pairs $\lambda_{h} \in \mathbb{R}$ and $w_{h} \in V_{h}$ which satisfy
\begin{equation}
a\left(w_{h}, v_{h}\right)=\lambda_{h}\left(w_{h}, v_{h}\right) \quad \forall v_{h} \in V_{h} .
\end{equation}
From an algebraic viewpoint, problem (2.35) can be formulated as follows:
$$
\mathrm{A} \mathbf{w}=\lambda_{h} \mathrm{M} \mathbf{w}
$$
where $A$ is the stiffness matrix and $M$ the mass matrix. We are therefore dealing with a generalized eigenvalue problem.
Such eigenvalues are all positive and $N_{h}$ in number ($N_{h}$ being as usual the dimension of the subspace $V_{h}$); after ordering them in ascending order, $\lambda_{h}^{1} \leq \lambda_{h}^{2} \leq \ldots \leq \lambda_{h}^{N_{h}}$, we have:
$$
\lambda_{h}^{N_{h}} \rightarrow \infty \quad \text { for } N_{h} \rightarrow \infty
$$
Moreover, the corresponding eigenfunctions form a basis for the subspace $V_{h}$ and can be chosen to be orthonormal with respect to the scalar product of $\mathrm{L}^{2}(\Omega)$. This means that, denoting by $w_{h}^{i}$ the eigenfunction corresponding to the eigenvalue $\lambda_{h}^{i}$, we have $\left(w_{h}^{i}, w_{h}^{j}\right)=\delta_{i j}$ $\forall i, j=1, \ldots, N_{h}$.
Thus, each function $v_{h} \in V_{h}$ can be represented as follows:
$$
v_{h}(\mathbf{x})=\sum_{j=1}^{N_{h}} v_{j} w_{h}^{j}(\mathbf{x})
$$
and, thanks to the eigenfunction orthonormality:
\begin{equation}
    \left\|v_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}=\sum_{j=1}^{N_{h}} v_{j}^{2}
\end{equation}
Let us consider an arbitrary $\theta \in[0,1]$ and let us limit ourselves to the case where the bilinear form $a(\cdot, \cdot)$ is symmetric (otherwise, although the final stability result holds in general, the following proof would not work, as the eigenfunctions would not necessarily form a basis).

Since $u_{h}^{k} \in V_{h}$, we can write

$$
u_{h}^{k}(\mathbf{x})=\sum_{j=1}^{N_{h}} u_{j}^{k} w_{h}^{j}(\mathbf{x})
$$
We observe that in this modal expansion, the $u_{j}^{k}$ no longer represent the nodal values of $u_{h}^{k}$.
If we now set $F=0$ in (2.20) and take $v_{h}=w_{h}^{i}$, we find:
\begin{enumerate}
    \item Original Equation (2.20):
    \[
    \left(\frac{u_h^{k+1} - u_h^k}{\Delta t}, v_h\right) + a\left(\theta u_h^{k+1} + (1-\theta) u_h^{k}, v_h\right) = \theta F^{k+1}(v_h) + (1-\theta) F^k(v_h) \quad \forall v_h \in V_h
    \]
    Here, \(u_h^{k}\) and \(u_h^{k+1}\) are the approximate solutions at time steps \(k\) and \(k+1\), \(v_h\) is a test function from the finite element space \(V_h\), and \(a(\cdot, \cdot)\) is a bilinear form.

    \item Discretization of \(u_h\):
    The solution \(u_h\) is approximated as a linear combination of basis functions \(w_h^j\) in the finite element space:
    \[
    u_h = \sum_{j=1}^{N_h} u_j w_h^j
    \]
    where \(N_h\) is the number of basis functions, and \(u_j\) are the coefficients.

    \item Substituting in Equation (2.20):\\ \\
    The term \(\left(\frac{u_h^{k+1} - u_h^k}{\Delta t}, v_h\right)\) in (2.20) becomes:
    \[
    \left(\frac{1}{\Delta t} \sum_{j=1}^{N_h} (u_j^{k+1} - u_j^k) w_h^j, v_h\right)
    \]
    By choosing \(v_h = w_h^i\), this term translates to:
    \[
    \frac{1}{\Delta t} \sum_{j=1}^{N_h} (u_j^{k+1} - u_j^k) (w_h^j, w_h^i)
    \]
    where \((w_h^j, w_h^i)\) represents the inner product of the basis functions.\\ 
    Similarly, the bilinear form \(a(\theta u_h^{k+1} + (1-\theta) u_h^{k}, v_h)\) becomes:
    \[
    a\left(\sum_{j=1}^{N_h} (\theta u_j^{k+1} + (1-\theta) u_j^k) w_h^j, w_h^i\right) = \sum_{j=1}^{N_h} (\theta u_j^{k+1} + (1-\theta) u_j^k) a(w_h^j, w_h^i)
    \]

    \item Thus, the substitution leads to the equation:
    \[
    \frac{1}{\Delta t} \sum_{j=1}^{N_h}[u_j^{k+1} - u_j^k](w_h^j, w_h^i) + \sum_{j=1}^{N_h}[\theta u_j^{k+1} + (1-\theta) u_j^k] a(w_h^j, w_h^i) = 0
    \]
\end{enumerate}

$$
\frac{1}{\Delta t} \sum_{j=1}^{N_{h}}\left[u_{j}^{k+1}-u_{j}^{k}\right]\left(w_{h}^{j}, w_{h}^{i}\right)+\sum_{j=1}^{N_{h}}\left[\theta u_{j}^{k+1}+(1-\theta) u_{j}^{k}\right] a\left(w_{h}^{j}, w_{h}^{i}\right)=0
$$
for each $i=1, \ldots, N_{h}$. \\ \\
For each pair $i, j=1, \ldots, N_{h}$ we have:
$$
a\left(w_{h}^{j}, w_{h}^{i}\right)=\lambda_{h}^{j}\left(w_{h}^{j}, w_{h}^{i}\right)=\lambda_{h}^{j} \delta_{i j}
$$
and thus, for each $i=1, \ldots, N_{h}$,
$$
\frac{u_{i}^{k+1}-u_{i}^{k}}{\Delta t}+\left[\theta u_{i}^{k+1}+(1-\theta) u_{i}^{k}\right] \lambda_{h}^{i}=0 .
$$
Solving now for $u_{i}^{k+1}$, we find
$$
u_{i}^{k+1}=u_{i}^{k} \frac{1-(1-\theta) \lambda_{h}^{i} \Delta t}{1+\theta \lambda_{h}^{i} \Delta t}
$$
Recalling (2.36), we can conclude that for the method to be absolutely stable, we must impose the inequality
$$
\left|\frac{1-(1-\theta) \lambda_{h}^{i} \Delta t}{1+\theta \lambda_{h}^{i} \Delta t}\right|<1
$$
that is:
$$
-1-\theta \lambda_{h}^{i} \Delta t<1-(1-\theta) \lambda_{h}^{i} \Delta t<1+\theta \lambda_{h}^{i} \Delta t .
$$
Hence,
$$
-\frac{2}{\lambda_{h}^{i} \Delta t}-\theta<\theta-1<\theta
$$
The second inequality is always verified, while the first one can be rewritten as:
$$
2 \theta-1>-\frac{2}{\lambda_{h}^{i} \Delta t}
$$
If $\theta \geq 1 / 2$, the left-hand side is non-negative, while the right-hand side is negative, so the inequality holds for each $\Delta t$. Instead, if $\theta<1 / 2$, the inequality is satisfied (hence the method is stable) only if:
$$
\Delta t<\frac{2}{(1-2 \theta) \lambda_{h}^{i}}.
$$
As such relation must hold for all the eigenvalues $\lambda_{h}^{i}$ of the bilinear form, it will suffice to require that it holds for the largest among them, which we have supposed to be $\lambda_{h}^{N_{h}}$.\\
To summarize, we have:
\begin{itemize}
  \item if $\theta \geq 1 / 2$, the $\theta$-method is unconditionally absolutely stable, i.e. it is absolutely stable for each $\Delta t$;
  \item if $\theta<1 / 2$, the $\theta$-method is absolutely stable only for $\Delta t \leq \frac{2}{(1-2 \theta) \lambda_{h}^{N_{h}}}$.
\end{itemize}
Thanks to the definition of eigenvalue (2.35) and to the continuity property of $a(\cdot, \cdot)$, we deduce
$$
\lambda_{h}^{N_{h}}=\frac{a\left(w_{N_{h}}, w_{N_{h}}\right)}{\left\|w_{N_{h}}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}} \leq \frac{M\left\|w_{N_{h}}\right\|_{V}^{2}}{\left\|w_{N_{h}}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}} \leq M\left(1+C^{2} h^{-2}\right)
$$
The constant $C>0$ which appears in the latter step derives from the following inverse inequality:
$$
\exists C>0:\left\|\nabla v_{h}\right\|_{\mathrm{L}^{2}(\Omega)} \leq C h^{-1}\left\|v_{h}\right\|_{\mathrm{L}^{2}(\Omega)} \quad \forall v_{h} \in v_{h}
$$
Hence, for $h$ small enough, $\lambda_{h}^{N_{h}} \leq C h^{-2}$. In fact, we can prove that $\lambda_{h}^{N_{h}}$ is indeed of the order of $h^{-2}$, that is

$$
\lambda_{h}^{N_{h}}=\max _{i} \lambda_{h}^{i} \simeq c h^{-2} .
$$
Keeping this into account, we obtain that for $\theta<1 / 2$ the method is absolutely stable only if
$$
\Delta t \leq C(\theta) h^{2}
$$
where $C(\theta)$ denotes a positive constant depending on $\theta$.
The latter relation implies that for $\theta<1 / 2, \Delta t$ cannot be chosen arbitrarily but is bound to the choice of $h$.

\section{Convergence analysis of the $\theta$-method}
\begin{theorem}
Under the hypothesis that $u_{0}, f$ and the exact solution are sufficiently regular, the following a priori error estimate holds: $\forall n \geq 1$,
\[
\left\|u\left(t^{n}\right)-u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+2 \alpha \Delta t \sum_{k=1}^{n}\left\|u\left(t^{k}\right)-u_{h}^{k}\right\|_{V}^{2} \leq C\left(u_{0}, f, u\right)\left(\Delta t^{p(\theta)}+h^{2 r}\right),
\]
where $p(\theta)=2$ if $\theta \neq 1 / 2, p(1 / 2)=4$ and $C$ depends on its arguments but not on $h$ and $\Delta t$.
\end{theorem}

\section{Parabolic ADR equation}
Consider the parabolic PDE, where $\Omega \subset \mathbb{R}^{2}$ is an open bounded domain:

\begin{equation}
\begin{cases}\frac{\partial u}{\partial t}-\mu \Delta u+\boldsymbol{\beta} \cdot \nabla u+\sigma u=f & \text { in } \Omega \times(0, \mathrm{~T}) \\ u=0 & \text { on } \partial \Omega \times(0, \mathrm{~T}), \\ u(0)=u_{0} & \text { in } \Omega,\end{cases}
\end{equation}
and where $\mu, \boldsymbol{\beta}, \sigma$ and $f$ are regular functions, satisfying:$$
\begin{array}{ll}
0<\mu_{0} \leq \mu \leq \mu_{1} & \text { a.e. in } \Omega \\
|\boldsymbol{\beta}| \leq b_{1} & \text { a.e. in } \Omega \\
0<\sigma_{0} \leq \sigma \leq \sigma_{1} & \text { a.e. in } \Omega
\end{array}$$
Introducing a finite dimensional space $V_{h} \subset H_{0}^{1}(\Omega)$, the semi-discrete Galerkin formulation reads: for all $t \in(0, T]$ find $u_{h}(t) \in V_{h}$ such that:
\begin{equation}
\left\{\begin{array}{c}
\int_{\Omega} \frac{\partial u_{h}(t)}{\partial t} v_{h} d x+\int_{\Omega} \mu \nabla u_{h}(t) \cdot \nabla v_{h}+\int_{\Omega} \beta \cdot \nabla u_{h}(t) v_{h}+\int_{\Omega} \sigma u_{h}(t) v_{h} \\
=\int_{\Omega} f v_{h} \quad \forall v_{h} \in v_{h},
\end{array}\right.
\end{equation}
and such that $u_{h}(0)=u_{0, h}$, where $u_{0, h}$ is the projection of the initial condition into $V_{h}$.

\section{A semimplicit scheme}
We consider a time-advancing scheme, where the diffusion and reaction terms are treated implicitly, while the advection term is treated explicitly. Let us denote $t_{k}=k \Delta t$, for $k=0, \ldots, N$, where $\Delta t=T / N$. Let $u_{h}^{(k)}$ be the approximation of $u\left(t_{k}\right)$. A fully discretized version of (2.37) reads:
\begin{equation}
\left\{\begin{array}{l}
\left(\frac{u_{h}^{(k+1)}-u_{h}^{(k)}}{\Delta t}, v_{h}\right)+\left(\mu \nabla u_{h}^{(k+1)}, \nabla v_{h}\right)+\left(\boldsymbol{\beta} \cdot \nabla u_{h}^{(k)}, v_{h}\right) \\
\quad+\left(\sigma u_{h}^{(k+1)}, v_{h}\right)=\left(f, v_{h}\right) \quad \forall v_{h} \in v_{h}, \quad k=0, \ldots, N-1 \\
u_{h}^{(0)}=u_{0, h}
\end{array}\right.
\end{equation}
where $(\cdot, \cdot)$ denotes the $L^{2}(\Omega)$ scalar product.

\section{Stability analysis of the semimplicit scheme}
\begin{theorem}
In the following derivations, the assumption of $f=0$ will be made.
If the coefficients of the problem satisfy:
$$
b_{1}^{2}<4 \mu_{0} \sigma_{0},
$$
then the semimplicit scheme (2.39) is absolutely stable for any choice of $\Delta t$. Consider now the case $\sigma=0$. If the coefficients of the problem satisfy $\left(C_{p}\right.$ being the Poincaré constant):
$$
b_{1}<\mu_{0} / C_{p}
$$
then the scheme is absolutely stable for any choice of $\Delta t$.  \\ \\
\end{theorem}
\textbf{Proof.} 
Let us choose $v_{h}=u_{h}^{(k+1)}$. We have:
$$
\begin{aligned}
& \left(\mu \nabla u_{h}^{(k+1)}, \nabla u_{h}^{(k+1)}\right) \geq \mu_{0}\left\|\nabla u_{h}^{(k+1)}\right\|^{2} \\
& \left(\sigma u_{h}^{(k+1)}, u_{h}^{(k+1)}\right) \geq \sigma_{0}\left\|u_{h}^{(k+1)}\right\|^{2}
\end{aligned}
$$
which entails, assuming $f = 0$, for every $k$
$$
\begin{array}{r}
\left\|u_{h}^{(k+1)}\right\|^{2}+\Delta t \mu_{0}\left\|\nabla u_{h}^{(k+1)}\right\|^{2}+\Delta t \sigma_{0}\left\|u_{h}^{(k+1)}\right\|^{2} \leq \\
\left|\left(u_{h}^{(k)}, u_{h}^{(k+1)}\right)\right|+\Delta t\left|\left(\boldsymbol{\beta} \cdot \nabla u_{h}^{(k)}, u_{h}^{(k+1)}\right)\right|
\end{array}
$$
The two right-hand side terms can be bounded by combining the Cauchy-Schwarz and the Young inequalities:
$$
\begin{aligned}
& \left|\left(u_{h}^{(k)}, u_{h}^{(k+1)}\right)\right| \leq \frac{1}{2 \eta_{1}}\left\|u_{h}^{(k)}\right\|^{2}+\frac{\eta_{1}}{2}\left\|u_{h}^{(k+1)}\right\|^{2} \\
& \left|\left(\boldsymbol{\beta} \cdot \nabla u_{h}^{(k)}, u_{h}^{(k+1)}\right)\right| \leq \frac{b_{1}}{2 \eta_{2}}\left\|\nabla u_{h}^{(k)}\right\|^{2}+\frac{\eta_{2} b_{1}}{2}\left\|u_{h}^{(k+1)}\right\|^{2},
\end{aligned}
$$
where the positive constants $\eta_{1}$ and $\eta_{2}$ will be later fixed according to our best convenience.
We end up with the following inequality:
$$
\begin{aligned}
& \underbrace{\left[1+\Delta t \sigma_{0}-\frac{\eta_{1}}{2}-\frac{\Delta t \eta_{2} b_{1}}{2}\right]}_{A}\left\|u_{h}^{(k+1)}\right\|^{2}+\underbrace{\Delta t \mu_{0}}_{B}\left\|\nabla u_{h}^{(k+1)}\right\|^{2} \\
& \leq \underbrace{\frac{1}{2 \eta_{1}}}_{A^{\prime}}\left\|u_{h}^{(k)}\right\|^{2}+\underbrace{\frac{\Delta t b_{1}}{2 \eta_{2}}}_{B^{\prime}}\left\|\nabla u_{h}^{(k)}\right\|^{2}
\end{aligned}
$$
In order to prove stability, we need $A>A^{\prime}$ and $B>B^{\prime}$. Indeed, if this were true, then we would have:
$$
A\left\|u_{h}^{(k+1)}\right\|^{2}+B\left\|\nabla u_{h}^{(k+1)}\right\|^{2} \leq \max \left(\frac{A^{\prime}}{A}, \frac{B^{\prime}}{B}\right)\left[A\left\|u_{h}^{(k)}\right\|^{2}+B\left\|\nabla u_{h}^{(k)}\right\|^{2}\right]
$$
which is the seeked stability result in the norm
$\|\cdot\|_{A, B}:=\left(A\|\cdot\|^{2}+B\|\nabla \cdot\|^{2}\right)^{1 / 2}$, equivalent to the standard $V_{h}$ norm.
Therefore, we look for a suitable choice (if it exists) of $\eta_{1}$ and $\eta_{2}$ that
ensures $A>A^{\prime}$ and $B>B^{\prime}$. The second inequality is satifsied if and only if

$$
\eta_{2}=\frac{b_{1}+\epsilon}{2 \mu_{0}}
$$
for some $\epsilon>0$.
Hence, the first inequality reads
$$
1+\Delta t \sigma_{0}-\frac{\Delta t b_{1}\left(b_{1}+\epsilon\right)}{4 \mu_{0}}>\frac{1}{2 \eta_{1}}+\frac{\eta_{1}}{2}
$$
The right-hand side is minimized for $\eta_{1}=1$, thus leading to the condition:
\begin{equation}
4 \frac{\mu_{0} \sigma_{0}}{b_{1}\left(b_{1}+\epsilon\right)}>1
\end{equation}
Clearly, it is possible to find $\epsilon>0$ such that this holds if and only if:
\begin{equation}
b_{1}^{2}<4 \mu_{0} \sigma_{0}
\end{equation}
In conclusion, whenever the coefficients of the problem satisfy the condition (2.41), the scheme (2.39) is absolutely stable, for any choice of $\Delta t$.
Let us consider now the case $\sigma=0$. Proceeding as before, we have:
$$
\begin{gathered}
{\left[1-\frac{\eta_{1}}{2}-\frac{\Delta t \eta_{2} b_{1}}{2}\right]\left\|u_{h}^{(k+1)}\right\|^{2}+\Delta t \mu_{0}\left\|\nabla u_{h}^{(k+1)}\right\|^{2}} 
\leq \frac{1}{2 \eta_{1}}\left\|u_{h}^{(k)}\right\|^{2}+\frac{\Delta t b_{1}}{2 \eta_{2}}\left\|\nabla u_{h}^{(k)}\right\|^{2}
\end{gathered}
$$
Let us introduce a constant $\omega \in(0,1)$ (to be fixed later). By the Poincaré inequality, we have:
$$
\begin{aligned}
\left\|\nabla u_{h}^{(k+1)}\right\|^{2} & =(1-\omega)\left\|\nabla u_{h}^{(k+1)}\right\|^{2}+\omega\left\|\nabla u_{h}^{(k+1)}\right\|^{2} \\
& \geq \frac{1-\omega}{C_{p}^{2}}\left\|u_{h}^{(k+1)}\right\|^{2}+\omega\left\|\nabla u_{h}^{(k+1)}\right\|^{2}
\end{aligned}
$$
Combining the latter inequalities, we have
$$
\begin{aligned}
& \underbrace{\left[1-\frac{\eta_{1}}{2}-\frac{\Delta t \eta_{2} b_{1}}{2}+\frac{(1-\omega) \Delta t \mu_{0}}{C_{p}^{2}}\right]}\left\|u_{h}^{(k+1)}\right\|^{2}+\underbrace{\omega \Delta t \mu_{0}}_{B}\left\|\nabla u_{h}^{(k+1)}\right\|^{2} \\
& \leq \underbrace{\frac{1}{2 \eta_{1}}}_{A^{\prime}}\left\|u_{h}^{(k)}\right\|^{2}+\underbrace{\frac{\Delta t b_{1}}{2 \eta_{2}}}_{B^{\prime}}\left\|\nabla u_{h}^{(k)}\right\|^{2}
\end{aligned}
$$
As in the previous point, we look for conditions on the coefficients such that $A>A^{\prime}$ and $B>B^{\prime}$. The second inequality is satifsied if and only if
$$
\eta_{2}=\frac{b_{1}+\epsilon}{2 \omega \mu_{0}}
$$
for some $\epsilon>0$.
Then, the first inequality reads
$$
1-\frac{\Delta t b_{1}\left(b_{1}+\epsilon\right)}{4 \omega \mu_{0}}+\frac{(1-\omega) \Delta t \mu_{0}}{C_{p}^{2}}>\frac{1}{2 \eta_{1}}+\frac{\eta_{1}}{2}
$$
The right-hand side is minimized for $\eta_{1}=1$. Rearranging the terms, we get
$$
-\omega^{2}+\omega-\frac{b_{1}\left(b_{1}+\epsilon\right) C_{p}^{2}}{4 \mu_{0}^{2}}>0
$$
Real solutions $\omega \in(0,1)$ exists whenever the discriminant is positive, that is:
$$
b_{1}\left(b_{1}+\epsilon\right) C_{p}^{2}<\mu_{0}^{2}
$$
The latter condition can be satisfied (by suitably choosing $\epsilon$) if and only if
\begin{equation}
b_{1}<\mu_{0} / C_{p}
\end{equation}
In conclusion, if (2.42) is satisfied, the scheme is absolutely stable for any choice of $\Delta t$.

\chapter{Domain Decomposition Methods}
An elementary introduction in 1D
Consider the 1D elliptic BVP:
$$
\begin{cases}\mathcal{L} u:=-u^{\prime \prime}=f & a<x<b \\ u(a)=u(b)=0 & \end{cases}
$$
This is equivalent to:
$$
\left\{\begin{array}{l}
\text { Find } u \in V=H_{0}^{1}(a, b) \text { s.t. } \\
a(u, v)=(f, v)
\end{array} \quad \forall v \in V\right.
$$
with
$$
a(u, v)=\int_{a}^{b} u^{\prime} v^{\prime}, \quad(f, v)=\int_{a}^{b} f v
$$
Consider the domain splitting: $\bar{\Omega}=\bar{\Omega}_{1} \cup \bar{\Omega}_{2}, \Omega_{1} \cap \Omega_{2}=\emptyset$

$$
a<\gamma<b
$$
Correspondingly, consider the following splitting of the space $V$ :
$$
V=V_{1} \oplus H_{\gamma} \oplus V_{2}
$$
where
$$
v_{i}=\tilde{H}_{0}^{1}\left(\Omega_{i}\right):=\left\{v \in H_{0}^{1}(\Omega):\left.v\right|_{\Omega_{i}} \in H_{0}^{1}\left(\Omega_{i}\right),\left.v\right|_{\Omega \backslash \Omega_{i}}=0\right\}, i=1,2
$$
$H_{\gamma}=$ space of harmonic extensions of functions whose values at $\gamma$ is given $=\left\{v_{H} \in V: v(\gamma)=v_{\gamma},\left.v\right|_{\Omega_{i}}=v_{i}, v_{i}^{\prime \prime}=0, i=1,2 \quad \forall v_{\gamma} \in \mathbb{R}\right\}$

\begin{proposition}
$u$ is a solution to (2) if and only if:
\[
\left\{\begin{array}{lll}
-u_{1}^{\prime \prime}=f & \text{in } \Omega_{1} & \\
-u_{2}^{\prime \prime}=f & \text{in } \Omega_{2} & \\
u_{1}=u_{2} & \text{on } \gamma & \text{Transmission condition (D)} \\
u_{1}^{\prime}=u_{2}^{\prime} & \text{on } \gamma & \text{Transmission condition (N)} \\
u_{1}(a)=0, u_{2}(b)=0 & & \text{B.C.}
\end{array}\right.
\]
The converse is also true, that is: if $u_{1}, u_{2}$ are solutions to (3.8), then, setting $u$ such that $\left.u\right|_{\Omega_{1}}=u_{1},\left.u\right|_{\Omega_{2}}=u_{2}$, $u$ is a solution to (2).
Therefore, Problem (1) is equivalent to Problem (3.5).
Similar results in $\mathbb{R}^{d}$:
\[
\begin{aligned}
& \left\{\begin{array}{ll} 
- \Delta u = f & \text{in } \Omega \\
u = 0 & \text{on } \partial \Omega
\end{array} \right. \Longleftrightarrow \left\{\begin{array}{ll}
-\Delta u_{1}=f & \text{in } \Omega_{1} \\
-\Delta u_{2}=f & \text{in } \Omega_{2} \\
u_{1}=u_{2} & \text{on } \Gamma \text{(Transmission cond. (D))} \\
\frac{\partial u_{1}}{\partial \mathbf{n}}=\frac{\partial u_{2}}{\partial \mathbf{n}} & \text{on } \Gamma \text{(Transmission cond. (N))} \\
u_{1}=0 & \text{on } \partial \Omega_{1} \backslash \Gamma \\
u_{2}=0 & \text{on } \partial \Omega_{2} \backslash \Gamma
\end{array}\right.
\end{aligned}
\]
\end{proposition}

\begin{center}
\includegraphics[scale = 0.3]{2023_12_08_07eeb097c3e16596478fg-008}
\end{center}

\section*{Motivation}
\textbf{Domain Decomposition (DD)} can be applied within any discretization method for PDEs, such as Finite Element Method (FEM), Finite Volume (FV), Finite Difference (FD), and Spectral Element Method (SEM), to enhance the efficiency of their algebraic solutions on parallel computing platforms. \textbf{DD methods} facilitate the division of a boundary-value problem across subdivided computational domains. This approach offers a \textit{particularly advantageous framework for solving heterogeneous or multiphysics problems}, that is, those involving different types of differential equations in various sections of the computational domain.

\section*{The Idea}
The computational domain $\Omega$, where the boundary value problem (BVP) is established, is partitioned into two or more subdomains. In these subdomains, problems of reduced dimension are solved. Parallel solution algorithms are applicable here. There are two methods for dividing the computational domain: using either disjoint or overlapping subdomains.

\section*{References}
\begin{itemize}
    \item B.F. Smith, P.E. Bjørstad, W.D. Gropp (1996) \textit{Domain Decomposition}. Cambridge University Press, Cambridge.
    \item A. Quarteroni and A. Valli (1999) \textit{Domain Decomposition Methods for Partial Differential Equations}. Oxford Science Publications, Oxford.
    \item A. Toselli and O.B. Widlund (2005) \textit{Domain Decomposition Methods – Algorithms and Theory}. Springer-Verlag, Berlin and Heidelberg.
\end{itemize}

\section{Classical Iterative DD Methods}
\subsection*{Model problem}
Consider the model problem:
Find $u: \Omega \rightarrow \mathbb{R}$ s.t.
$$
\begin{cases}L u=f & \text { in } \Omega \\ u=0 & \text { on } \partial \Omega\end{cases}
$$
$L$ is a generic second order elliptic operator.
The weak formulation reads:
$$
\text { find } u \in V=H_{0}^{1}(\Omega): \quad a(u, v)=(f, v) \quad \forall v \in V
$$
where $a(\cdot, \cdot)$ is the bilinear form associated with $L$.


\subsection*{Non Overlapping Decomposition}
We partition now the domain $\Omega$ in two disjoint subdomains:
\begin{center}
\includegraphics[scale = 0.3]{2023_12_08_07eeb097c3e16596478fg-031}
\end{center}
The following equivalence result holds.
\begin{theorem}
The solution $u$ of the model problem is such that $u_{\left.\right|_{\Omega_{i}}}=u_{i}$ for $i=1,2$, where $u_{i}$ is the solution to the problem
$$
\begin{cases}L u_{i}=f & \text { in } \Omega_{i} \\ u_{i}=0 & \text { on } \partial \Omega_{i} \backslash \Gamma\end{cases}
$$
with interface conditions:
$$
u_{1}=u_{2} \quad \text { and } \quad \frac{\partial u_{1}}{\partial n_{L}}=\frac{\partial u_{2}}{\partial n_{L}} \quad \text { on } \Gamma
$$
where $\partial / \partial n_{L}$ is the conormal derivative.
\end{theorem}
\subsection*{Dirichlet-Neumann Method}
Given $u_{2}^{(0)}$ on $\Gamma$, for $k \geq 1$ solve the problems:
$$
\begin{aligned}
& \begin{cases}L u_{1}^{(k)}=f & \text { in } \Omega_{1} \\
u_{1}^{(k)}=u_{2}^{(k-1)} & \text { on } \Gamma \\
u_{1}^{(k)}=0 & \text { on } \partial \Omega_{1} \backslash \Gamma\end{cases} \\
& \begin{cases}L u_{2}^{(k)}=f & \text { in } \Omega_{2} \\
\frac{\partial u_{2}^{(k)}}{\partial n_{L}}=\frac{\partial u_{1}^{(k)}}{\partial n_{L}} & \text { on } \Gamma \\
u_{2}^{(k)}=0 & \text { on } \partial \Omega_{2} \backslash \Gamma\end{cases}
\end{aligned}
$$
The equivalence theorem ensures that if the sequences $\{u_{1}^{(k)}\}$ and $\{u_{2}^{(k)}\}$ converge, their limits will necessarily be the solution to the original problem. Thus, the Dirichlet-Neumann (DN) algorithm is consistent. However, it's important to note that convergence of this algorithm is not always assured.


\subsection*{Example}
Let $\Omega=(a, b), \gamma \in(a, b), L=-d^{2} / d x^{2}$ and $f=0$. At every $k \geq 1$ the DN algorithm generates the two subproblems:

$$
\begin{aligned}
& \begin{cases}-\left(u_{1}^{(k)}\right)^{\prime \prime}=0 & a<x<\gamma \\
u_{1}^{(k)}=u_{2}^{(k-1)} & x=\gamma \\
u_{1}^{(k)}=0 & x=a\end{cases} \\
& \begin{cases}-\left(u_{2}^{(k)}\right)^{\prime \prime}=0 & \gamma<x<b \\
\left(u_{2}^{(k)}\right)^{\prime}=\left(u_{1}^{(k)}\right)^{\prime} & x=\gamma \\
u_{2}^{(k)}=0 & x=b .\end{cases}
\end{aligned}
$$
The two sequences converge only if $\gamma>(a+b) / 2$ :
\begin{center}
    \includegraphics[scale=0.2]{2023_12_08_07eeb097c3e16596478fg-034}
\end{center}
A variant of the Dirichlet-Neumann (DN) algorithm can be implemented by modifying the Dirichlet condition in the first subdomain to 
$$
u_{1}^{(k)}=\theta u_{2}^{(k-1)}+(1-\theta) u_{1}^{(k-1)} \text{ on } \Gamma
$$
using a relaxation parameter $\theta > 0$. This approach allows for a reduction in the error between two subsequent iterates.
In the aforementioned example, it can be easily verified that by selecting 
$$
\theta_{\text{opt}} = -\frac{u_{1}^{(k-1)}}{u_{2}^{(k-1)} - u_{1}^{(k-1)}}
$$
the algorithm converges to the exact solution in a single iteration.

More generally, there exists an appropriate value for $0 < \theta_{\max} < 1$ such that the DN algorithm converges for any choice of the relaxation parameter $\theta$ in the interval $(0, \theta_{\max})$.


\subsection*{Neumann-Neumann Algorithm}
Consider again a partition of $\Omega$ into two disjoint subdomains and denote by $\lambda$ the (unknown) value of the solution $u$ on their interface $\Gamma$ :
$$
\lambda=u_{i} \text { on } \Gamma \quad(i=1,2)
$$
Consider the following iterative algorithm: for any given $\lambda^{(0)}$ on $\Gamma$, for $k \geq 0$ and $i=1,2$, solve the following problems:

$$
\begin{gathered}
\begin{cases}L u_{i}^{(k+1)}=f & \text { in } \Omega_{i} \\
u_{i}^{(k+1)}=\lambda^{(k)} & \text { on } \Gamma \\
u_{i}^{(k+1)}=0 & \text { on } \partial \Omega_{i} \backslash \Gamma\end{cases} \\
\begin{cases}L \psi_{i}^{(k+1)}=0 & \text { in } \Omega_{i} \\
\frac{\partial \psi_{i}^{(k+1)}}{\partial n}=\frac{\partial u_{1}^{(k+1)}}{\partial n}-\frac{\partial u_{2}^{(k+1)}}{\partial n} & \text { on } \Gamma \\
\psi_{i}^{(k+1)}=0 & \text { on } \partial \Omega_{i} \backslash \Gamma\end{cases}
\end{gathered}
$$
with
$$
\lambda^{(k+1)}=\lambda^{(k)}-\theta\left(\sigma_{1} \psi_{1 \mid \Gamma}^{(k+1)}-\sigma_{2} \psi_{2 \mid \Gamma}^{(k+1)}\right)
$$
where $\theta$ is a positive acceleration parameter, while $\sigma_{1}$ and $\sigma_{2}$ are two positive coefficients.



\section{The Steklov-Poincaré interface Equation}
\subsection*{Multi-Domain Formulation of Poisson Problem and Interface Conditions}
We consider now the model problem:
$$
\begin{cases}-\triangle u=f & \text { in } \Omega \\ u=0 & \text { on } \partial \Omega\end{cases}
$$
For a domain partitioned into two disjoint subdomains, we can write the equivalent multi-domain formulation $\left(u_{i}=u_{\mid \Omega_{i}}, i=1,2\right)$ :
$$
\begin{cases}-\triangle u_{1}=f & \text { in } \Omega_{1} \\ u_{1}=0 & \text { on } \partial \Omega_{1} \backslash \Gamma \\ -\triangle u_{2}=f & \text { in } \Omega_{2} \\ u_{2}=0 & \text { on } \partial \Omega_{2} \backslash \Gamma \\ u_{1}=u_{2} & \text { on } \Gamma \\ \frac{\partial u_{1}}{\partial n}=\frac{\partial u_{2}}{\partial n} & \text { on } \Gamma\end{cases}
$$
\subsection*{Remark}
\begin{itemize}
  \item On the interface $\Gamma$ we have the normal unit vectors $\mathbf{n}_{1}$ and $\mathbf{n}_{2}$ :
\end{itemize}
\begin{center}
\includegraphics[scale = 0.3]{2023_12_08_07eeb097c3e16596478fg-040}
\end{center}
\begin{itemize}
  \item There holds: $\mathbf{n}_{1}=-\mathbf{n}_{2}$ on $\Gamma$.
  \item We denote $\mathbf{n}=\mathbf{n}_{1}$ so that
\end{itemize}
$$
\frac{\partial}{\partial n}=\frac{\partial}{\partial n_{1}}=-\frac{\partial}{\partial n_{2}} \quad \text { on } \Gamma \text {. }
$$
\subsection*{The Steklov-Poincaré Operator}
Let $\lambda$ be the unknown value of the solution $u$ on the interface $\Gamma$ :
$$
\lambda=u_{\mid r}
$$
Should we know a priori the value $\lambda$ on $\Gamma$, we could solve the following two independent boundary-value problems with Dirichlet condition on $\Gamma$ $(i=1,2)$ :
$$
\begin{cases}-\triangle w_{i}=f & \text { in } \Omega_{i} \\ w_{i}=0 & \text { on } \partial \Omega_{i} \backslash \Gamma \\ w_{i}=\lambda & \text { on } \Gamma .\end{cases}
$$
With the aim of obtaining the value $\lambda$ on $\Gamma$, let us split $w_{i}$ as follows
$$
w_{i}=w_{i}^{*}+u_{i}^{0}
$$
where $w_{i}^{*}$ and $u_{i}^{0}$ represent the solutions of the following problems $(i=1,2)$ :
$$
\begin{cases}-\triangle w_{i}^{*}=f & \text { in } \Omega_{i} \\ w_{i}^{*}=0 & \text { on } \partial \Omega_{i} \cap \partial \Omega \\ w_{i}^{*}=0 & \text { on } \Gamma\end{cases}
$$
and
$$
\begin{cases}-\triangle u_{i}^{0}=0 & \text { in } \Omega_{i} \\ u_{i}^{0}=0 & \text { on } \partial \Omega_{i} \cap \partial \Omega \\ u_{i}^{0}=\lambda & \text { on } \Gamma .\end{cases}
$$
The functions \( w_{i}^{*} \) depend solely on the source data \( f \), thus we can express this as
\[ w_{i}^{*} = G_{i} f \]
where \( G_{i} \) is a linear continuous operator.

Furthermore, \( u_{i}^{0} \) depend solely on the value \( \lambda \) on \( \Gamma \), leading to the expression
\[ u_{i}^{0} = H_{i} \lambda \]
where \( H_{i} \) is the so-called harmonic extension operator of \( \lambda \) on the domain \( \Omega_{i} \).\\
We have that:

\begin{align*}
 & w_{i}=w_{i}^{*}+u_{i}^{0} \quad(i=1,2) \\ 
\Leftrightarrow & \frac{\partial w_{1}}{\partial n}=\frac{\partial w_{2}}{\partial n} \quad \text { on } \Gamma \\ 
\Leftrightarrow & \frac{\partial}{\partial n}\left(w_{1}^{*}+u_{1}^{0}\right)=\frac{\partial}{\partial n}\left(w_{2}^{*}+u_{2}^{0}\right) \quad \text { on } \Gamma \\ 
\Leftrightarrow & \frac{\partial}{\partial n}\left(G_{1} f+H_{1} \lambda\right)=\frac{\partial}{\partial n}\left(G_{2} f+H_{2} \lambda\right) \quad \text { on } \Gamma \\ 
\Leftrightarrow & \left(\frac{\partial H_{1}}{\partial n}-\frac{\partial H_{2}}{\partial n}\right) \lambda=\left(\frac{\partial G_{2}}{\partial n}-\frac{\partial G_{1}}{\partial n}\right) f \quad \text { on } \Gamma .
\end{align*}

The equivalence between the decomposition \( w_i = w_i^* + u_i^0 \) and the equality of normal derivatives is not a direct mathematical derivation but rather a condition for the physical and mathematical consistency of the problem setup and its solution.
We have obtained the Steklov-Poincaré equation for the unknown $\lambda$ on the interface $\Gamma$ :
$$
S \lambda=\chi \quad \text { on } \Gamma
$$
\begin{itemize}
  \item $S$ is the Steklov-Poincaré pseudo-differential operator:
\end{itemize}
$$
S \mu=\frac{\partial}{\partial n} H_{1} \mu-\frac{\partial}{\partial n} H_{2} \mu=\sum_{i=1}^{2} \frac{\partial}{\partial n_{i}} H_{i} \mu
$$
\begin{itemize}
  \item $\chi$ is a linear functional which depends on $f$ :
\end{itemize}
$$
\chi=\frac{\partial}{\partial n} G_{2} f-\frac{\partial}{\partial n} G_{1} f=-\sum_{i=1}^{2} \frac{\partial}{\partial n_{i}} G_{i} f
$$
\begin{itemize}
  \item The operator
\end{itemize}
$$
S_{i}: \mu \rightarrow S_{i} \mu=\left.\frac{\partial}{\partial n_{i}}\left(H_{i} \mu\right)\right|_{\Gamma} \quad i=1,2
$$
is called local Steklov-Poincaré operator (Dirichlet-to-Neumann) which operates between the trace space
$$
\Lambda=\left\{\mu: \exists v \in V \text { s.t. } \mu=v_{\mid \Gamma}\right\}=H_{00}^{1 / 2}(\Gamma)
$$
and its dual $\Lambda^{\prime}$.
\subsection*{Example}
To provide an example of the operator $S$, we consider a simple 1D problem.

Let $\Omega=(a, b) \subset \mathbb{R}$ as illustrated below. We split $\Omega$ in two nonoverlapping subdomains. In this case the interface $\Gamma$ reduces to the point $\gamma \in(a, b)$ and the Steklov-Poincaré operator $S$ becomes
$$
S \lambda=\left(\frac{d H_{1}}{d x}-\frac{d H_{2}}{d x}\right) \lambda=\left(\frac{1}{l_{1}}+\frac{1}{l_{2}}\right) \lambda
$$
where $I_{1}=\gamma-a$ et $I_{2}=b-\gamma$.
\begin{center}
\includegraphics[scale = 0.3]{2023_12_08_07eeb097c3e16596478fg-045}
\end{center}

\section*{Equivalence Between the DD Schemes and Classical Iterative Methods}
The preconditioned Richardson method is an iterative technique for solving linear systems $Ax = b$, enhanced with a preconditioning operator $P$. It is formulated as:
\begin{equation}
    P(x^{(k+1)} - x^{(k)}) = \theta (b - Ax^{(k)})
\end{equation}
where $x^{(k)}$ is the $k$-th iteration approximation, and $\theta$ is a relaxation parameter.

In the context of numerical methods for partial differential equations, specifically domain decomposition (DD) methods, this concept is adapted as follows:

\begin{itemize}
    \item \textbf{DN Method:} The Dirichlet-Neumann (DN) method uses a preconditioning operator $P_{DN} = S_2 = \frac{\partial(H_2\mu)}{\partial n_2}$. The method's iterative scheme can be seen as a variant of the preconditioned Richardson method:
    \begin{equation}
        P_{DN}(\lambda^{(k)} - \lambda^{(k-1)}) = \theta(\chi - S\lambda^{(k-1)})
    \end{equation}
    where $\lambda^{(k)}$ represents the interface variable in the $k$-th iteration.

    \item \textbf{NN Method:} The Neumann-Neumann (NN) method is characterized by a different preconditioning operator $P_{NN} = (\sigma_1 S_1^{-1} + \sigma_2 S_2^{-1})^{-1}$. Its equivalence to the preconditioned Richardson method is reflected in its iterative formula:
    \begin{equation}
        P_{NN}(\lambda^{(k)} - \lambda^{(k-1)}) = \theta(\chi - S\lambda^{(k-1)})
    \end{equation}
\end{itemize}

These formulations illustrate that both DN and NN methods in domain decomposition can be viewed as specific applications of the preconditioned Richardson method, where the preconditioning operators and iterative schemes are tailored to suit the challenges of solving PDEs in subdivided domains.

\section{FEM: Multi-Domain Formulation}
Consider the Poisson problem:
$$
\left\{\begin{aligned}
-\triangle u=f & \text { in } \Omega \\
u=0 & \text { on } \partial \Omega
\end{aligned}\right.
$$
Its weak formulation reads
$$
\text { find } u \in V: \quad a(u, v)=F(v) \quad \forall v \in V
$$
where $V=H_{0}^{1}(\Omega)$,
$$
a(v, w)=\int_{\Omega} \nabla v \cdot \nabla w \quad \forall v, w \in V
$$
and
$$
F(v)=\int_{\Omega} f v \quad \forall v \in V
$$
Suppose that $\Omega$ is split into two nonoverlapping subdomains and consider a uniform triangulation $\mathcal{T}_{h}$ of $\Omega$, conforming on $\Gamma$ :

\begin{center}
\includegraphics[scale = 0.3]{2023_12_08_07eeb097c3e16596478fg-056}
\end{center}
The Galerkin finite element approximation of the Poisson problem reads:
\begin{equation}
    \text { find } u_{h} \in V_{h}: \quad a\left(u_{h}, v_{h}\right)=F\left(v_{h}\right) \quad \forall v_{h} \in V_{h}
\end{equation}
where
$$
V_{h}=\left\{v_{h} \in C^{0}(\bar{\Omega}): v_{\left.h\right|_{K}} \in \mathbb{P}_{r} \quad r \geq 1, \forall K \in \mathcal{T}_{h}, v_{h}=0 \text { on } \partial \Omega\right\}
$$
is the space of finite element functions of degree $r$ with basis $\left\{\varphi_{i}\right\}_{i=1}^{N_{h}}$.
The Galerkin approximation (1) is equivalent to:
\begin{equation}
\text { find } u_{h} \in V_{h}: \quad a\left(u_{h}, \varphi_{i}\right)=F\left(\varphi_{i}\right) \quad \forall i=1, \ldots, N_{h} \text {. }
\end{equation}
We partition the nodes of the triangulation as follows:
\begin{enumerate}
  \item $\left\{x_{j}^{(1)}, 1 \leq j \leq N_{1}\right\}$ nodes in subdomain $\Omega_{1}$
  \item $\left\{x_{j}^{(2)}, 1 \leq j \leq N_{2}\right\}$ nodes in subdomain $\Omega_{2}$
  \item $\left\{x_{j}^{(\Gamma)}, 1 \leq j \leq N_{\Gamma}\right\}$ nodes on the interface $\Gamma$,
\end{enumerate}
and we split the basis functions accordingly:
\begin{enumerate}
  \item $\varphi_{j}^{(1)}$ functions associated to the nodes $x_{j}^{(1)}$
  \item $\varphi_{j}^{(2)}$ functions associated to the nodes $x_{j}^{(2)}$
  \item $\varphi_{j}^{(\ulcorner)}$functions associated to the nodes $x_{j}^{(\Gamma)}$ on the interface.
\end{enumerate}

\subsection*{Example}
\begin{center}
\includegraphics[scale = 0.3]{2023_12_08_07eeb097c3e16596478fg-059}
\end{center}
Problem (2) can be equivalently rewritten as:
\begin{equation}
\begin{aligned}
\text { find } u_{h} \in V_{h}: \begin{cases}a\left(u_{h}, \varphi_{i}^{(1)}\right)=F\left(\varphi_{i}^{(1)}\right) & \forall i=1, \ldots, N_{1} \\ a\left(u_{h}, \varphi_{j}^{(2)}\right)=F\left(\varphi_{j}^{(2)}\right) & \forall j=1, \ldots, N_{2} \\ a\left(u_{h}, \varphi_{k}^{(\Gamma)}\right)=F\left(\varphi_{k}^{(\Gamma)}\right) & \forall k=1, \ldots, N_{\Gamma}\end{cases}
\end{aligned}
\end{equation}
We introduce the bilinear form on $\Omega_{i}$ :
$$
a_{i}(v, w)=\int_{\Omega_{i}} \nabla v \cdot \nabla w \quad \forall v, w \in V, i=1,2
$$
the space
$$
V_{h}^{i}=\left\{v \in H^{1}\left(\Omega_{i}\right): v=0 \text { on } \partial \Omega_{i} \backslash \Gamma\right\} \quad(i=1,2)
$$
and let $u_{h}^{(i)}=u_{h \mid \Omega_{i}}(i=1,2)$.
Then, problem (3) can be written equivalently as:
find $u_{h}^{(1)} \in V_{h}^{1}, u_{h}^{(2)} \in V_{h}^{2}$ s.t.
\begin{equation}
\left\{
\begin{aligned}
a_{1}\left(u_{h}^{(1)}, \varphi_{i}^{(1)}\right) = F_{1}\left(\varphi_{i}^{(1)}\right), & \quad \forall i=1, \ldots, N_{1} \\
a_{2}\left(u_{h}^{(2)}, \varphi_{j}^{(2)}\right) = F_{2}\left(\varphi_{j}^{(2)}\right), & \quad \forall j=1, \ldots, N_{2} \\
a_{1}\left(u_{h}^{(1)}, \varphi_{k}^{(\Gamma)} \mid \Omega_{1}\right) + a_{2}\left(u_{h}^{(2)}, \varphi_{k}^{(\Gamma)} \mid \Omega_{2}\right) = \\ =F_{1}\left(\varphi_{k}^{(\Gamma)} \mid \Omega_{1}\right) + F_{2}\left(\varphi_{k}^{(\Gamma)} \mid \Omega_{2}\right), & \quad \forall k=1, \ldots, N_{\Gamma}
\end{aligned}
\right.
\end{equation}


\subsection*{Remark}
\begin{itemize}
  \item Problem (3.7) corresponds to the finite element approximation of the multi-domain formulation of the Poisson problem:
\end{itemize}
$$
\begin{cases}-\triangle u_{1}=f & \text { in } \Omega_{1} \\ u_{1}=0 & \text { on } \partial \Omega_{1} \backslash \Gamma \\ -\triangle u_{2}=f & \text { in } \Omega_{2} \\ u_{2}=0 & \text { on } \partial \Omega_{2} \backslash \Gamma \\ u_{1}=u_{2} & \text { on } \Gamma \\ \frac{\partial u_{1}}{\partial n}=\frac{\partial u_{2}}{\partial n} & \text { on } \Gamma .\end{cases}
$$
\begin{itemize}
  \item The condition $u_{1}=u_{2}$ on $\Gamma$ is satisfied by definition of $u_{h}^{(i)}$.
\end{itemize}
We can write:
$$
\begin{aligned}
u_{h}(x)= & \sum_{j=1}^{N_{1}} u_{h}\left(x_{j}^{(1)}\right) \varphi_{j}^{(1)}(x)+\sum_{j=1}^{N_{2}} u_{h}\left(x_{j}^{(2)}\right) \varphi_{j}^{(2)}(x) \\
& +\sum_{j=1}^{N_{\Gamma}} u_{h}\left(x_{j}^{(\Gamma)}\right) \varphi_{j}^{(\Gamma)}(x)
\end{aligned}
$$
and we substitute this expression in (3.7) to obtain:
$$
\begin{cases}\sum_{j=1}^{N_{1}} u_{h}\left(x_{j}^{(1)}\right) a_{1}\left(\varphi_{j}^{(1)}, \varphi_{i}^{(1)}\right)+\sum_{j=1}^{N_{\Gamma}} u_{h}\left(x_{j}^{(\Gamma)}\right) a_{1}\left(\varphi_{j}^{(\Gamma)}, \varphi_{i}^{(1)}\right)=F_{1}\left(\varphi_{i}^{(1)}\right) & \forall i=1, \ldots, N_{1} \\ \sum_{j=1}^{N_{2}} u_{h}\left(x_{j}^{(2)}\right) a_{2}\left(\varphi_{j}^{(2)}, \varphi_{i}^{(2)}\right)+\sum_{j=1}^{N_{\Gamma}} u_{h}\left(x_{j}^{(\Gamma)}\right) a_{2}\left(\varphi_{j}^{(\Gamma)}, \varphi_{i}^{(2)}\right)=F_{2}\left(\varphi_{i}^{(2)}\right) & \forall i=1, \ldots, N_{2} \\ \sum_{j=1}^{N_{\Gamma}} u_{h}\left(x_{j}^{(\Gamma)}\right)\left[a_{1}\left(\varphi_{j}^{(\Gamma)}, \varphi_{i}^{(\Gamma)}\right)+a_{2}\left(\varphi_{j}^{(\Gamma)}, \varphi_{i}^{(\Gamma)}\right)\right] & \\ \quad+\sum_{j=1}^{N_{1}} u_{h}\left(x_{j}^{(1)}\right) a_{1}\left(\varphi_{j}^{(1)}, \varphi_{i}^{(\Gamma)}\right)+\sum_{j=1}^{N_{2}} u_{h}\left(x_{j}^{(2)}\right) a_{2}\left(\varphi_{j}^{(2)}, \varphi_{i}^{(\Gamma)}\right) & \\ \quad=F_{1}\left(\left.\varphi_{i}^{(\Gamma)}\right|_{\Omega_{1}}\right)+F_{2}\left(\left.\varphi_{i}^{(\Gamma)}\right|_{\Omega_{2}}\right) & \forall i=1, \ldots, N_{\Gamma} .\end{cases}
$$

A bit of algebra...

$$
\begin{cases}\sum_{j=1}^{N_{1}} u_{h}\left(x_{j}^{(1)}\right)\left(A_{11}\right)_{i j}+\sum_{j=1}^{N_{\Gamma}} u_{h}\left(x_{j}^{(\Gamma)}\right)\left(A_{1 \Gamma}\right)_{i j}=F_{1}\left(\varphi_{i}^{(1)}\right) & \forall i=1, \ldots, N_{1} \\ \sum_{j=1}^{N_{2}} u_{h}\left(x_{j}^{(2)}\right)\left(A_{22}\right)_{i j}+\sum_{j=1}^{N_{\Gamma}} u_{h}\left(x_{j}^{(\Gamma)}\right)\left(A_{2 \Gamma}\right)_{i j}=F_{2}\left(\varphi_{i}^{(2)}\right) & \forall i=1, \ldots, N_{2} \\ \sum_{j=1}^{N_{\Gamma}} u_{h}\left(x_{j}^{(\Gamma)}\right)\left[\left(A_{\Gamma \Gamma}^{(1)}\right)_{i j}+\left(A_{\Gamma \Gamma}^{(2)}\right)_{i j}\right] & \\ \quad+\sum_{j=1}^{N_{1}} u_{h}\left(x_{j}^{(1)}\right)\left(A_{\Gamma 1}\right)_{i j}+\sum_{j=1}^{N_{2}} u_{h}\left(x_{j}^{(2)}\right)\left(A_{\Gamma 2}\right)_{i j} & \\ \quad=F_{1}\left(\varphi_{i}^{(\Gamma)}_{\mid \Omega_{1}}\right)+F_{2}\left(\varphi_{i}^{(\Gamma)}_{\mid \Omega_{2}}\right) & \forall i=1, \ldots, N_{\Gamma} .\end{cases}
$$

A bit of algebra... [continued]

$$
\left\{\begin{aligned}
\sum_{j=1}^{N_{1}} \mathbf{u}_{1}\left(A_{11}\right)_{i j}+\sum_{j=1}^{N_{\Gamma}} \mathbf{u}_{\Gamma}\left(A_{1 \Gamma}\right)_{i j}=\mathbf{f}_{1} & \forall i=1, \ldots, N_{1} \\
\sum_{j=1}^{N_{2}} \mathbf{u}_{2}\left(A_{22}\right)_{i j}+\sum_{j=1}^{N_{\Gamma}} \mathbf{u}_{\Gamma}\left(A_{2 \Gamma}\right)_{i j}=\mathbf{f}_{2} & \forall i=1, \ldots, N_{2} \\
\sum_{j=1}^{N_{\Gamma}} \mathbf{u}_{\Gamma}\left[\left(A_{\Gamma \Gamma}^{(1)}\right)_{i j}+\left(A_{\Gamma \Gamma}^{(2)}\right)_{i j}\right] & \\
+\sum_{j=1}^{N_{1}} \mathbf{u}_{1}\left(A_{\Gamma 1}\right)_{i j}+\sum_{j=1}^{N_{2}} \mathbf{u}_{2}\left(A_{\Gamma 2}\right)_{i j} & \\
=\mathbf{f}_{1}^{(\Gamma)}+\mathbf{f}_{2}^{(\Gamma)} & \forall i=1, \ldots, N_{\Gamma} .
\end{aligned}\right.
$$

... so that we obtain the algebraic form:

$$
\left\{\begin{array}{l}
A_{11} \mathbf{u}_{1}+A_{1 \Gamma} \boldsymbol{\lambda}=\mathbf{f}_{1} \\
A_{22} \mathbf{u}_{2}+A_{2 \Gamma} \boldsymbol{\lambda}=\mathbf{f}_{2} \\
A_{\Gamma 1} \mathbf{u}_{1}+A_{\Gamma 2} \mathbf{u}_{2}+\left(A_{\Gamma \Gamma}^{(1)}+A_{\Gamma \Gamma}^{(2)}\right) \boldsymbol{\lambda}=\mathbf{f}_{1}^{(\Gamma)}+\mathbf{f}_{2}^{(\Gamma)}
\end{array}\right.
$$

or

$$
\left(\begin{array}{ccc}
A_{11} & 0 & A_{1 \Gamma} \\
0 & A_{22} & A_{2 \Gamma} \\
A_{\Gamma 1} & A_{\Gamma 2} & A_{\Gamma \Gamma}
\end{array}\right)\left(\begin{array}{c}
\mathbf{u}_{1} \\
\mathbf{u}_{2} \\
\boldsymbol{\lambda}
\end{array}\right)=\left(\begin{array}{l}
\mathbf{f}_{1} \\
\mathbf{f}_{2} \\
\mathbf{f}_{\Gamma}
\end{array}\right)
$$

where we denoted $A_{\Gamma \Gamma}=\left(A_{\Gamma \Gamma}^{(1)}+A_{\Gamma \Gamma}^{(2)}\right)$ and $\mathbf{f}_{\Gamma}=\mathbf{f}_{1}^{(\Gamma)}+\mathbf{f}_{2}^{(\Gamma)}$.

\section{The Schur Complement System}
Since $\lambda$ represents the unknown value of $u$ on $\Gamma$, its finite element correspondent is the vector $\boldsymbol{\lambda}$ of the values of $u_{h}$ at the interface nodes. By Gaussian elimination, we can obtain a new reduced system in the sole unknown $\boldsymbol{\lambda}$:
\begin{itemize}
  \item Matrices $A_{11}$ and $A_{22}$ are invertible since they are associated with two homogeneous Dirichlet boundary-value problems for the Laplace operator:
\end{itemize}
\begin{equation}
\mathbf{u}_{1} = A_{11}^{-1}(\mathbf{f}_{1} - A_{1 \Gamma} \boldsymbol{\lambda}) \quad \text{and} \quad \mathbf{u}_{2} = A_{22}^{-1}(\mathbf{f}_{2} - A_{2 \Gamma} \boldsymbol{\lambda}).
\end{equation}
\begin{itemize}
  \item From the third equation we obtain:
\end{itemize}
\begin{equation*}
\begin{aligned}
& \left[\left(A_{\Gamma \Gamma}^{(1)} - A_{\Gamma 1} A_{11}^{-1} A_{1 \Gamma}\right) + \left(A_{\Gamma \Gamma}^{(2)} - A_{\Gamma 2} A_{22}^{-1} A_{2 \Gamma}\right)\right] \boldsymbol{\lambda} = \\
& \mathbf{f}_{\Gamma} - A_{\Gamma 1} A_{11}^{-1} \mathbf{f}_{1} - A_{\Gamma 2} A_{22}^{-1} \mathbf{f}_{2}.
\end{aligned}
\end{equation*}
Setting:
\begin{equation*}
\Sigma = \Sigma_{1} + \Sigma_{2} \quad \text{with} \quad \Sigma_{i} = A_{\Gamma \Gamma}^{(i)} - A_{\Gamma i} A_{i i}^{-1} A_{i \Gamma} \quad (i=1,2),
\end{equation*}
and
\begin{equation*}
\chi_{\Gamma} = \mathbf{f}_{\Gamma} - A_{\Gamma 1} A_{11}^{-1} \mathbf{f}_{1} - A_{\Gamma 2} A_{22}^{-1} \mathbf{f}_{2},
\end{equation*}
we obtain the Schur complement system:
\begin{equation*}
\Sigma \lambda = \chi_{\ulcorner}.
\end{equation*}
\begin{itemize}
  \item $\Sigma$ and $\chi_{\ulcorner}$ approximate $S$ and $\chi$.
  \item $\Sigma$ is the so-called Schur complement of $A$ with respect to $\mathbf{u}_{1}$ and $\mathbf{u}_{2}$.
  \item $\Sigma_{i}$ are the Schur complements related to the subdomains $\Omega_{i}$ $(i=1,2)$.
\end{itemize}

\subsection*{Remark}
After solving the Schur complement system in $\boldsymbol{\lambda}$, thanks to (3.8) we can compute $\mathbf{u}_{1}$ and $\mathbf{u}_{2}$.
This is equivalent to solving two Poisson problems in the subdomains $\Omega_{1}$ and $\Omega_{2}$ with the Dirichlet boundary condition $u_{h}^{(i)}{ }_{\mid r}=\lambda_{h}(i=1,2)$ on the interface $\Gamma$.

\subsection*{Properties of the Schur Complement $\Sigma$}
The Schur complement $\Sigma$ inherits some of the properties of $A$ :
\begin{itemize}
  \item if $A$ is singular, so is $\Sigma$;
  \item if $A$ (respectively, $A_{i i}$ ) is symmetric, then $\Sigma$ (respectively, $\Sigma_{i}$ ) is symmetric too;
  \item if $A$ is positive definite, so is $\Sigma$.
\end{itemize}

Moreover, concerning the condition number, we have
\begin{itemize}
  \item $\kappa(A) \simeq C h^{-2}$
  \item $\kappa(\Sigma) \simeq C h^{-1}$
\end{itemize}

\subsection*{Preconditioners for the Schur Complement System}
The iterative methods that we have illustrated are equivalent to preconditioned Richardson methods for the Schur complement system with preconditioners:

\begin{itemize}
\item for the DN algorithm: $P_h=\Sigma_2$
\item for the ND algorithm: $P_h=\Sigma_1$
\item for the NN algorithm: $P_h=\left(\sigma_1 \Sigma_1^{-1}+\sigma_2 \Sigma_2^{-1}\right)^{-1}$
\item for the RR algorithm: $P_h=\left(\gamma_1+\gamma_2\right)^{-1}\left(\gamma_1 I+\Sigma_1\right)\left(\gamma_2 I+\Sigma_2\right)$
\end{itemize}
All these preconditioners are optimal in the sense of the following definition. We must anticipate that these optimality results do not hold in the case of multiple subdomains.
\\ \\
\textbf{Definition} \\ \\
A preconditioner $P$ is optimal for a matrix $A \in \mathbb{R}^{N \times N}$ if the condition number of $P^{-1} A$ is bounded uniformly with respect to the dimension $N$ of $A$.

In particular, we have

\begin{itemize}
\item $\kappa\left(\Sigma_i^{-1} \Sigma\right)=O(1)$ (for $i=1,2$)
\item $\kappa\left(\left(\sigma_1 \Sigma_1^{-1}+\sigma_2 \Sigma_2^{-1}\right) \Sigma\right)=O(1) \forall \sigma_1, \sigma_2>0$
\end{itemize}


\section{Nonoverlapping Multiple Subdomains}
\subsection*{Multi-Domain Formulation for $M>2$ Subdomains}
We generalize now the nonoverlapping methods to the case of a domain $\Omega$ split into $M>2$ subdomains:

\begin{itemize}
  \item $\Omega_{i}(i=1, \ldots, M)$ such that $\bigcup \bar{\Omega}_{i}=\bar{\Omega}$
  \item $\Gamma_{i}=\partial \Omega_{i} \backslash \partial \Omega$
  \item $\Gamma=\bigcup \Gamma_{i}$.
\end{itemize}
\includegraphics[max width=\textwidth, center]{2023_12_08_07eeb097c3e16596478fg-085}
At the differential level, we have the equivalent multi-domain formulation:
$$
\left\{\begin{array} { l l } 
{ - \triangle u = f } & { \text { in } \Omega } \\
{ u = 0 } & { \text { on } \partial \Omega }
\end{array} \Leftrightarrow \left\{\begin{array}{ll}
-\triangle u_{i}=f & \text { in } \Omega_{i} \\
u_{i}=u_{k} & \text { on } \Gamma_{i k} \\
\frac{\partial u_{i}}{\partial n_{i}}=\frac{\partial u_{k}}{\partial n_{i}} & \text { on } \Gamma_{i k} \\
u_{i}=0 & \text { on } \partial \Omega_{i} \cap \partial \Omega
\end{array}\right.\right.
$$

where $\Gamma_{i k}=\partial \Omega_{i} \cap \partial \Omega_{k} \neq \emptyset$.

\subsection*{Finite Element Approximation}
Considering a conforming finite element approximation, we obtain the linear system:
\begin{equation}
\left(
\begin{array}{cc}
A_{II} & A_{I\Gamma} \\
A_{\Gamma I} & A_{\Gamma \Gamma}
\end{array}
\right)
\left(
\begin{array}{c}
\mathbf{u}_{I} \\
\boldsymbol{\lambda}
\end{array}
\right) =
\left(
\begin{array}{c}
\mathbf{f}_{I} \\
\mathbf{f}_{\Gamma}
\end{array}
\right)
\end{equation}
where $\mathbf{u}_{I}$ is the vector of unknowns in the internal nodes and $\boldsymbol{\lambda}$ is the vector of unknowns on $\Gamma$: $\boldsymbol{\lambda} = \mathbf{u}_{\Gamma}$.\\
The submatrix $A_{II}$ associated with the internal nodes is block-diagonal:
\begin{equation}
A_{II} = 
\left(
\begin{array}{cccc}
A_{11} & 0 & \ldots & 0 \\
0 & \ddots & & \vdots \\
\vdots & & \ddots & 0 \\
0 & \ldots & 0 & A_{MM}
\end{array}
\right)
\end{equation}\\
$A_{I\Gamma}$ is a banded matrix (interactions with local interfaces).

\subsection*{Remark}
On each subdomain $\Omega_{i}$, the matrix
\begin{equation}
A_{i} = 
\left(
\begin{array}{cc}
A_{ii} & A_{i\Gamma} \\
A_{\Gamma i} & A_{\Gamma_{i}\Gamma_{i}}
\end{array}
\right)
\end{equation}
represents the local stiffness matrix associated with a Neumann problem.

\subsection*{The Schur Complement System}

\begin{align*}
    A_{II} u_I + A_{I\Gamma} \lambda = &f_I \quad  \\
    A_{\Gamma I} u_I + A_{\Gamma\Gamma} \lambda =& f_\Gamma \quad  \\ 
    \text{These two equations imply:}\\
    u_I = &A_{II}^{-1}(f_I - A_{I\Gamma} \lambda)   \\
    A_{\Gamma I} (A_{II}^{-1} (f_I - A_{I\Gamma} \lambda)) + A_{\Gamma\Gamma} \lambda =& f_\Gamma \\
    A_{\Gamma I} A_{II}^{-1} f_I - A_{\Gamma I} A_{II}^{-1} A_{I\Gamma} \lambda + A_{\Gamma\Gamma} \lambda =& f_\Gamma \\
    (A_{\Gamma\Gamma} - A_{\Gamma I} A_{II}^{-1} A_{I\Gamma}) \lambda = &f_\Gamma - A_{\Gamma I} A_{II}^{-1} f_I \\
\end{align*}
Denoting
\begin{equation*}
\Sigma = A_{\Gamma\Gamma} - A_{\Gamma I} A_{II}^{-1} A_{I\Gamma}
\end{equation*}
and
\begin{equation*}
\chi_{\Gamma} = \mathbf{f}_{\Gamma} - A_{\Gamma I} A_{II}^{-1} \mathbf{f}_{I},
\end{equation*}
we obtain the Schur complement system in the multi-domain case:
\begin{equation*}
\Sigma \boldsymbol{\lambda} = \chi_{\Gamma}.
\end{equation*}


\subsection*{Remarks}
The local Schur complements are defined as:
\begin{equation*}
    \Sigma_{i} = A_{\Gamma_{i} \Gamma_{i}} - A_{\Gamma_{i} i} A_{i i}^{-1} A_{i \Gamma_{i}}
\end{equation*}

so that:

\begin{equation*}
    \Sigma = \Sigma_{1} + \ldots + \Sigma_{M}
\end{equation*}

\subsection*{A Simple Algorithm}
To compute a Finite Element (FE) approximation of the solution \( u \) of the Poisson problem:
\begin{equation*}
\begin{cases}
    -\triangle u = f & \text{in } \Omega \\
    u = 0 & \text{on } \partial \Omega
\end{cases}
\end{equation*}
we can follow these steps:
\begin{enumerate}
    \item Solve the Schur complement system
    \begin{equation*}
        \Sigma \lambda = \chi_{\Gamma}
    \end{equation*}
    to compute \( \boldsymbol{\lambda} \) on the whole interface \( \Gamma \);
    \item Solve
    \begin{equation*}
        A_{II} \mathbf{u}_{I} = \mathbf{f}_{I} - A_{|\Gamma} \lambda
    \end{equation*}
    i.e., \( M \) independent problems of reduced dimension
    \begin{equation*}
        A_{ii} \mathbf{u}_{l}^{i} = \mathbf{g}^{i} \quad (i=1, \ldots, M)
    \end{equation*}
    possibly in parallel.
\end{enumerate}





\subsection*{Estimate of the Condition Number}
The following estimate can be proved for the condition number of the Schur complement matrix \( \Sigma \):

There exists a constant \( C > 0 \), independent of \( h \) and \( H \), such that:
\begin{equation*}
    \kappa(\Sigma) \leq C \frac{H}{h H_{\min}^{2}}
\end{equation*}
where \( H \) and \( H_{\min} \) are the maximal and minimal diameters of the subdomains, respectively. \\
To elaborate further, the diameter of each subdomain \( \Omega_{k} \) is denoted as \( H_{k} \), defined by:
\begin{equation*}
    H_{k} = \operatorname{diam}(\Omega_{k})
\end{equation*}
We can also express the condition number of a  matrix \( \Sigma \) as:
\begin{equation*}
    \operatorname{cond}(\Sigma) \simeq O\left(\frac{H_{\text{max}}}{h H_{\text{min}}^{2}}\right)
\end{equation*}

In the specific case where \( H_{k} \) approximates \( H \), the condition number of \( \Sigma \) is approximated as:
\begin{equation*}
    \operatorname{cond}(\Sigma_{1}) \simeq O\left(\frac{1}{h H}\right)
\end{equation*}

Considering a scenario with two subdomains, we have:
\begin{align*}
    H & \cong \frac{1}{2} \operatorname{diam}(\Omega) \\
    \operatorname{cond}(\Sigma) & \simeq O\left(\frac{1}{h}\right)
\end{align*}
When solving \( \sum \vec{\lambda} = \vec{X}_{\Gamma}  \) using the Richardson method without preconditioning, the convergence rate \( \rho \) is given by:
\begin{equation*}
    \rho = \frac{\operatorname{cond}(\Sigma) - 1}{\operatorname{cond}(\Sigma) + 1}
\end{equation*}
\begin{equation*}
    \Rightarrow \quad \rho = \rho\left(h^{-1}, H^{-1}\right)
\end{equation*}

This result indicates that the method is not optimal or scalable, as evidenced by the fact that increasing \( M \) leads to a decrease in \( H \).

Therefore, to address these limitations, a parallel preconditioner is necessary. The ideal preconditioner should be both optimal and scalable.

% \subsection*{Additional remarks}
% The Schur complement matrix, denoted by $\Sigma$, is characterized by its dense structure. 
% In the context of iterative solvers applied to the Schur complement system
% \[
% \Sigma \lambda = \chi_{\Gamma}
% \]
% it becomes evident that computing the elements of $\Sigma$ explicitly is not memory-efficient. Therefore, this approach is not recommended from a memory usage perspective.

% Instead, the focus should be on computing the product $\Sigma \mathbf{x}_{\Gamma}$, where $\mathbf{x}_{\Gamma}$ represents any given vector. 
% Considering two subdomains:
% \begin{itemize}
%   \item Apply the Dirichlet-Neumann (DN) method.
%   \item Employ relaxation on \( A = \Sigma_1 + \Sigma_2 \).
%   \item Implement Finite Element (FE) splitting in \( \Omega \).
% \end{itemize}

% Given \( A \) is symmetric positive definite (spd), then \( \Sigma_1 \) and \( \Sigma_2 \) are spd.

% For the Richardson method (R):
% \[ \Sigma_2 (\chi_{\Gamma}^{k+1} - \chi_{\Gamma}^{k}) = \theta_{k} (\chi_{\Gamma} - \Sigma_1 \chi_{\Gamma}^{k}) \quad \text{for} \quad k = 0, 1, \ldots \]

% This leads to:
% \begin{itemize}
%   \item Richardson (R)
%   \item Basic Gradient Method (G)
%   \item Conjugate Gradient Method (CG)
% \end{itemize}

% The Conjugate Gradient (CG) method outperforms both Richardson (R) and Basic Gradient (G) methods.

% The convergence rate for CG is:
% \[ \rho_{CG} = \sqrt{\frac{\operatorname{cond}(\Sigma_{2}^{-1}\Sigma_{1}) - 1}{\operatorname{cond}(\Sigma_{2}^{-1}\Sigma_{1}) + 1}} \]
% which is significantly less than
% \[ \operatorname{cond}(\Sigma_{2}^{-1}\Sigma_{1}) = \rho_{G} \]

% \textbf{Suggestion}:
% Consider substituting the Richardson method (R) with the Conjugate Gradient (CG) method while maintaining the same preconditioner.

% $$
% \rho_{C G}=\frac{\sqrt{\operatorname{cond}\left(\Sigma_{2}^{-1} \Sigma\right)-1}}{\sqrt{\operatorname{cond}\left(\Sigma_{2}^{-1} \Sigma\right)}+1} \ll \frac{\operatorname{cond}\left(\Sigma_{2}^{-1} \Sigma\right)-1}{\operatorname{cond}\left(\Sigma_{2}^{-1} \Sigma\right)+1}=\rho_{G}
% $$
% \textbf{Idea:}
% Why not replace (R) by the (CG) method with the same preconditioner?
% For example, in the conjugate gradient method:\\ \\
% \begin{tabular}{l}
%   Choose a starting solution $\lambda_{0}$. \\
%   Compute $\mathbf{r}_{0} = \chi_{\Gamma} - \Sigma \lambda_{0}$, set $\mathbf{p}_{0} = \mathbf{r}_{0}$. \\
%   For $k \geq 0$ until convergence, do: \\
%   $\quad \alpha_{k} = \frac{(\mathbf{p}_{k}, \mathbf{r}_{k})}{(\Sigma \mathbf{p}_{k}, \mathbf{p}_{k})}$. \\
%   $\quad \lambda_{k+1} = \lambda_{k} + \alpha_{k} \mathbf{p}_{k}$. \\
%   $\quad \mathbf{r}_{k+1} = \mathbf{r}_{k} - \alpha_{k} \Sigma \mathbf{p}_{k}$. \\
%   $\quad \beta_{k} = \frac{(\Sigma \mathbf{p}_{k}, \mathbf{r}_{k+1})}{(\Sigma \mathbf{p}_{k}, \mathbf{p}_{k})}$. \\
%   $\quad \mathbf{p}_{k+1} = \mathbf{r}_{k+1} + \beta_{k} \mathbf{p}_{k}$. \\ 
%   End For
% \end{tabular}
% \\ \\
% Or, in the preconditioned conjugate gradient method, denoting by $P$ the preconditioning matrix:\\ \\
% \begin{tabular}{l}
%   Choose a starting solution $\lambda_{0}$. \\
%   Compute $\mathbf{r}_{0} = \chi_{\Gamma} - \Sigma \lambda_{0}$, $\mathbf{z}_{0} = P^{-1} \mathbf{r}_{0}$, set $\mathbf{p}_{0} = \mathbf{z}_{0}$. \\
%   For $k \geq 0$ until convergence, do: \\
%   $\quad \alpha_{k} = \frac{(\mathbf{p}_{k}, \mathbf{r}_{k})}{(\Sigma \mathbf{p}_{k}, \mathbf{p}_{k})}$. \\
%   $\quad \lambda_{k+1} = \lambda_{k} + \alpha_{k} \mathbf{p}_{k}$. \\
%   $\quad \mathbf{r}_{k+1} = \mathbf{r}_{k} - \alpha_{k} \Sigma \mathbf{p}_{k}$. \\
%   $\quad \mathbf{z}_{k+1} = P^{-1} \mathbf{r}_{k+1}$. \\
%   $\quad \beta_{k} = \frac{(\Sigma \mathbf{p}_{k}, \mathbf{z}_{k+1})}{(\Sigma \mathbf{p}_{k}, \mathbf{p}_{k})}$. \\
%   $\quad \mathbf{p}_{k+1} = \mathbf{z}_{k+1} - \beta_{k} \mathbf{p}_{k}$. \\
%   End For
% \end{tabular}


\subsection*{Scalability}
\textbf{Definition}
A preconditioner $P_{h}$ of $\Sigma$ is said to be scalable if the condition number of the preconditioned matrix $P_{h}^{-1} \Sigma$ is independent of the number of subdomains. \\

Iterative methods using scalable preconditioners allow henceforth to achieve convergence rates independent of the subdomain number.

\subsection*{Dirichlet-Neumann Preconditioner}
The Dirichlet-Neumann preconditioner is not widely used for many subdomains due to the necessity of a "black and white" subdivision. This subdivision requires alternating between Dirichlet (prescribed values) and Neumann (prescribed normal derivatives) boundary conditions on subdomain interfaces to ensure convergence. However, this strict subdivision can be challenging for complex geometries or numerous subdomains, leading to load imbalances and inefficiencies. As a result, alternative domain decomposition methods like the additive Schwarz or Balancing Neumann-Neumann techniques, which do not require this strict subdivision, are often preferred for problems with many subdomains or irregular decompositions. Anyway we can define the following preconditioning strategy, I will cite word for word the book by Quarteroni Valli Domain Decomposition Methods for Partial Differential Equations.

The Dirichlet-Neumann iterative substructuring method introduced for the differential boundary value problem  can be formulated at the algebraic level as follows. We use a superscript $B$ (black) and $W$ (white) to denote the colour of the subdomain. Then the Schur complement
matrix can be split as

$$
\Sigma_{h}=\sum_{i \in I_{B}}\left(R_{\Gamma_{i}}^{(B)}\right)^{T} \Sigma_{i, h}^{(B)} R_{\Gamma_{i}}^{(B)}+\sum_{i \in I_{W}}\left(R_{\Gamma_{i}}^{(W)}\right)^{T} \Sigma_{i, h}^{(W)} R_{\Gamma_{i}}^{(W)}
$$

and the Dirichlet-Neumann preconditioner is defined through

$$
\left(P_{h}^{\mathrm{DN}}\right)^{-1}:=\sum_{i \in I_{W}}\left(R_{\Gamma_{i}}^{(W)}\right)^{T}\left(\Sigma_{i, h}^{(W)}\right)^{-1} R_{\Gamma_{i}}^{(W)}
$$

Also for this preconditioner it is possible to include a global coarse problem.

\subsection*{Neumann-Neumann Preconditioner}
The Neumann-Neumann preconditioner for more subdomains reads:
$$
\left(P_h^{N N}\right)^{-1}=\sum_{i=1}^M R_{\Gamma_i}^T D_i \Sigma_i^* D_i R_{\Gamma_i}
$$
where $\Sigma_i^*$ is either $\Sigma_i^{-1}$ or an approximation of $\Sigma_i^{-1}$.
$D_i$ is a diagonal matrix of positive weights
$$
D_i=\left(\begin{array}{ccc}
d_1 & & \\
& \ddots & \\
& & d_n
\end{array}\right)
$$
$d_j$ is the number of subdomains that share the $j$-th node.
We have the following estimate:
$$
\kappa\left(\left(P_h^{N N}\right)^{-1} \Sigma\right) \leq C H^{-2}\left(1+\log \frac{H}{h}\right)^2
$$
The presence of $D_i$ and $R_{\Gamma}$ only entails matrix-matrix multiplications. On the other hand, if $\Sigma_i^*=\Sigma_i^{-1}$, applying $\Sigma_i^{-1}$ to a given vector can be reconducted to the use of local inverses. As a matter of fact, let $\mathbf{q}$ be a vector whose components are the nodal values on the local interface $\Gamma_i$; then
$$
\Sigma_i^{-1} \mathbf{q}=[0, I] A_i^{-1}[0, I]^T \mathbf{q} .
$$

In particular, $[0, I]^T \mathbf{q}=[0, \mathbf{q}]^T$, and the matrix-vector product
$$
\underbrace{\left[\begin{array}{c|c}
\text { internal } \\
\text { nodes } & \\
&
\end{array}\right]}_{A_i^{-1}}\left[\begin{array}{c}
0 \\
\vdots \\
0 \\
\hline \mathbf{q}
\end{array}\right]
$$
corresponds to the solution on $\Omega_i$ of the Neumann boundary-value problem:
$$
\begin{cases}-\Delta w_i=0 & \text { in } \Omega_i \\ \frac{\partial w_i}{\partial n}=q & \text { on } \Gamma_i\end{cases}
$$

In essence, the operation \( \Sigma_i^{-1} q \) (applying the inverse to vector \( q \)) can be interpreted as solving a Neumann boundary value problem in the subdomain \( \Omega_i \) where the boundary condition on \( \Gamma_i \) is given by the vector \( q \). This provides a way to apply the preconditioner by solving local PDE problems rather than explicitly inverting the matrix \( \Sigma_i \).

\subsection*{Balanced Neumann-Neumann Preconditioner}
The Neumann-Neumann preconditioner of the Schur complement system is not scalable. A substantial improvement can be achieved by adding a coarse grid correction:
$$
\left(P_{h}^{B N N}\right)^{-1}=\Sigma_{H}^{-1}+\left(I-\Sigma_{H}^{-1} \Sigma\right)\left(P_{h}^{N N}\right)^{-1}\left(I-\Sigma \Sigma_{H}^{-1}\right).
$$
where $\Sigma_{H}^{-1}=R_{\Gamma}^{T} A_{H}^{-1} R_{\Gamma}$. This is called balanced Neumann-Neumann preconditioner.
We can prove that:
$$
\kappa\left(\left(P_{h}^{B N N}\right)^{-1} \Sigma\right) \leq C\left(1+\log \frac{H}{h}\right)^{2}.
$$

If the original matrix was symmetric positive definite this is an almost (except for a  logarithmic term) scalable and optimal preconditioner, which can be used also with the conjugate gradient method if the original matrix satisfy the applicability conditions.


\subsection{Convergence Properties}

For the two-subdomain cases of the Laplace problem with homogeneous Dirichlet boundary conditions, both Dirichlet–Neumann and Neumann–Neumann iterations converge at a rate independent of h. The Neumann–Neumann algorithm has no particular advantage over the Dirichlet–Neumann algorithm (and actually the former requires more subdomain solves per iteration than the latter). The situation can, however, be different in the case of many subdomains.

\subsection*{The need of pseud-inverses in multiple-subdomains decomposition}

In a domain decomposition method, when the computational domain is divided into only two subdomains, each subdomain will indeed inherit part of the original problem's boundary. Typically, the original problem has Dirichlet boundary conditions (fixed values) on at least part of its boundary. Therefore, in a two-subdomain decomposition, each subdomain ends up with a portion of the boundary where Dirichlet conditions apply and the interface between the two subdomains, where Neumann conditions (or, more precisely, continuity conditions that can be mathematically similar to Neumann conditions) are enforced. This mixture of boundary conditions (Dirichlet on the outer boundary and Neumann-like on the interface) helps ensure that each subdomain problem remains well-posed, meaning that it has a unique solution.

However, when a domain is decomposed into more than two subdomains, especially in complex geometries or in 3D, some subdomains may end up without any part of the original boundary. This means that they are entirely enclosed by interfaces with other subdomains and thus are only subject to Neumann-like conditions derived from those interfaces. Such subdomains can lead to ill-posed problems because Neumann problems require additional conditions (like a fixed integral value of the solution over the domain) to ensure uniqueness of the solution. Without any Dirichlet boundary conditions (which help pin down the solution), these purely Neumann subdomains can suffer from indeterminacy issues (e.g., the solution might be determined only up to an additive constant).

The need for a pseudoinverse, rather than a straightforward matrix inverse, in these cases stems from this potential lack of well-posedness. So in this context, waht we indicated as inverse $\Sigma_{H}^{-1}$ must be interpreted as a regularized inverse of the origimal matrix, and not the strict inverse which might not even exist.

\section*{Concluding Remarks}
From the numerical results that we have presented, we can conclude with the following remarks:
\begin{itemize}
  \item Even if better conditioned with respect to $A, \Sigma$ is ill-conditioned, and therefore a suitable preconditioner must be applied.
  \item The Neumann-Neumann preconditioner can be satisfactorily applied using a moderate number of subdomains, while for larger $M$, $\kappa\left(\left(P_{h}^{N N}\right)^{-1} \Sigma\right)>\kappa(\Sigma)$.
  \item The balancing Neumann-Neumann preconditioner is almost optimally scalable and therefore recommended for partitions with a large number of subdomains.
\end{itemize}

\chapter{Navier Stokes}
Navier-Stokes equations describe the motion of a fluid with constant density $\rho$ in a domain $\Omega \subset \mathbb{R}^{d}$ (with $d=2,3$ ). They read as follows:
\begin{align}
\begin{cases}
\frac{\partial \mathbf{u}}{\partial t}-\operatorname{div}\left[\nu\left(\nabla \mathbf{u}+\nabla \mathbf{u}^{T}\right)\right]+(\mathbf{u} \cdot \nabla) \mathbf{u}+\nabla p &= \mathbf{f}, \quad \mathbf{x} \in \Omega, t>0 \\
\operatorname{div} \mathbf{u} = 0, & \hspace{1.2cm} \mathbf{x} \in \Omega, t>0
\end{cases}
\end{align}
where:
\begin{itemize}
  \item $\mathbf{u}$ is the fluid's velocity
  \item $p$ is the pressure divided by the density (which will simply be called "pressure")
  \item $\nu$ is the kinematic viscosity
  \item $\mathbf{f} \in \mathrm{L}^{2}\left(\mathbb{R}^{+} ;\left[\mathrm{L}^{2}(\Omega)\right]^{d}\right)$ is a forcing term per unit of mass
\end{itemize}

The first equation is that of conservation of linear momentum, the second one that of conservation of mass, which is also called the continuity equation.
\begin{itemize}
  \item The term $(\mathbf{u} \cdot \nabla) \mathbf{u}$ describes the process of convective transport.
  \item The term $-\operatorname{div}\left[\nu\left(\nabla \mathbf{u}+\nabla \mathbf{u}^{T}\right)\right]$ describes the molecular diffusion process.
\end{itemize}
When $\nu$ is constant, from the continuity equation we obtain:\\
$$
\operatorname{div}\left[\nu\left(\nabla \mathbf{u}+\nabla \mathbf{u}^{T}\right)\right]=\nu(\Delta \mathbf{u}+\nabla \operatorname{div} \mathbf{u})=\nu \Delta \mathbf{u}
$$
whence system (4.1) can be written in the equivalent form:
\begin{align}
\begin{cases}\frac{\partial \mathbf{u}}{\partial t}-\nu \Delta \mathbf{u}+(\mathbf{u} \cdot \nabla) \mathbf{u}+\nabla p=\mathbf{f}, & \mathbf{x} \in \Omega, t>0 \\ \operatorname{div} \mathbf{u}=0, & \mathbf{x} \in \Omega, t>0\end{cases}
\end{align}
which is the one that we will consider in the following.\\ \\
Equations (4.2) are often called incompressible Navier-Stokes equations. More in general, fluids satisfying the incompressibility condition  $ \operatorname{div}\mathbf{u}=0$ are said to be incompressible.\\ \\
Constant density fluids necessarily satisfy this condition, however there exist incompressible fluids featuring variable density (e.g., stratified fluids) that are governed by a different system of equations in which the density $\rho$ explicitly shows up.\\ \\
In order for problem (4.2) to be well posed it is necessary to assign the initial condition:
\begin{equation}
\mathbf{u}(\mathbf{x}, 0)=\mathbf{u}_{0}(\mathbf{x}) \quad \forall \mathbf{x} \in \Omega
\end{equation}

where $\mathbf{u}_{0}$ is a given divergence-free vector field, together with suitable boundary conditions, such as, e.g., $\forall t>0$,

\begin{equation}
\left\{\begin{array}{l}
\mathbf{u}(\mathbf{x}, t)=\varphi(\mathbf{x}, t) \quad \forall \mathbf{x} \in \Gamma_{D}, \\
\left(\nu \frac{\partial \mathbf{u}}{\partial \mathbf{n}}-p \mathbf{n}\right)(\mathbf{x}, t)=\psi(\mathbf{x}, t) \quad \forall \mathbf{x} \in \Gamma_{N},
\end{array}\right.
\end{equation}

where $\varphi$ and $\psi$ are given vector functions, while $\Gamma_{D}$ and $\Gamma_{N}$ provide a partition of the domain boundary $\partial \Omega$, that is $\Gamma_{D} \cup \Gamma_{N}=\partial \Omega, \Gamma_{D}^{\circ} \cap \Gamma_{N}^{\circ}=\emptyset$. 

Finally, as usual $\mathbf{n}$ is the outward unit normal vector to $\partial \Omega$.\\ \\
If we use the alternative formulation (4.1), the second equation in (4.4) must be replaced by:
$$
\left[\nu\left(\nabla \mathbf{u}+\nabla \mathbf{u}^{T}\right) \mathbf{n}-p \mathbf{n}\right](\mathbf{x}, t)=\psi(\mathbf{x}, t) \quad \forall \mathbf{x} \in \Gamma_{N}
$$

Denoting with $u_{i}, i=1, \ldots, d$ the components of the vector $\mathbf{u}$ with respect to a Cartesian frame, and with $f_{i}$ the components of $\mathbf{f}$, system (4.2) can be written componentwise as

$$
\left\{\begin{array}{l}
\frac{\partial u_{i}}{\partial t}-\nu \Delta u_{i}+\sum_{j=1}^{d} u_{j} \frac{\partial u_{i}}{\partial x_{j}}+\frac{\partial p}{\partial x_{i}}=f_{i}, \quad i=1, \ldots, d \\
\sum_{j=1}^{d} \frac{\partial u_{j}}{\partial x_{j}}=0
\end{array}\right.
$$

\section{Well-posedness}
In the two-dimensional case the Navier-Stokes equations with the boundary conditions previously indicated yield well-posed problems. This means that if all data (initial condition, forcing term, boundary data) are smooth enough, then the solution is continuous together with its derivatives and does not develop singularities in time.

Things may go differently in three dimensions, where existence and uniqueness of classical solutions have been proven only locally in time (that is for a sufficiently small time interval). In the following slides we will introduce the weak formulation of the Navier-Stokes equations, for which existence of a solution has been proven for all times. However, the issue of uniqueness (which is related to that of regularity) is still open, and is actually the central issue of Navier-Stokes theory.

\section{Alternative formulations}
The Navier-Stokes equations have been written in terms of the primitive variables $\mathbf{u}$ and $p$, but other sets of variables may be used, too. For instance, in the two-dimensional case it is common to see the vorticity $\omega$ and the streamfunction $\psi$, that are related to the velocity as follows:
$$
\omega=\operatorname{rot} \mathbf{u}=\frac{\partial u_{2}}{\partial x_{1}}-\frac{\partial u_{1}}{\partial x_{2}}, \quad \mathbf{u}=\left[\begin{array}{c}
\frac{\partial \psi}{\partial x_{2}} \\
-\frac{\partial \psi}{\partial x_{1}}
\end{array}\right]
$$

The various formulations are in fact equivalent from a mathematical standpoint, although they give rise to different numerical methods. 



\section{Weak formulation of Navier-Stokes equations}
A weak formulation of problem (4.2) can be obtained by proceeding formally, as follows. Let us multiply the first equation of (4.2) by a test function $\mathbf{v}$ belonging to a suitable space $V$ that will be specified later on, and integrate in $\Omega$

$$
\begin{aligned}
& \int_{\Omega} \frac{\partial \mathbf{u}}{\partial t} \cdot \mathbf{v} d \Omega-\int_{\Omega} \nu \Delta \mathbf{u} \cdot \mathbf{v} d \Omega+\int_{\Omega}[(\mathbf{u} \cdot \nabla) \mathbf{u}] \cdot \mathbf{v} d \Omega+\int_{\Omega} \nabla p \cdot \mathbf{v} d \Omega \\
& \quad=\int_{\Omega} \mathbf{f} \cdot \mathbf{v} d \Omega
\end{aligned}
$$

Using Green's formulae [ref. NMDP, 3.16 \& 3.17] we find:

$$
\begin{aligned}
-\int_{\Omega} \nu \Delta \mathbf{u} \cdot \mathbf{v} d \Omega & =\int_{\Omega} \nu \nabla \mathbf{u} \cdot \nabla \mathbf{v} d \Omega-\int_{\partial \Omega} \nu \frac{\partial \mathbf{u}}{\partial \mathbf{n}} \cdot \mathbf{v} d \gamma \\
\int_{\Omega} \nabla p \cdot \mathbf{v} d \Omega & =-\int_{\Omega} p \operatorname{div} \mathbf{v} d \Omega+\int_{\partial \Omega} p \mathbf{v} \cdot \mathbf{n} d \gamma
\end{aligned}
$$

Using these relations in the first of (4.2), we obtain:
\begin{equation}
\begin{aligned}
& \int_{\Omega} \frac{\partial \mathbf{u}}{\partial t} \cdot \mathbf{v} d \Omega+\int_{\Omega} \nu \nabla \mathbf{u} \cdot \nabla \mathbf{v} d \Omega+\int_{\Omega}[(\mathbf{u} \cdot \nabla) \mathbf{u}] \cdot \mathbf{v} d \Omega \\
& -\int_{\Omega} p \operatorname{div} \mathbf{v} d \Omega=\int_{\Omega} \mathbf{f} \cdot \mathbf{v} d \Omega+\int_{\partial \Omega}\left(\nu \frac{\partial \mathbf{u}}{\partial \mathbf{n}}-p \mathbf{n}\right) \cdot \mathbf{v} d \gamma \quad \forall \mathbf{v} \in V .
\end{aligned}
\end{equation}
\textbf{Remark} \\
All boundary integrals should indeed be regarded as duality pairings between $V^{\prime}$ and $V$. \\
Similarly, by multiplying the second equation of (4.2) by a test function $q$, belonging to a suitable space $Q$ to be specified, then integrating on $\Omega$ it follows:
\begin{equation}
\int_{\Omega} q \operatorname{div} \mathbf{u} \, d \Omega = 0 \quad \forall q \in Q .
\end{equation}
Customarily $V$ is chosen so that the test functions vanish on the boundary portion where a Dirichlet data is prescribed on $\mathbf{u}$, that is:
\begin{equation}
V=\left[\mathrm{H}_{\Gamma_{D}}^{1}(\Omega)\right]^{d}=\left\{\mathbf{v} \in\left[\mathrm{H}^{1}(\Omega)\right]^{d}:\left.\mathbf{v}\right|_{\Gamma_{D}}=\mathbf{0}\right\}
\end{equation}\\ \\
It will coincide with $\left[\mathrm{H}_{0}^{1}(\Omega)\right]^{d}$ if $\Gamma_{D}=\partial \Omega$. If $\Gamma_{N}$ has positive measure, we can choose $Q=\mathrm{L}^{2}(\Omega)$. If $\Gamma_{D}=\partial \Omega$, then the pressure space should be $\mathrm{L}_{0}^{2}$ to ensure uniqueness for the pressure $p$.\\ \\
Moreover, if $t>0$, then $\mathbf{u}(t) \in\left[\mathrm{H}^{1}(\Omega)\right]^{d}$, with $\mathbf{u}(t)=\varphi(t)$ on $\Gamma_{D}$, $\mathbf{u}(0)=\mathbf{u}_{0}$ and $p(t) \in Q$.\\ \\
\textbf{Notation Remark} \\
For every function $\mathbf{v} \in \mathbf{H}^{1}(\Omega)$, we denote by

$$
\|\mathbf{v}\|_{\mathbf{H}^{1}(\Omega)}=\left(\sum_{k=1}^{d}\left\|v_{k}\right\|_{H^{1}(\Omega)}^{2}\right)^{1 / 2}
$$

its norm and by

$$
|\mathbf{v}|_{\mathbf{H}^{1}(\Omega)}=\left(\sum_{k=1}^{d}\left|v_{k}\right|_{H^{1}(\Omega)}^{2}\right)^{1 / 2}
$$
its seminorm. \\ \\
The notation $\|\mathbf{v}\|_{L^{p}(\Omega)}, 1 \leq p<\infty$, has a similar meaning. The same symbols will be used in case of tensor functions. Thanks to Poincaré's inequality, $|\mathbf{v}|_{\mathbf{H}^{1}(\Omega)}$ is equivalent to the norm $\|\mathbf{v}\|_{\mathbf{H}^{1}(\Omega)}$ for all functions belonging to $V$, provided that the Dirichlet boundary has a positive measure. \\ \\

Having chosen these functional spaces, we can note first of all that:
$$
\int_{\partial \Omega}\left(\nu \frac{\partial \mathbf{u}}{\partial \mathbf{n}}-p \mathbf{n}\right) \cdot \mathbf{v} d \gamma=\int_{\Gamma_{N}} \psi \cdot \mathbf{v} d \gamma \quad \forall \mathbf{v} \in V
$$


All the integrals involving bilinear terms are finite. More precisely, by using the vector notation $\mathbf{H}^{k}(\Omega)=\left[\mathrm{H}^{k}(\Omega)\right]^{d}, \mathbf{L}^{p}(\Omega)=\left[\mathrm{L}^{p}(\Omega)\right]^{d}, k \geq 1$, $1 \leq p<\infty$, we find:

$$
\begin{aligned}
& \left|\nu \int_{\Omega} \nabla \mathbf{u} \cdot \nabla \mathbf{v} d \Omega\right| \leq \nu|\mathbf{u}|_{\mathbf{H}^{1}(\Omega)}|\mathbf{v}|_{\mathbf{H}^{1}(\Omega)}, \\
& \left|\int_{\Omega} p \operatorname{div} \mathbf{v} d \Omega\right| \leq\|p\|_{\mathrm{L}^{2}(\Omega)}|\mathbf{v}|_{\mathbf{H}^{1}(\Omega)}, \\
& \left|\int_{\Omega} q \nabla \cdot \mathbf{u} d \Omega\right| \leq\|q\|_{\mathrm{L}^{2}(\Omega)}|\mathbf{u}|_{\mathbf{H}^{1}(\Omega)} .
\end{aligned}
$$
\\
Also the integral involving the trilinear term is finite. To see how, let us start by recalling the following result. 
\begin{proposition}
    If $d \leq 3$, $\forall \mathbf{v} \in \mathbf{H}^{1}(\Omega)$, then $\mathbf{v} \in \mathbf{L}^{4}(\Omega)$ and $\exists C>0$ s.t. $\|\mathbf{v}\|_{\mathbf{L}^{4}(\Omega)} \leq C\|\mathbf{v}\|_{\mathbf{H}^{1}(\Omega)}$.
    \end{proposition}
Using the following three-term Hölder inequality:
$$
\left|\int_{\Omega} f g h d \Omega\right| \leq\|f\|_{L^{p}(\Omega)}\|g\|_{L^{q}(\Omega)}\|h\|_{L^{r}(\Omega)}
$$
valid for all $p, q, r>1$ such that $p^{-1}+q^{-1}+r^{-1}=1$, we conclude that:$$\left|\int_{\Omega}[\mathbf{u} \cdot (\nabla \mathbf{u})] \cdot \mathbf{v} d \Omega\right| \leq\|\nabla \mathbf{u}\|_{\mathbf{L}^{2}(\Omega)}\|\mathbf{u}\|_{\mathbf{L}^{4}(\Omega)}\|\mathbf{v}\|_{\mathbf{L}^{4}(\Omega)} \leq C^{2}\|\mathbf{u}\|_{\mathbf{H}^{1}(\Omega)}^{2}\|\mathbf{v}\|_{\mathbf{H}^{1}(\Omega)}.$$

\section{Solution Uniqueness}
As for the solution's uniqueness, let us consider again the Navier-Stokes equations in strong form (4.2) (similar considerations can be made on the weak form).\\ \\
If $\Gamma_{D}=\partial \Omega$, when only boundary conditions of Dirichlet type are imposed, the pressure appears merely in terms of its gradient; in such a case, if we call $(\mathbf{u}, p)$ a solution of (4.2), for any possible constant $c$ the couple $(\mathbf{u}, p+c)$ is a solution too, since $\nabla(p+c)=\nabla p$.\\ \\
To avoid such indeterminacy one can fix a priori the value of $p$ at one given point $\mathbf{x}_{0}$ of the domain $\Omega$, that is set $p\left(\mathbf{x}_{0}\right)=p_{0}$, or, alternatively, require the pressure to have null average, i.e., $\int_{\Omega} p d \Omega=0$.\\ \\
The former condition requires to prescribe a pointwise value for the pressure, but this is inconsistent with our ansatz that $p \in \mathrm{L}^{2}(\Omega)$. (We anticipate, however, that this is admissible at the numerical level when we look for a continuous finite-dimensional pressure).\\ \\
Prescribing a pointwise value for the pressure is therefore inconsistent with the assumption that \( p \) is an \( L^2 \) function. This kind of pointwise condition is more appropriate for functions in spaces that include continuity (like continuous functions or, in a numerical setting, continuous finite-dimensional approximations).\\ \\
For this reason we assume from now on that the pressure is average-free. More specifically, we will consider the following pressure space:
$$
\mathrm{Q}=\mathrm{L}_{0}^{2}(\Omega)=\left\{p \in \mathrm{L}^{2}(\Omega): \int_{\Omega} p d \Omega=0\right\}
$$
Further, we observe that if $\Gamma_{D}=\partial \Omega$, the prescribed Dirichlet data $\varphi$ must be compatible with the incompressibility constraint; indeed,
$$
\int_{\partial \Omega} \varphi \cdot \mathbf{n} d \gamma=\int_{\Omega} \operatorname{div} \mathbf{u} \mathrm{d} \Omega=0 .
$$
If $\Gamma_{N}$ is not empty, i.e. in presence of either Neumann or mixed Dirichlet-Neumann boundary conditions, the problem of pressure indeterminacy (up to an additive constant) no longer exists. In this case we can take $\mathrm{Q}=\mathrm{L}^{2}(\Omega)$.\\ \\
In conclusion, from now on we shall implicitly assume:
\begin{equation}
Q=\mathrm{L}^{2}(\Omega) \quad \text { if } \quad \Gamma_{N} \neq \emptyset, \quad Q=\mathrm{L}_{0}^{2}(\Omega) \quad \text { if } \Gamma_{N}=\emptyset
\end{equation}
The weak formulation of the system (4.2), is therefore: \\ 
find $\mathbf{u} \in \mathrm{L}^{2}\left(\mathbb{R}^{+} ;\left[\mathrm{H}^{1}(\Omega)\right]^{d}\right) \cap C^{0}\left(\mathbb{R}^{+} ;\left[\mathrm{L}^{2}(\Omega)\right]^{d}\right), p \in \mathrm{L}^{2}\left(\mathbb{R}^{+} ; Q\right)$ such that:
\begin{equation}
\left\{\begin{array}{l}
\int_{\Omega} \frac{\partial \mathbf{u}}{\partial t} \cdot \mathbf{v} d \Omega+\nu \int_{\Omega} \nabla \mathbf{u} \cdot \nabla \mathbf{v} d \Omega+\int_{\Omega}[(\mathbf{u} \cdot \nabla) \mathbf{u}] \cdot \mathbf{v} d \Omega \\
-\int_{\Omega} p \operatorname{div} \mathbf{v} d \Omega=\int_{\Omega} \mathbf{f} \cdot \mathbf{v} d \Omega+\int_{\Gamma_{N}} \psi \cdot \mathbf{v} d \gamma \quad \forall \mathbf{v} \in V, \\
\int_{\Omega} q \operatorname{div} \mathbf{u} d \Omega=0 \quad \forall q \in Q,
\end{array}\right.
\end{equation}
with $\left.\mathbf{u}\right|_{\Gamma_{D}}=\varphi_{D}$ and $\left.\mathbf{u}\right|_{t=0}=\mathbf{u}_{0}$. The space $V$ is the one in (4.7) while $Q$ is the space introduced in (4.8).\\ \\
As we have already anticipated, existence of solutions can be proven for this problem for both dimensions $d=2$ and $d=3$, whereas uniqueness has been proven only in the case $d=2$ for sufficiently small data.\\ \\


\section{The Reynolds number}
Let us define the Reynolds number:
$$
\operatorname{Re}=\frac{|\mathbf{U}| L}{\nu}
$$
where $L$ is a representative length of the domain $\Omega$ (e.g. the length of a channel where the fluid's flow is studied) and $\mathbf{U}$ a representative fluid velocity.\\ \\
The Reynolds number measures the extent to which convection dominates over diffusion.
\begin{itemize}
  \item When $\operatorname{Re} \ll 1$ the convective term $(\mathbf{u} \cdot \nabla) \mathbf{u}$ can be omitted, and the Navier-Stokes equations reduce to the so-called Stokes equations, that will be investigated later.
  \item On the other hand, if $R e$ is large, problems may arise concerning uniqueness of the solution, the existence of stationary and stable solutions, the possible existence of strange attractors, the transition towards turbulent flows.
\end{itemize}

\section{Stokes equations and their approximation}
In this section we will consider the following generalized Stokes problem with homogeneous Dirichlet boundary conditions:
\begin{equation}
\begin{cases}\sigma \mathbf{u}-\nu \Delta \mathbf{u}+\nabla p=\mathbf{f} & \text { in } \Omega \\ \operatorname{div} \mathbf{u}=0 & \text { in } \Omega \\ \mathbf{u}=\mathbf{0} & \text { on } \partial \Omega\end{cases}
\end{equation}
for a given coefficient $\sigma \geq 0$.\\ \\
This problem describes the motion of an incompressible viscous flow in which the (quadratic) convective term has been neglected, a simplification that is acceptable when $\operatorname{Re} \ll 1$.\\ \\
Moreover, one can generate a problem like (4.10) also while using an implicit temporal discretization of the Navier-Stokes equations and by neglecting the convective term (i.e. $(\mathbf{u} \cdot \nabla) \mathbf{u})$.\\ \\
We have indeed the following scheme, where $k$ denotes the temporal index:
$$
\begin{cases}\frac{\mathbf{u}^{k}-\mathbf{u}^{k-1}}{\Delta t}-\nu \Delta \mathbf{u}^{k}+\nabla p^{k}=\mathbf{f}\left(t^{k}\right), & \mathbf{x} \in \Omega, t>0, \\ \operatorname{div} \mathbf{u}^{k}=0, & \mathbf{x} \in \Omega, t>0 \\ + \text { B.C. } & \end{cases}
$$
Hence, at each time step $t^{k}$ we need to solve the following Stokes-like system of equations:
\begin{equation}
\begin{cases}\sigma \mathbf{u}^{k}-\nu \Delta \mathbf{u}^{k}+\nabla p^{k}=\tilde{\mathbf{f}}^{k} & \text { in } \Omega, \\ \operatorname{div} \mathbf{u}^{k}=0 & \text { in } \Omega \\ + \text { B.C. } & \end{cases}
\end{equation}
where $\sigma=(\Delta t)^{-1}$ and $\tilde{\mathbf{f}}^{k}=\mathbf{f}\left(t^{k}\right)+\frac{\mathbf{u}^{k-1}}{\Delta t}$.\\ \\
The weak formulation of problem (4.10) reads:\\ \\
Find $\mathbf{u} \in V$ and $p \in Q$ such that
\begin{equation}
\begin{cases}\int_{\Omega}(\sigma \mathbf{u} \cdot \mathbf{v}+\nu \nabla \mathbf{u} \cdot \nabla \mathbf{v}) d \Omega-\int_{\Omega} p \operatorname{div} \mathbf{v} d \Omega=\int_{\Omega} \mathbf{f} \cdot \mathbf{v} d \Omega & \forall \mathbf{v} \in V, \\ \int_{\Omega} q \operatorname{div} \mathbf{u} d \Omega=0 & \forall q \in Q,\end{cases}
\end{equation}
where $V=\left[\mathrm{H}_{0}^{1}(\Omega)\right]^{d}$ and $Q=\mathrm{L}_{0}^{2}(\Omega)$.\\ \\
In the weak formulation of fluid dynamics problems like the Stokes problem, a key transformation occurs with the term \(\nabla p \cdot \mathbf{v}\) from the strong form. This term is modified to \(p \, \nabla \cdot \mathbf{v}\) in the weak form through integration by parts. The process is as follows:

Integrating the term \(\nabla p \cdot \mathbf{v}\) over the domain \(\Omega\). This step involves applying integration by parts, which transforms the integral into a different form. The integration by parts formula states that \(\int_{\Omega} \nabla p \cdot \mathbf{v} \, d\Omega = -\int_{\Omega} p \, \nabla \cdot \mathbf{v} \, d\Omega + \int_{\partial\Omega} p \, \mathbf{v} \cdot \mathbf{n} \, dS\), where \(\partial\Omega\) denotes the boundary of \(\Omega\) and \(\mathbf{n}\) is the outward unit normal on the boundary. However, due to Dirichlet boundary conditions, the boundary term \(\int_{\partial\Omega} p \, \mathbf{v} \cdot \mathbf{n} \, dS\) vanishes. Thus, the result simplifies to \(\int_{\Omega} \nabla p \cdot \mathbf{v} \, d\Omega = -\int_{\Omega} p \, \nabla \cdot \mathbf{v} \, d\Omega\).


\begin{equation}
\begin{aligned}
& a(\mathbf{u}, \mathbf{v})=\int_{\Omega}(\sigma \mathbf{u} \cdot \mathbf{v}+\nu \nabla \mathbf{u} \cdot \nabla \mathbf{v}) d \Omega, \\
& b(\mathbf{u}, q)=-\int_{\Omega} q \operatorname{div} \mathbf{u} d \Omega .
\end{aligned}
\end{equation}

With these notations, problem (4.12) becomes: find $(\mathbf{u}, p) \in V \times Q$ such that:
\begin{equation}
\begin{cases}a(\mathbf{u}, \mathbf{v})+b(\mathbf{v}, p)=(\mathbf{f}, \mathbf{v}) & \forall \mathbf{v} \in V, \\ b(\mathbf{u}, q)=0 & \forall q \in Q,\end{cases}
\end{equation}
where $(\mathbf{f}, \mathbf{v})=\sum_{i=1}^{d} \int_{\Omega} f_{i} v_{i} d \Omega$.\\ \\
If we consider non-homogeneous boundary conditions, as indicated in (4.4), the weak formulation of the Stokes problem becomes: find $\left(\dot{\mathbf{u}}^{\circ} p\right) \in V \times Q$ such that:
\begin{equation}
\begin{cases}a(\stackrel{\circ}{\mathbf{u}}, \mathbf{v})+b(\mathbf{v}, p)=\mathbf{F}(\mathbf{v}) & \forall \mathbf{v} \in V, \\ b(\stackrel{\circ}{\mathbf{u}}, q)=G(q) & \forall q \in Q,\end{cases}
\end{equation}

where $V$ and $Q$ are the spaces introduced in (4.7) and (4.8), respectively.\\ \\
Having denoted with $\mathbf{R} \varphi \in\left[\mathrm{H}^{1}(\Omega)\right]^{d}$ a lifting of the boundary datum $\varphi$, we have set $\stackrel{\circ}{\mathbf{u}}=\mathbf{u}-\mathbf{R} \varphi$, while the new terms on the right-hand side have the following expression:
\begin{equation}
\mathbf{F}(\mathbf{v})=(\mathbf{f}, \mathbf{v})+\int_{\Gamma_{N}} \psi \mathbf{v} d \gamma-a(\mathbf{R} \varphi, \mathbf{v}), \quad G(q)=-b(\mathbf{R} \varphi, q)
\end{equation}

\section{Galerkin Approximation}
The Galerkin approximation of problem (4.14) has the following form: find $\left(\mathbf{u}_{h}, p_{h}\right) \in V_{h} \times Q_{h}$ such that
\begin{equation}
\begin{cases}a\left(\mathbf{u}_{h}, \mathbf{v}_{h}\right)+b\left(\mathbf{v}_{h}, p_{h}\right)=\left(\mathbf{f}, \mathbf{v}_{h}\right) & \forall \mathbf{v}_{h} \in V_{h} \\ b\left(\mathbf{u}_{h}, q_{h}\right)=0 & \forall q_{h} \in Q_{h}\end{cases}
\end{equation}
where $\left\{V_{h} \subset V\right\}$ and $\left\{Q_{h} \subset Q\right\}$ represent two families of finite-dimensional subspaces depending on a real discretization parameter $h$.\\ \\
If, instead, we consider problem (4.15)-(4.16) corresponding to non-homogeneous boundary data (4), the above formulation needs to be modified by using $\mathbf{F}\left(\mathbf{v}_{h}\right)$ on the right-hand side of the first equation and $G\left(q_{h}\right)$ on that of the second equation. These new functionals can be obtained from (4.16) by replacing $\mathbf{R} \varphi$ with the interpolant of $\varphi$ at the nodes of $\Gamma_{D}$ (and vanishing at all other nodes), and replacing $\boldsymbol{\psi}$ with its interpolant at the nodes sitting on $\Gamma_{N}$.
\section{Existence and Uniqueness}
The following celebrated theorem is due to F. Brezzi, and guarantees uniqueness and existence for problem (4.17):
\begin{theorem}[Existence and Uniqueness]
    
The Galerkin approximation (4.17) admits one and only one solution if the following conditions hold:

\begin{itemize}
  \item The bilinear form $a(\cdot, \cdot)$ is:
\end{itemize}

a) coercive, that is $\exists \alpha>0$ (possibly depending on $h$ ) such that

$$
a\left(\mathbf{v}_{h}, \mathbf{v}_{h}\right) \geq \alpha\left\|\mathbf{v}_{h}\right\|_{V}^{2} \quad \forall \mathbf{v}_{h} \in V_{h}^{*},
$$

where $V_{h}^{*}=\left\{\mathbf{v}_{h} \in V_{h}: b\left(\mathbf{v}_{h}, q_{h}\right)=0 \forall q_{h} \in Q_{h}\right\} ;$

b) continuous, that is $\exists \gamma>0$ such that

$$
\left|a\left(\mathbf{u}_{h}, \mathbf{v}_{h}\right)\right| \leq \gamma\left\|\mathbf{u}_{h}\right\| v\left\|\mathbf{v}_{h}\right\| v \quad \forall \mathbf{u}_{h}, \mathbf{v}_{h} \in V_{h}
$$

\begin{itemize}
  \item The bilinear form $b(\cdot, \cdot)$ is continuous, that is $\exists \delta>0$ such that
\end{itemize}

$$
\left|b\left(\mathbf{v}_{h}, q_{h}\right)\right| \leq \delta\left\|\mathbf{v}_{h}\right\| v\left\|q_{h}\right\|_{Q} \quad \forall \mathbf{v}_{h} \in V_{h}, q_{h} \in Q_{h}
$$

\begin{itemize}
  \item Finally, there exists a positive constant $\beta$ (possibly depending on $h$) such that
\end{itemize}

$$
\forall q_{h} \in Q_{h}, \exists \mathbf{v}_{h} \in V_{h}: b\left(\mathbf{v}_{h}, q_{h}\right) \geq \beta\left\|\mathbf{v}_{h}\right\|_{\mathbf{H}^{1}(\Omega)}\left\|q_{h}\right\|_{L^{2}(\Omega)}
$$

Under the previous assumptions the discrete solution fulfills the following a-priori estimates:

$$
\begin{aligned}
\left\|\mathbf{u}_{h}\right\|_{V} & \leq \frac{1}{\alpha}\|\mathbf{f}\|_{V^{\prime}} \\
\left\|p_{h}\right\|_{Q} & \leq \frac{1}{\beta}\left(1+\frac{\gamma}{\alpha}\right)\|\mathbf{f}\|_{V^{\prime}}
\end{aligned}
$$

where $V^{\prime}$ is the dual space of $V$.

Moreover, the following convergence results hold:

$$
\begin{aligned}
\left\|\mathbf{u}-\mathbf{u}_{h}\right\|_{v} & \leq\left(1+\frac{\delta}{\beta}\right)\left(1+\frac{\gamma}{\alpha}\right) \inf _{\mathbf{v}_{h} \in V_{h}}\left\|\mathbf{u}-\mathbf{v}_{h}\right\|_{v} \\
& +\frac{\delta}{\alpha} \inf _{q_{h} \in Q_{h}}\left\|p-q_{h}\right\|_{Q}, \\
\left\|p-p_{h}\right\|_{Q} & \leq \frac{\gamma}{\beta}\left(1+\frac{\gamma}{\alpha}\right)\left(1+\frac{\delta}{\beta}\right) \inf _{\mathbf{v}_{h} \in V_{h}}\left\|\mathbf{u}-\mathbf{v}_{h}\right\|_{v} \\
& +\left(1+\frac{\delta}{\beta}+\frac{\delta \gamma}{\alpha \beta}\right) \inf _{q_{h} \in Q_{h}}\left\|p-q_{h}\right\|_{Q} .
\end{aligned}
$$

\end{theorem}
It is worth noticing that condition on $\beta$ is equivalent to the existence of a positive constant $\beta$ such that:
$$
\inf _{q_{h} \in Q_{h}, q_{h} \neq 0} \sup _{\mathbf{v}_{h} \in V_{h}, \mathbf{v}_{h} \neq \mathbf{0}} \frac{b\left(\mathbf{v}_{h}, q_{h}\right)}{\left\|\mathbf{v}_{h}\right\|_{\mathbf{H}^{1}(\Omega)}\left\|q_{h}\right\|_{\mathrm{L}^{2}(\Omega)}} \geq \beta
$$
For such a reason it is often called the inf-sup condition.

\section{Algebraic formulation of the Stokes problem}
Let us investigate the structure of the algebraic system associated to the Galerkin approximation (4.17) to the Stokes problem. Denote with:
$$
\left\{\varphi_{j} \in V_{h}\right\}, \quad\left\{\phi_{k} \in Q_{h}\right\}
$$

the basis functions of the spaces $V_{h}$ and $Q_{h}$, respectively.\\ \\ Let us expand the discrete solutions $\mathbf{u}_{h}$ and $p_{h}$ with respect to such bases,
\begin{equation}
\mathbf{u}_{h}(\mathbf{x})=\sum_{j=1}^{N} u_{j} \varphi_{j}(\mathbf{x}), \quad p_{h}(\mathbf{x})=\sum_{k=1}^{M} p_{k} \phi_{k}(\mathbf{x})
\end{equation}
having set $N=\operatorname{dim} V_{h}$ and $M=\operatorname{dim} Q_{h}$. \\ \\
By choosing as test functions in (4.18) the same basis functions we obtain the following block linear system:
\begin{equation}
    \left\{\begin{array}{l}
\mathrm{A} \mathbf{U}+\mathrm{B}^{T} \mathbf{P}=\mathbf{F} \\
\mathrm{B} \mathbf{U}=\mathbf{0}
\end{array}\right.
\end{equation}
where $\mathrm{A} \in \mathbb{R}^{N \times N}$ and $\mathrm{B} \in \mathbb{R}^{M \times N}$ are the matrices related respectively to the bilinear forms $a(\cdot, \cdot)$ and $b(\cdot, \cdot)$, whose elements are given by:
$$
\mathrm{A}=\left[a_{i j}\right]=\left[a\left(\varphi_{j}, \varphi_{i}\right)\right], \quad \mathrm{B}=\left[b_{k m}\right]=\left[b\left(\varphi_{m}, \phi_{k}\right)\right]
$$
while $\mathbf{U}$ and $\mathbf{P}$ are the vectors of the unknowns,
$$
\mathbf{U}=\left[u_{j}\right], \quad \mathbf{P}=\left[p_{j}\right]
$$
The $(N+M) \times (N+M)$ matrix:
\begin{equation}
\mathbf{S} = 
\begin{bmatrix}
\mathbf{A} & \mathbf{B}^{\top} \\
\mathbf{B} & 0
\end{bmatrix}
\end{equation}

is block symmetric (as $\mathbf{A}$ is symmetric) and indefinite, featuring real eigenvalues with variable sign (either positive and negative).\\ \\
\begin{proposition}
    $\mathbf{S}$ is non-singular if and only if no eigenvalue is null, iff $\operatorname{ker}(\mathbf{B}^T)=0$.
\end{proposition}

\begin{proof}
    
Since $\mathbf{A}$ is non-singular - it is associated with the coercive bilinear form $a(\cdot, \cdot)$. From the first of (4.19) we can formally obtain $\mathbf{U}$ as:
\begin{equation}
\mathbf{U} = \mathbf{A}^{-1} (\mathbf{F} - \mathbf{B}^{\top} \mathbf{P})
\end{equation}

Using (4.21) in the second equation of (4.19) yields:

\[
\mathbf{RP} = \mathbf{BA}^{-1} \mathbf{F}, \quad \text{where} \quad \mathbf{R} = \mathbf{BA}^{-1} \mathbf{B}^{\top}
\]

This corresponds to having carried out a block Gaussian elimination on system (4.20).

In this way, we obtain a reduced system for the sole unknown $\mathbf{P}$ (the pressure), which admits a unique solution in case $\mathbf{R}$ is non-singular. Since $\mathbf{A}$ is non-singular and positive definite, we want to prove that the latter condition is satisfied if and only if $\mathbf{B}^{\top}$ has a null kernel, that is

\begin{equation}
\operatorname{ker}(\mathbf{B}^{\top}) = \{\mathbf{0}\}
\end{equation}

where $\operatorname{ker}(\mathbf{B}^{\top}) = \left\{ \mathbf{x} \in \mathbb{R}^{M} : \mathbf{B}^{\top} \mathbf{x} = \mathbf{0} \right\}$.

We proceed as follows:

\[
\mathbf{R}\mathbf{p} = 0 \implies p = 0 \quad \text{(uniqueness)}
\]
that is
\[
\langle \mathbf{B}\mathbf{A}^{-1} \mathbf{B}^T p, q \rangle = 0 \quad \forall q \implies p = 0
\]
Let us take \( \mathbf{q} = \mathbf{p} \). We require:
\[
\langle \mathbf{A}^{-1} \mathbf{B}^T \mathbf{p}, \mathbf{B}^T \mathbf{p} \rangle = 0 \implies \mathbf{p} = 0
\]
Set \( \mathbf{w} = \mathbf{B}^T \mathbf{p} \). Since \( \mathbf{A} \) is spd, we have
\[
\langle \mathbf{A}^{-1} \mathbf{w}, \mathbf{w} \rangle = 0 \implies w = 0
\]
Finally, 
\[
(\mathbf{w} = \mathbf{B}^T \mathbf{p} = 0 \implies \mathbf{p} = 0) \iff \ker \mathbf{B}^T = \{0\}
\]

\end{proof}

\begin{proposition}{Equivalency between inf-sup condition and full rank condition of $ \operatorname{B}^T $}
Condition (4.22) is equivalent to the inf-sup condition.
\end{proposition}


\begin{proof}
 Note that condition (4.22) is violated iff $\exists \mathbf{p}^{*} \neq \mathbf{0}$ with $\mathbf{p}^{*} \in \mathbb{R}^{M}$ such that $\mathrm{B}^{T} \mathbf{p}^{*}=\mathbf{0}$ or, equivalently, if $\exists p_{h}^{*} \in \mathbb{Q}_{h}$ such that $b\left(\varphi_{n}, p_{h}^{*}\right)=0$ $\forall n=1, \ldots, N$. This is equivalent to $b\left(\mathbf{v}_{h}, p_{h}^{*}\right)=0 \quad \forall \mathbf{v}_{h} \in V_{h}$, which in turn is equivalent to violating the inf-sup condition. Indeed:

$$
\exists \beta_{h}>0 \ \forall \mathbf{q}_{h} \in Q_{h} \ \exists \mathbf{v}_{h} \text { in } X_{h}: \frac{b\left(\mathbf{v}_{h}, \mathbf{q}_{h}\right)}{\left\|\mathbf{v}_{h}\right\| v\left\|\mathbf{q}_{h}\right\|_{Q}} \geq \beta_{h}
$$

is violated if

$$
\forall \beta_{h}>0 \ \exists \mathbf{p}_{h}^{*} \in Q_{h} \ \forall \mathbf{v}_{h} \text { in } X_{h}: \frac{b\left(\mathbf{v}_{h}, \mathbf{p}_{h}^{*}\right)}{\left\|\mathbf{v}_{h}\right\| v\left\|\mathbf{p}_{h}^{*}\right\|_{Q}}<\beta_{h}
$$

Take now $-\mathbf{v}_{h}$ :

$$
\frac{b\left(-\mathbf{v}_{h}, \mathbf{p}_{h}^{*}\right)}{\left\|\mathbf{v}_{h}\right\| v\left\|\mathbf{p}_{h}^{*}\right\|_{Q}}=-\frac{b\left(\mathbf{v}_{h}, \mathbf{p}_{h}^{*}\right)}{\left\|\mathbf{v}_{h}\right\| v\left\|\mathbf{p}_{h}^{*}\right\|_{Q}}<\beta_{h}
$$

Then

$$
-\beta_{h}<\frac{b\left(\mathbf{v}_{h}, \mathbf{p}_{h}^{*}\right)}{\left\|\mathbf{v}_{h}\right\| v\left\|\mathbf{p}_{h}^{*}\right\|_{Q}}<\beta_{h}
$$

Because of the arbitrariness of $\beta_{h}$ we conclude that $b\left(\mathbf{v}_{h}, \mathbf{p}_{h}^{*}\right)=0 
\ \forall \mathbf{v}_{h} \in X_{h}$.
On the other hand, since $A$ is non-singular, from the existence and uniqueness of $\mathbf{P}$ we infer that there exists a unique vector $\mathbf{U}$ which satisfies (4.21), and so the inf-sup condition does hold.
In conclusion, system (4.22) admits a unique solution (U, P) if and only if condition (4.22) holds.

\end{proof}

\paragraph{Remark}
We recall that, for an arbitrary matrix $B^{T} \in \mathbb{R}^{N \times M}$, we have $\operatorname{rank}\left(B^{T}\right)+\operatorname{dim} \operatorname{ker}\left(B^{T}\right)=\min (M, N)$. \\ \\
Then, condition (4.22) is equivalent to asking that $B^{T}$ (and consequently $B$) has full rank, i.e. that $\operatorname{rank}\left(B^{T}\right)=\min (N, M)$, because $\operatorname{rank}\left(B^{T}\right)$ is the maximum number of linearly independent row vectors (or, equivalently, column vectors) of $B^{T}$. \\ \\
Let us consider again the remark about spurious pressure modes concerning the general saddle-point problem and suppose that the inf-sup condition does not hold. Then:
\begin{equation}
    \exists q_{h}^{*} \in Q_{h}: \quad b\left(\mathbf{v}_{h}, q_{h}^{*}\right)=0 \quad \forall \mathbf{v}_{h} \in V_{h} .
\end{equation}
Consequently, if $\left(\mathbf{u}_{h}, p_{h}\right)$ is a solution to the Stokes problem (4.17), then $\left(\mathbf{u}_{h}, p_{h}+q_{h}^{*}\right)$ is a solution too, as:

$$
\begin{aligned}
a\left(\mathbf{u}_{h}, \mathbf{v}_{h}\right)+b\left(\mathbf{v}_{h}, p_{h}+q_{h}^{*}\right) & =a\left(\mathbf{u}_{h}, \mathbf{v}_{h}\right)+b\left(\mathbf{v}_{h}, p_{h}\right)+b\left(\mathbf{v}_{h}, q_{h}^{*}\right) \\
& =a\left(\mathbf{u}_{h}, \mathbf{v}_{h}\right)+b\left(\mathbf{v}_{h}, p_{h}\right)=\left(\mathbf{f}, \mathbf{v}_{h}\right) \quad \forall \mathbf{v}_{h} \in V_{h} .
\end{aligned}
$$
Functions $q_{h}^{*}$ which fail to satisfy the inf-sup condition are invisible to the Galerkin problem(4.17). For this reason, as already observed, they are called spurious pressure modes, or even parasitic modes. Their presence inhibits the pressure solution from being unique, yielding numerical instabilities. For this reason, those finite-dimensional subspaces that violate the compatibility condition are said to be unstable, or incompatible.\\ \\
Two strategies are generally adopted in order to guarantee well-posedness of the numerical problem:
\begin{itemize}
  \item choose spaces $V_{h}$ and $Q_{h}$ that satisfy the inf-sup condition;
  \item stabilize (either a priori or a posteriori) the finite dimensional problem by eliminating the spurious modes.
\end{itemize}

\section{Compatible couples of spaces}
Let us analyze the first type of strategy. To start with, we will consider the case of finite element spaces. To characterize $Q_{h}$ and $V_{h}$ it suffices to choose on every element of the triangulation their degrees of freedom. Since the weak formulation does not require a continuous pressure, we will consider first the case of discontinuous pressures. \\ \\
As Stokes equations are of order one in $p$ and order two in $\mathbf{u}$, generally speaking it makes sense to use piecewise polynomials of degree $k \geq 1$ for the velocity space $V_{h}$ and of degree $k-1$ for the space $Q_{h}$.\\ \\
In particular, we might want to use piecewise linear finite elements $\mathbb{P}_{1}$ for each velocity component, and piecewise constant finite elements $\mathbb{P}_{0}$ for the pressure. In fact, this choice, although being quite natural, does not pass the inf-sup test (4.19).\\ \\
When looking for a compatible couple of spaces, the larger the velocity space $V_{h}$, the more likely the inf-sup condition is satisfied. Otherwise said, the space $V_{h}$ should be "rich" enough compared to the space $Q_{h}$.\\ \\
In the following pictures, by means of the symbol $\square$ we indicate the degrees of freedom for the pressure, whereas the symbol $\bullet$ identifies those for each velocity component.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.4]{2023_12_28_f9ec6ef997eb6f048c53g-53}
    \caption{Case of discontinuous pressure: choices that do not satisfy the inf-sup condition, on triangles (left), and on quadrilaterals (right)}
    \label{fig:discontinuous-pressure}
\end{figure}
\noindent In this figure we report three different choices of spaces that fulfill the inf-sup condition, still in the case of continuous velocity and discontinuous pressure. Choice (left) is made by $\mathbb{P}_{2}-\mathbb{P}_{0}$ elements, (center) by $\mathbb{Q}_{2}-\mathbb{P}_{0}$ elements, while choice (right) by piecewise linear discontinuous elements for the pressure, while the velocity components are made by piecewise quadratic continuous elements enriched by a cubic bubble function on each triangle - these are the so-called Crouzeix-Raviart elements.

\begin{figure}[ht]
    \centering
    \includegraphics[max width=\textwidth]{2023_12_28_f9ec6ef997eb6f048c53g-54}
    \caption{Case of discontinuous pressure: choices that do satisfy the inf-sup condition: on triangles, (left), and on quadrilaterals, (center). Also the couple (right), known as Crouzeix-Raviart elements, satisfies the inf-sup condition}
    \label{fig:discontinuous-pressure-inf-sup}
\end{figure}

\noindent In this figure, we report two choices of incompatible finite elements in the case of continuous pressure. They consist of piecewise linear elements on triangles (respectively, bilinear on quadrilaterals) for both velocity and pressure. More generally, finite elements of the same polynomial degree \(k \geq 1\) for both velocity and pressure are unstable (equal order interpolation).



\FloatBarrier % Prevents floats from passing this point in the document
\begin{figure}[!htb] % The exclamation mark can force LaTeX to respect the "here" preference
    \centering
    \includegraphics[max width=\textwidth]{images/1.jpg}
    \caption{Case of continuous pressure: the couples displayed in this figure do not satisfy the inf-sup condition.}
    \label{fig:continuous-pressure}
\end{figure}
\FloatBarrier % Prevents floats from passing this point in the document


\noindent In this figure, the elements displayed are instead stable. In both cases, pressure is a piecewise linear continuous function, whereas velocities are piecewise linear polynomials on each of the four sub-triangles (left), or piecewise linear polynomials enriched by a cubic bubble function (right).
 \FloatBarrier % Prevents floats from passing this point in the document
\begin{figure}[!htb] % The exclamation mark can force LaTeX to respect the "here" preference
    \centering
    \includegraphics[max width=\textwidth]{images/2.jpg}
    \caption{Case of continuous pressure: the elements used for the velocity components in (left) are known as $\mathbb{P}_1$-iso $\mathbb{P}_2$ finite elements, whereas couple (right) is called mini-element}
    \label{fig:continuous-pressure}
\end{figure}
\FloatBarrier % Prevents floats from passing this point in the document
\noindent The pair $\mathbb{P}_2-\mathbb{P}_1$ (continuous piecewise quadratic velocities and continuous piecewise linear pressure) is stable. This is the smallest degree representative of the family of the so-called Taylor-Hood elements $\mathbb{P}_k-\mathbb{P}_{k-1}, k \geq 2$ (continuous velocities and continuous pressure), that are inf-sup stable.
They yield the following error estimate:
$$
\left\|\mathbf{u}-\mathbf{u}_h\right\|_{\left[H^1(\Omega)\right]^d}+\left\|p-p_h\right\|_{L^2(\Omega)} \leq C h^k\left(|\mathbf{u}|_{\left[H^{k+1}(\Omega)\right]^d}+|p|_{\left[H^k(\Omega)\right]^d}\right)
$$

\section{Time discretization of Navier-Stokes equations}
We consider the following semidiscretized formulation:
\begin{equation}
\left\{\begin{array}{l}
\mathrm{M} \frac{d \mathbf{u}(t)}{d t}+\mathrm{A} \mathbf{u}(t)+\mathrm{C}(\mathbf{u}(t)) \mathbf{u}(t)+\mathrm{B}^{T} \mathbf{p}(t)=\mathbf{f}(t) \\
\mathrm{Bu}(t)=\mathbf{0}
\end{array}\right.
\end{equation}

with $\mathbf{u}(0)=\mathbf{u}_{0} . C(\mathbf{u}(t))$ is in fact a matrix depending on $\mathbf{u}(t)$, whose generic coefficient is $c_{m i}(t)=c\left(\mathbf{u}(t), \boldsymbol{\varphi}_{i}, \boldsymbol{\varphi}_{m}\right)$.\\ \\
For the temporal discretization of this system let us use, for instance, the $\theta$-method, that was introduced for parabolic equations. By setting:
$$
\begin{aligned}
& \mathbf{u}_{\theta}^{n+1}=\theta \mathbf{u}^{n+1}+(1-\theta) \mathbf{u}^{n} \\
& \mathbf{p}_{\theta}^{n+1}=\theta \mathbf{p}^{n+1}+(1-\theta) \mathbf{p}^{n} \\
& \mathbf{f}_{\theta}^{n+1}=\theta \mathbf{f}\left(t^{n+1}\right)+(1-\theta) \mathbf{f}\left(t^{n}\right) \\
& C_{\theta}\left(\mathbf{u}^{n+1, n}\right) \mathbf{u}^{n+1, n}=\theta C\left(\mathbf{u}^{n+1}\right) \mathbf{u}^{n+1}+(1-\theta) \mathrm{C}\left(\mathbf{u}^{n}\right) \mathbf{u}^{n}
\end{aligned}
$$

we obtain the following system of algebraic equations:
\begin{equation}
\left\{\begin{array}{l}
\mathrm{M} \frac{\mathbf{u}^{n+1}-\mathbf{u}^{n}}{\Delta t}+\mathrm{A}_{\theta}^{n+1}+\mathrm{C}_{\theta}\left(\mathbf{u}^{n+1, n}\right) \mathbf{u}^{n+1, n}+\mathrm{B}^{T} \mathbf{p}_{\theta}^{n+1}=\mathbf{f}_{\theta}^{n+1} \\
\mathrm{~B} \mathbf{u}^{n+1}=\mathbf{0}
\end{array}\right.
\end{equation}
\\
Except for the special case $\theta=0$, which corresponds to the forward Euler method, the solution of this system is quite involved.\\ \\
A possible alternative is to use a semi-implicit scheme, in which the linear part of the equation is advanced implicitly, while nonlinear terms explicitly.\\ \\
By doing so, if $\theta \geq 1 / 2$, the resulting scheme is unconditionally stable, whereas it must obey a stability restriction on the time step $\Delta t$ (depending on $h$ and $\nu$) in all other cases.


\section{Finite difference methods}
We consider at first an explicit temporal discretization of the first equation in (4.24), corresponding to the choice $\theta=0$ in (4.25). If we suppose that all quantities are known at the time $t^{n}$, we can write the associated problem at time $t^{n}+1$ as follows:
\begin{equation}
    \left\{\begin{array}{l}
\mathrm{M} \mathbf{u}^{n+1}=H\left(\mathbf{u}^{n}, \mathbf{p}^{n}, \mathbf{f}^{n}\right) \\
\mathrm{B} \mathbf{u}^{n+1}=\mathbf{0}
\end{array}\right.
\end{equation}

where $M$ is the mass matrix whose entries are

$$
m_{i j}=\int_{\Omega} \varphi_{i} \varphi_{j} d \Omega
$$

This system does not allow the determination of the pressure $\mathbf{p}^{n+1}$. In particular, there is no way to enforce the divergence free constraint on $\mathbf{u}^{n+1}$.

However, if we replace $\mathbf{p}^{n}$ by $\mathbf{p}^{n+1}$ in the momentum equation, we obtain the new linear system

$$
\left\{\begin{array}{l}
\frac{1}{\Delta t} \mathrm{M}\mathbf{u}^{n+1}+\mathrm{B}^{T} \mathbf{p}^{n+1}=\mathbf{G} \\
\mathrm{B} \mathbf{u}^{n+1}=\mathbf{0}
\end{array}\right.
$$
$\mathrm{G}$ being a suitable known vector. \\ \\
\noindent This system corresponds to a semi-explicit discretization of (4.24). Since $M$ is symmetric and positive definite, if condition (4.22) is satisfied, then the reduced system $\mathrm{BM}^{-1} \mathrm{~B}^{T} \mathbf{p}^{n+1}=\mathrm{BM}^{-1} \mathbf{G}$ is non-singular.\\ \\
This discretization method is temporally stable provided the time step satisfies the following limitation (of parabolic type):
$$
\Delta t \leq C \min \left(\frac{h^{2}}{\nu}, \frac{h}{\max _{\mathbf{x} \in \Omega}\left|\mathbf{u}^{n}(\mathbf{x})\right|}\right)
$$

\noindent Let us now consider an implicit discretization of (4.24), for instance the backward Euler method, which corresponds to choosing $\theta=1$ in (4.25). As already observed, this scheme is unconditionally stable. It yields a nonlinear algebraic system which can be regarded as the finite element space approximation to the steady Navier-Stokes problem

$$
\left\{\begin{array}{l}
-\nu \Delta \mathbf{u}^{n+1}+\left(\mathbf{u}^{n+1} \cdot \nabla\right) \mathbf{u}^{n+1}+\nabla p^{n+1}+\frac{\mathbf{u}^{n+1}}{\Delta t}=\tilde{\mathbf{f}} \\
\operatorname{div} \mathbf{u}^{n+1}=0
\end{array}\right.
$$

 The solution of such nonlinear algebraic system can be achieved by Newton-Krylov techniques, that is by using a Krylov method (e.g. GMRES or BiCGStab) for the solution of the linear system that is obtained at each Newton iteration step.\\\\
 We recall that Newton's method is based on the full linearization of the convective term, $\mathbf{u}_k^{n+1} \cdot \nabla \mathbf{u}_{k+1}^{n+1}+\mathbf{u}_{k+1}^{n+1} \cdot \nabla \mathbf{u}_k^{n+1}$. \\ \\
A popular approach consists in starting Newton iterations after few Piccard iterations in which the convective term is evaluated as follows: $\mathbf{u}_{k}^{n+1} \cdot \nabla \mathbf{u}_{k+1}^{n+1}$. \\ \\ 
This approach entails three nested cycles:
\begin{itemize}
  \item temporal iteration: $t^{n} \rightarrow t^{n+1}$;
  \item Newton iteration: $\mathbf{x}_{k}^{n+1} \rightarrow \mathbf{x}_{k+1}^{n+1}$;
  \item Krylov iteration: $\left[\mathbf{x}_{k}^{n+1}\right]_{j} \rightarrow\left[\mathbf{x}_{k}^{n+1}\right]_{j+1}$;
\end{itemize}

for simplicity we have called $\mathbf{x}^{n}$ the couple $\left(\mathbf{u}^{n}, \mathbf{p}^{n}\right)$. Obviously, the goal is the following convergence result:

$$
\lim _{k \rightarrow \infty} \lim _{j \rightarrow \infty}\left[\mathbf{x}_{k}^{n+1}\right]_{j}=\left[\begin{array}{l}
\mathbf{u}^{n+1} \\
\mathbf{p}^{n+1}
\end{array}\right]
$$
Finally, let us operate a semi-implicit, temporal discretization, consisting in treating explicitly the nonlinear convective term. The following algebraic linear system, whose form is similar to (4.22), is obtained in this case
\begin{equation}
\left\{\begin{array}{l}
\frac{1}{\Delta t} \mathrm{Mu}^{n+1}+\mathrm{A} \mathbf{u}^{n+1}+\mathrm{B}^{T} \mathbf{p}^{n+1}=\mathbf{G}, \\
\mathrm{B} \mathbf{u}^{n+1}=\mathbf{0},
\end{array}\right.
\end{equation}

where $\mathbf{G}$ is a suitable known vector. In this case the stability restriction on the time step takes the following form:
\begin{equation}
\Delta t \leq C \frac{h}{\max _{\mathbf{x} \in \Omega}\left|\mathbf{u}^{n}(\mathbf{x})\right|}
\end{equation}

\noindent In all cases, optimal error estimates can be proven.


\section{Time dependent Generalized Stokes problem}

\subsubsection*{Fully explicit discretization}

The fully explicit discretization of the weak formulation of (Time Dependant Stokes) using (FE) with time step $\Delta t$ is

\begin{align*}
\begin{cases}
\int_{\Omega} \frac{\mathbf{u}_{h}^{n+1}-\mathbf{u}_{h}^{n}}{\Delta t} \mathbf{v}_{h}+a\left(\mathbf{u}_{h}^{n}, \mathbf{v}_{h}\right)+b\left(\mathbf{v}_{h}, p_{h}^{n}\right)=\mathbf{F}^{n}\left(\mathbf{v}_{h}\right) & \forall \mathbf{v}_{h} \in V_{h}, \\
b\left(\mathbf{u}_{h}^{n+1}, q_{h}\right)=0 & \forall q_{h} \in Q_{h}, \\
\mathbf{u}^{0}=\mathbf{u}_{0}.
\end{cases}
\end{align*}
Which can be put in the form:
\begin{align*}
\begin{cases}
M \mathbf{u}^{n+1}=\Delta t G\left(\mathbf{u}^{n}, \mathbf{p}^{n}, \mathbf{f}^{n}\right), \\
B \mathbf{u}^{n+1}=\mathbf{0}.
\end{cases}
\end{align*}

The second equation is impossible to be satisfied, so this is not a functioning numerical method.


\subsubsection*{Semi-implicit discretization}

The semi-implicit discretization using (FE) and $(\mathrm{BE})$ is in the same form as the fully explicit one, except for the first equation, which is instead:
\begin{equation*}
\int_{\Omega} \frac{\mathbf{u}_{h}^{n+1}-\mathbf{u}_{h}^{n}}{\Delta t} \mathbf{v}_{h}+a\left(\mathbf{u}_{h}^{n}, \mathbf{v}_{h}\right)+b\left(\mathbf{v}_{h}, p_{h}^{n+1}\right)=\mathbf{F}^{n+1}\left(\mathbf{v}_{h}\right).
\end{equation*}
The problem can be put in the form:
\begin{equation*}
\begin{bmatrix}
\frac{M}{\Delta T} & B^{T} \\
B & 0 
\end{bmatrix}
\begin{bmatrix}
\mathbf{u}^{n+1} \\
\mathbf{p}^{n+1}
\end{bmatrix}
=
\begin{bmatrix} H (\mathbf{u}^{n},\mathbf{f}^{n+1}) \\ 0 \end{bmatrix}
\end{equation*}
The stability condition is the following one:
$$\Delta T \leq \frac{Ch^2}{\nu}.$$
\subsubsection*{Fully implicit discretization}

The fully implicit discretization using (BE) is in the same form as the semi-implicit one, except for the first equation, which is instead:
\begin{equation*}
\int_{\Omega} \frac{\mathbf{u}_{h}^{n+1}-\mathbf{u}_{h}^{n}}{\Delta t} \mathbf{v}_{h}+a\left(\mathbf{u}_{h}^{n+1}, \mathbf{v}_{h}\right)+b\left(\mathbf{v}_{h}, p_{h}^{n+1}\right)=\mathbf{F}^{n+1}\left(\mathbf{v}_{h}\right).
\end{equation*}
\begin{equation*}
\underbrace{\begin{bmatrix}
\frac{1}{\Delta t}(M+A) & B^{\top} \\
B & 0
\end{bmatrix}} _{S} \underbrace{\begin{bmatrix}
\mathbf{u}^{n+1} \\
\mathbf{p}^{n+1}
\end{bmatrix}}_{\mathbf{w}}
=
\begin{bmatrix} H (\mathbf{u}^{n},\mathbf{f}^{n+1}) \\ 0 \end{bmatrix}
\end{equation*}
This method is unconditionally stable


\section{Generalization of the Stokes problem to N-S}

In this section we will revise the concept already presented in section 4.13, starting from the generalization of the Stokes problem. Some of the information presented will be redundant. There is also an inconsistent notation since here, fully/semi or implicit/explicit refer just to the nonlinear $c$ term.  In particular in this section the approach followed in class will be presented, while section 4.12 followed the approach presented in the reference textbook.\\ \\
The terms in the semi-discretized formulation of (NS) in common with that of (Time-GS) are discretized in time using fully implicit discretization, while the remaining term $c(\mathbf{u}_{h}, \mathbf{u}_{h}, \mathbf{v}_{h})$ can be discretized in multiple ways.
We start from the linear system arising from the fully implicit Stokes discretization, $S\mathbf{w}=\mathbf{G}$ which has the advantage of not having any stability restrictions. The cost would be similar to the one starting from the semi-implicit discretization, so this last one is preferred.

\subsubsection*{Fully Explicit Discretization}


The discretized term $c$ using Forward Euler can be put in the form $N\left(\mathbf{u}^{n}\right) \mathbf{u}^{n}$. The problem is still in the form $S \mathbf{w}=\mathbf{\tilde{G}}$, with $\mathbf{\tilde{G}}:=\mathbf{G}-N\left(\mathbf{u}^{n}\right) \mathbf{u}^{n}$.
This method has the following stability condition:
\begin{equation}
\Delta t \leq C \frac{h}{\max _{\mathbf{x} \in \Omega}\left|\mathbf{u}^{n}(\mathbf{x})\right|}
\end{equation}

\subsubsection*{Semi-implicit discretization}

The term $c$ can be discretized using (FE) and (BE) and put in two different forms, $c\left(\mathbf{u}_{h}^{n+1}, \mathbf{u}_{h}^{n}, \mathbf{v}_{h}\right)$ and $c\left(\mathbf{u}_{h}^{n}, \mathbf{u}_{h}^{n+1}, \mathbf{v}_{h}\right)$, resulting in two different matrices and linear systems:
\begin{equation*}
\begin{bmatrix}
    \frac{1}{\Delta t} (M+A) + C_{1/2}   & B^{T} \\
    B & 0 \end{bmatrix} \begin{bmatrix}
\mathbf{u}^{n+1} \\
\mathbf{p}^{n+1}
\end{bmatrix}
=
\begin{bmatrix} H (\mathbf{u}^{n},\mathbf{f}^{n+1}) \\ 0 \end{bmatrix}
\end{equation*}

The only difference from the original algebraic version of the Stokes system is the fact that the resulting matrix is non symmetric.
The stability conditions are the following:
\begin{itemize}
\item $\Delta t_n \leq  Kh \   \text{in the case of }C_1, \ c\left(\mathbf{u}_{h}^{n+1}, \mathbf{u}_{h}^{n}, \mathbf{v}_{h}\right).$
\item$\Delta t_n \leq  \frac{h}{\max _{\mathbf{x} \in \Omega}\left|\mathbf{u}^{n}(\mathbf{x})\right|}  \  \text{in the case of }C_2, \ c\left(\mathbf{u}_{h}^{n}, \mathbf{u}_{h}^{n+1}, \mathbf{v}_{h}\right)$.
\end{itemize}



\subsubsection*{Fully implicit discretization}

The discretized term $c$ using (BE) can be put in the form $N\left(\mathbf{u}^{n+1}\right) \mathbf{u}^{n+1}$, which is a nonlinear term. The problem is in the form: 
$$
S \mathbf{w}+\left(\begin{array}{c}N\left(\mathbf{u}^{n+1}\right) \mathbf{u}^{n+1} \\ \mathbf{0}\end{array}\right)=\mathbf{G}$$
which is a nonlinear algebraic system.
This method is unconditionally stable.

\subsection*{Error analysis of fully discretized problems}

If Taylor-Hood elements with degree $k$ and a time discretization method of order $q$ are used, we have the following error estimate:
\begin{flalign*}
\forall t > 0, \| \mathbf{u}(t) - \mathbf{u}_{h}(t) \|_{H^{1}(\Omega)} + \| p(t) - p_{h}(t) \|_{L^{2}(\Omega)} \leq C \left( \Delta t^{q} + h^{k} \right)  \\
\left( | \mathbf{u}(t) |_{H^{k+1}(\Omega)} + | p(t) |_{H^{k}(\Omega)} + K \left( \partial_{t} \mathbf{u}(t), \partial_{t} p(t) \right) \right)
\end{flalign*}

for a certain $C \in \mathbb{R}$, where the term $K\left(\partial_{t} \mathbf{u}(t), \partial_{t} p(t)\right).$ represents a measure of the time derivatives of the exact solutions at time $t$.

These prove, in particular, that the gradient of the discrete solution (as well as that of the weak solution $u$) could be as large as $\mu_{0}$ is small.

Moreover, the Galerkin error inequality gives:


\begin{equation}
\left\|u-u_{h}\right\|_{V} \leq \frac{M}{\alpha} \inf _{v_{h} \in V_{h}}\left\|u-v_{h}\right\|_{V}
\end{equation}


By the definitions of $\alpha$ and $M$, the upper-bounding constant $M / \alpha$ becomes as large (and, correspondingly, the the previous
estimate meaningless) as the ratio $\|\mathbf{b}\|_{L^{\infty}(\Omega)} /\|\mu\|_{L^{\infty}(\Omega)}$ (resp. the ratio $\|\sigma\|_{L^{2}(\Omega)} /\|\mu\|_{L^{\infty}(\Omega)}$) grows, which happens when the convective (resp. reactive) term dominates over diffusive one.

In such cases the Galerkin method can give inaccurate solutions, unless - as we will see - an extremely small discretization step $h$ is used.


\chapter{Weak Formulation for the generic Elliptic BVP}

% \section{Conservative Weak Formulation Analyis}
% Let $\Omega \subset \mathbb{R}^{2}$ be an open set having a regular enough boundary $\partial \Omega=\Gamma_{D} \cup \Gamma_{N}$ with $\stackrel{\circ}{\Gamma}_{D} \cap \stackrel{\circ}{\Gamma}_{N}=\emptyset$. Let us consider the following advection-diffusion problem: find $u: \Omega \rightarrow \mathbb{R}$ such that:

% \begin{align}
% \begin{cases}
% -\nabla \cdot(\mu \nabla u) + \nabla \cdot(\mathbf{b} u) + \sigma u = f & \text{ in } \Omega  \\
% u = g_D & \text{ on } \Gamma_D \\
% (\mu \nabla u - \mathbf{b} u) \cdot \mathbf{n}  + \gamma u= g_N & \text{ on } \Gamma_N 
% \end{cases}
% \end{align}


% where $\mathbf{b}: \Omega \rightarrow \mathbb{R}^{2}$ and $\mu: \Omega \rightarrow \mathbb{R}$ are two continuous functions, $f: \Omega \rightarrow \mathbb{R}$ belongs to $L^{2}(\Omega)$ and $g_{D}: \Gamma_{D} \rightarrow \mathbb{R}, g_{N}: \Gamma_{R} \rightarrow \mathbb{R}$ are regular enough functions (Specify regularity please, like later and book). Moreover, let us suppose

% $$
% 0<\mu_{0} \leq \mu(\mathbf{x}) \leq \mu_{1}, \quad|\mathbf{b}(\mathbf{x})| \leq b_{1}, \quad \text { a.e. in } \Omega \text {. } \quad \sigma(\mathbf{x}) \in L^2(\Omega)
% $$
% \begin{enumerate}
% \item Write the weak formulation of problem (1) in the general form


% \begin{equation}
% \text { find } \quad u \in V \quad \text { such that } a(u, v)=F(v), \quad \forall v \in V \text {. } 
% \end{equation}


% In particular, define properly the functional space $V$ and the forms $a(\cdot, \cdot)$ and $F(\cdot)$.

% \item Define conditions on the coefficients under which problem (5.2) admits a unique solution.

% \item Let $V_{h}$ be a suitable finite dimensional subspace of $V$. Write the Galerkin formulation of problem (2).

% \item Analyse the existence and uniqueness, stability and convergence of the solution.
% \item Let now $V_{h}$, defined at point 3 , be the space of linear finite elements. Show that the Galerkin formulation is equivalent to the solution of the linear system $A \mathbf{u}=\mathbf{f}$ with dimension $n$. Define precisely the value of $n$ and give an explicit representation of the matrix $A$ and the vectors $\mathbf{u}$ and $\mathbf{f}$.

% \end{enumerate}

% \textbf{Solution}

% \begin{enumerate}

% \item  Let us multiply equation (5.1) by a regular enough test function $v$, and integrate over $\Omega$ :

% $$
% -\int_{\Omega} \nabla \cdot(\mu \nabla u) v d \Omega+\int_{\Omega} \nabla \cdot(\mathbf{b} u) v d \Omega  \int_{\Omega} \sigma u v d \Omega  + \int_{\Gamma_N} \gamma u v \, d\Gamma \nonumber =\int_{\Omega} f v d \Omega
% $$

% Integrating by part the two terms on the left-hand side, we find:


% \begin{align}
% \int_{\Omega} \mu \nabla u \cdot \nabla v \, d \Omega & - \int_{\Omega} \mathbf{b} \cdot \nabla v u \, d \Omega 
% + \int_{\Omega} \sigma u v \, d \Omega  + \int_{\Gamma_N} \gamma u v \, d\Gamma \nonumber \\
% &= \int_{\Omega} f v \, d \Omega  + \int_{\Gamma_{R}} g_{N} v \, d \Gamma 
% + \int_{\Gamma_{D}} (\mu \nabla u - \mathbf{b} u) \cdot \mathbf{n} v \, d \Gamma 
% \end{align}




% Since we will introduce a continuous lifting operator, we choose the test function $v$ such that $\left.v\right|_{\Gamma_{D}}=0$. Then, equation (5.3) simplifies in:


% \begin{equation}
% \int_{\Omega} \mu \nabla u \cdot \nabla v d \Omega-\int_{\Omega} \mathbf{b} \cdot \nabla v u d \Omega +  \int_{\Omega}  \sigma u v d \Omega + \int_{\Gamma_N}  \gamma uv d\Gamma  =\int_{\Omega} f v d \Omega+\int_{\Gamma_{N}} g_{N} v d \Gamma 
% \end{equation}


% Let us introduce the lifting operator $R_{g_{D}}$ such that $R_{g_{D}} \in H^{1}(\Omega)$ and $\left.R_{g_{D}}\right|_{\Gamma_{D}}=g_{D}$. We define a new unknown function $\stackrel{\circ}{u}=u-R_{g_{D}}$ that, by definition, will belong to $H_{\Gamma_{D}}^{1}(\Omega)$. By replacing $u$ in (5.4) with $\stackrel{\circ}{u}+R_{g_{D}}$, we get:


% \begin{flalign}
%     \int_{\Omega} \mu \nabla \stackrel{\circ}{u} \cdot \nabla v \, d\Omega 
%      - \int_{\Omega} \mathbf{b} \cdot \nabla v \stackrel{\circ}{u} \, d\Omega + \int_{\Omega}  \sigma \tilde{u} v d \Omega + \int_{\Gamma_N}  \gamma \tilde{u}v d\Gamma
%      & =     \\ \int_{\Omega} f v \, d\Omega + \int_{\Gamma_N} g_N v \, d\Gamma_N
%     - \int_{\Omega} \mu \nabla R_{g_D} \cdot \nabla v \, d\Omega 
%      + \int_{\Omega} \mathbf{b} \cdot \nabla v R_{g_D} \, d\Omega 
%      - \int_{\Omega} \sigma R_{g_D} v \, d\Omega && \nonumber
% \end{flalign}


% The weak formulation of (5.1) can be written in the general form (5.2):


% \begin{equation}
% \text { find } \quad \stackrel{\circ}{u} \in V \quad \text { such that } a(\stackrel{\circ}{u}, v)=F(v) \quad \forall v \in V \text {, } 
% \end{equation}


% where $V=H_{\Gamma_{D}}^{1}(\Omega)$, the bilinear form $a(\cdot, \cdot): V \times V \rightarrow \mathbb{R}$ and the linear functional $F(\cdot): V \rightarrow \mathbb{R}$, are defined as:
% $$
% \begin{array}{ll}
% a(w, v) \equiv \int_{\Omega} \mu \nabla w \cdot \nabla v d \Omega-\int_{\Omega} \mathbf{b} \cdot \nabla v w d \Omega + \int_{\Omega} \sigma u v d \Omega + \int_{\Gamma_N} \gamma wv d\Gamma  & \forall w, v \in V, \\ \\
% F(v) \equiv \int_{\Omega} f v d \Omega+\int_{\Gamma_{N}} g_{N} v d \Gamma-a\left(R_{g_{D}}, v\right) & \forall v \in V .
% \end{array}
% $$

% In the following we will denote by $\|\cdot\|_{V}$ the norm $\|\cdot\|_{H^{1}(\Omega)}$. The weak solution $u$ can be obtained a posteriori by using the relation $u=\stackrel{\circ}{u}+R_{g_{D}}$.

% \item To prove the existence and uniqueness of the solution of (6) we verify the hypothesis of the Lax-Milgram Lemma. Notice that $V$ is a Hilbert functional space. We next show the coercivity of the bilinear form $a(\cdot, \cdot)$. For every test function $w \in V$, we have

% $$
% \int_{\Omega} \mathbf{b} \cdot \nabla w w d \Omega=\frac{1}{2} \int_{\Omega} \mathbf{b} \cdot \nabla w^{2} d \Omega=\frac{1}{2} \int_{\Gamma_{R}} \mathbf{b} \cdot \mathbf{n} w^{2} d \Gamma-\frac{1}{2} \int_{\Omega} w^{2} \nabla \cdot \mathbf{b} d \Omega
% $$
% where we have integrated by part the convective term. Then,

% $$
% a(w, w)=\int_{\Omega} \mu|\nabla w|^{2} d \Omega-\frac{1}{2} \int_{\Gamma_{R}} \mathbf{b} \cdot \mathbf{n} w^{2} d \Gamma+ \int_{\Omega} w^{2} \Bigl( \frac{1}{2} \nabla \cdot \mathbf{b} + \sigma \Bigl) d \Omega.
% $$

% If $ \gamma - \frac{1}{2}\mathbf{b} \cdot \mathbf{n} \geq 0$ a.e. on $\Gamma_{N}$ and $ \frac{1}{2}\nabla \cdot \mathbf{b} + \sigma \geq 0$ a.e. in $\Omega$, we can write:

% $$
% a(w, w) \geq \mu_{0}\|\nabla w\|_{L^{2}(\Omega)}^{2}
% $$

% using that $\mu(\mathbf{x}) \geq \mu_{0}>0$. Applying now Poincaré inequality (since $w$ is null on $\Gamma_{D}$ ), we get

% $$
% \|\nabla w\|_{L^{2}(\Omega)}^{2} \geq \frac{1}{1+C_{\Omega}^{2}}\|w\|_{V}^{2}
% $$

% where $C_{\Omega}$ is the Poincaré constant. In other words we find that $a(\cdot, \cdot)$ is coercive, with coercivity constant $\alpha=\mu_{0} /\left(1+C_{\Omega}^{2}\right)$.\\ 
% \\
% Next, we show that $a(\cdot, \cdot)$ is continuous. We have that

% $$
% \begin{aligned}
% |a(w, v)| & \leq\left|\int_{\Omega} \mu \nabla w \cdot \nabla v d \Omega\right|+\left|\int_{\Omega} \mathbf{b} \cdot \nabla v w d \Omega\right| + \left|\int_{\Omega} \sigma uv\right|  + \int_{\Gamma_N} \gamma u v \, d\Gamma \\
% & \leq \mu_{1}\|\nabla w\|_{L^{2}(\Omega)}\|\nabla v\|_{L^{2}(\Omega)}+b_{1}\|\nabla v\|_{L^{2}(\Omega)}\|w\|_{L^{2}(\Omega)} + C^2 \|\sigma\|_{L^2(\Omega)} \|u\|_{L^2(\Omega)}\|v\|_{L^2(\Omega)} +\\ & + 
% |\gamma| C'^2 \|w\|_{V} \|v\|_V  \leq M\|w\|_{V}\|v\|_{V}
% \end{aligned}
% $$

% with continuity constant $M$, $C$ being the inclusion constant, and $C'$ the constant deriving from the trace inequality.\\ 

% Finally, to prove that the linear functional $F$ is continuous we use Cauchy-Schwarz inequality, the continuity of the trace operator on $\Gamma_{N}$ and the continuity of the bilinear form $a(\cdot, \cdot)$ shown before.

% $$
% \begin{aligned}
% |F(v)| & \leq\|f\|_{L^{2}(\Omega)}\|v\|_{L^{2}(\Omega)}+\left\|g_{N}\right\|_{L^{2}\left(\Gamma_{N}\right)}\|v\|_{L^{2}\left(\Gamma_{N}\right)}+M\left\|R_{g_{D}}\right\|_{V}\|v\|_{V} \\
% & \leq\left(\|f\|_{L^{2}(\Omega)}+C'\left\|g_{N}\right\|_{L^{2}\left(\Gamma_{N}\right)}+M\left\|R_{g_{D}}\right\|_{V}\right)\|v\|_{V} = K \|v\|_V .
% \end{aligned}
% $$

% Then, we can state that the solution of (5.6) exists and is unique under the hypothesis that  $ \gamma - \frac{1}{2}\mathbf{b} \cdot \mathbf{n} \geq 0$ a.e. on $\Gamma_{N}$ and $ \frac{1}{2}\nabla \cdot \mathbf{b} + \sigma \geq 0$ a.e. in $\Omega$.


%  By replicating the proof carried out above for the finite element formulation, the following estimates can be proved:

% $$
% \left\|u_{h}\right\|_{V_h} \leq \frac{1}{\alpha}K, \quad\left\|\nabla u_{h}\right\|_{\mathrm{L}^{2}(\Omega)} \leq \frac{1+C_{\Omega}^2}{\mu_{0}}K
% $$

% \item The Galerkin formulation of problem (5.2) is given by


% \begin{equation}
% \text { find } \quad \stackrel{\circ}{u}_{h} \in V_{h} \text { such that } a\left(\stackrel{\circ}{u}_{h}, v_{h}\right)=F\left(v_{h}\right) \quad \forall v_{h} \in V_{h} \text {, }
% \end{equation}


% where $V_{h}$ is a finite dimensional subspace of $V$ having dimension $N_{h}$.

% \item For the analysis of existence, uniqueness, stability and convergence of the Galerkin formulation we refer to the paragraph 4.2 of the Textbook.

% \item Denoting by $\left\{\phi_{i}, i=1, \ldots, N_{h}\right\}$ a basis for $V_{h}$, it is sufficient to show that (5.7) is verified by every $\phi_{i}$, that is


% \begin{equation}
% a\left(\stackrel{\circ}{u}_{h}, \phi_{i}\right)=F\left(\phi_{i}\right) \quad \text { for } \quad i=1, \ldots, N_{h} . 
% \end{equation}


% On the other side, since $\stackrel{\circ}{u}_{h} \in V_{h}$, we can express $\stackrel{\circ}{u}_{h}$ as a linear combination of basis functions

% $$
% \stackrel{\circ}{u}_{h}(\mathbf{x})=\sum_{j=1}^{N_{h}} u_{j} \phi_{j}(\mathbf{x})
% $$
% being the coefficients $u_{j}$ our unknowns. Substituting the above expression in (8) we find that

% $$
% \sum_{j=1}^{N_{h}} u_{j} a\left(\phi_{j}, \phi_{i}\right)=F\left(\phi_{i}\right) \quad \text { for } \quad i=1, \ldots, N_{h}
% $$

% or equivalently:

% \begin{equation}
% \underbrace{
%     \begin{bmatrix}
%         a(\phi_{1}, \phi_{1}) & a(\phi_{2}, \phi_{1}) & \cdots & a(\phi_{N_{h}}, \phi_{1}) \\
%         a(\phi_{1}, \phi_{2}) & a(\phi_{2}, \phi_{2}) & \cdots & a(\phi_{N_{h}}, \phi_{2}) \\
%         \vdots & \vdots & \ddots & \vdots \\
%         a(\phi_{1}, \phi_{N_{h}}) & a(\phi_{2}, \phi_{N_{h}}) & \cdots & a(\phi_{N_{h}}, \phi_{N_{h}})
%     \end{bmatrix}
% }_{A}
% \underbrace{
%     \begin{bmatrix}
%         u_{1} \\
%         u_{2} \\
%         \vdots \\
%         u_{N_{h}}
%     \end{bmatrix}
% }_{\mathbf{u}} = 
% \underbrace{
%     \begin{bmatrix}
%         f_1 \\
%         f_2 \\
%         \vdots \\
%         f_{N_{h}}
%     \end{bmatrix}
% }_{\mathbf{f}}
% \end{equation}


% Denoting by $V_{h}$ the space $X_{h}^{1}=\left\{v_{h} \in C^{0}(\bar{\Omega}):\left.v_{h}\right|_{K} \in \mathbb{P}^{1}, \forall K \in \mathcal{T}_{h}\right.$, con $\left.\left.v_{h}\right|_{\Gamma_{D}}=0\right\}$ of linear finite elements, being $\mathcal{T}_{h}$ the assigned computational mesh of $\Omega$, it follows that the dimension of linear system (9) is given by the total number of nodes $\mathrm{N}_{\mathrm{V}}$ minus the number $\mathrm{N}_{\mathrm{ND}}$ of boundary (Dirichlet) nodes, i.e., $N_{h}=\mathrm{N}_{\mathrm{V}}-\mathrm{N}_{\mathrm{ND}}$.

% \end{enumerate}

% \section{Non Conservative Weak Formulation Analyis}
% In this chapter we consider problems of the following form:

% \begin{equation}
% \begin{cases}
% -\nabla \cdot (\mu \nabla u) + \mathbf{b} \cdot \nabla u + \sigma u = f & \text{in } \Omega, \\
% u = g_D & \text{on } \Gamma_D, \\
% \mu \nabla u \cdot \mathbf{n} + \gamma u = g_N & \text{on } \Gamma_N,
% \end{cases}
% \end{equation}
    

% where $\mu, \sigma, f$ and $\mathbf{b}$ are given functions (or constants). In the most general case, we will suppose that $\mu \in \mathrm{L}^{\infty}(\Omega)$ with $\mu(\mathbf{x}) \geq \mu_{0}>0, \sigma \in \mathrm{L}^{2}(\Omega)$ with $\sigma(\mathbf{x}) \geq 0$ a.e. in $\Omega$ (not strictly necessary the non-negativity for the following derivations), $\mathbf{b} \in\left[\mathrm{L}^{\infty}(\Omega)\right]^{2}$ with $\operatorname{div}(\mathbf{b}) \in \mathrm{L}^{2}(\Omega)$, and $f \in \mathrm{L}^{2}(\Omega)$. Moreover $\gamma $ is a constant and $\mathbf{b}\cdot \mathbf{n} \in L^2(\Gamma_N)$

% In many practical applications, the diffusion term $-\operatorname{div}(\mu \nabla u)$ is dominated by the convection term $\mathbf{b} \cdot \nabla u$ (also called transport term) or by the reaction term $\sigma u$ (also called the absorption term when $\sigma$ is non-negative). In such cases, as we will see, the solution can give rise to boundary layers, that is regions, generally close to the boundary of $\Omega$, where the solution is characterized by strong gradients.



% \subsection*{Weak problem formulation}

% Let $V:0=\mathrm{H}_{0}^{1}(\Omega)$ and $V = \left\{ v \in H^1(\Omega) : v|_{\Gamma_D} = g_d \right\}
% $. By introducing the bilinear form $a: V \times V \mapsto \mathbb{R}$,


% \begin{equation}
% a(u, v)=\int_{\Omega} \mu \nabla u \cdot \nabla v d \Omega+\int_{\Omega} v \mathbf{b} \cdot \nabla u d \Omega+\int_{\Omega} \sigma u v d \Omega + \int_{\Gamma_N} \gamma u v d\Gamma \quad \forall u \in V, v \in V_0
% \end{equation}

% Introduce the lifting function \( R_{g_D} \) such that \( R_{g_D} \in H^1(\Omega) \) and \( R_{g_D} = g_D \) on \( \Gamma_D \). $R_{gD} $ is the extension of $g_D$ on $\Omega$, and in order to be in $V = H^1(\Omega)$ we required $g_D$ to be in $H^{1/2}(\Omega)$. The new unknown function is \( \tilde{u} = u - R_{g_D} \), which satisfies \( \tilde{u} = 0 \) on \( \Gamma_D \).


% The weak form simplifies to:

% \[
% \int_{\Omega} \mu \nabla \tilde{u} \cdot \nabla v \, d\Omega + \int_{\Omega} \mathbf{b} \cdot \nabla \tilde{u} v \, d\Omega + \int_{\Omega} \sigma \tilde{u} v \, d\Omega \int_{\Gamma_N} + \gamma \tilde{u} v d\Gamma = \int_{\Omega} fv \, d\Omega + \int_{\Gamma_N} g_N v \, d\Gamma \ - a(R_{g_D},v)
% \]

% where:
% \begin{itemize}

% \item \( V = H_{\Gamma_D}^1(\Omega) \) as the space of test functions.
% \item The bilinear form \( a(u, v) \) and the linear form \( F(v) \) are defined by:
%   \[
%   a(\tilde{u}, v) = \int_{\Omega} \mu \nabla \tilde{u} \cdot \nabla v \, d\Omega + \int_{\Omega} \mathbf{b} \cdot \nabla \tilde{u} v \, d\Omega + \int_{\Omega} \sigma \tilde{u} v \, d\Omega,
%   \]
%   \[
%   F(v) = \int_{\Omega} fv \, d\Omega + \int_{\Gamma_N} g_N v \, d\Gamma \ - a(R_{g_D},v).
%   \]

%   \end{itemize}

% The weak formulaton becomes :

% Find \( \tilde{u} \in V \) such that:
% \[
% a(\tilde{u}, v) = F(v) \quad \forall v \in V.
% \]

% The actual solution \( u \) can be recovered by \( u = \tilde{u} + R_{g_D} \).


% In order to prove the existence and uniqueness of the solution of (5.12) we will put ourselves in the condition to apply the Lax-Milgram lemma.

% To verify the coercivity of the bilinear form $a(\cdot, \cdot)$, we proceed separately on the single terms composing (5.11).

% For the first term we have:
% \begin{equation}
% \int_{\Omega} \mu \nabla v \cdot \nabla v d \Omega \geq \mu_{0}\|\nabla v\|_{\mathrm{L}^{2}(\Omega)}^{2} 
% \end{equation}


% As $v \in \mathrm{H}_{0}^{1}(\Omega)$, the Poincaré inequality holds (see (2.13 NMDP)); then

% $$
% \|v\|_{\mathrm{H}^{1}(\Omega)}^{2}=\|v\|_{\mathrm{L}^{2}(\Omega)}^{2}+\|\nabla v\|_{\mathrm{L}^{2}(\Omega)}^{2} \leq\left(1+C_{\Omega}^{2}\right)\|\nabla v\|_{\mathrm{L}^{2}(\Omega)}^{2}
% $$

% and therefore it follows from (5.13) that

% $$
% \int_{\Omega} \mu \nabla v \cdot \nabla v d \Omega \geq \frac{\mu_{0}}{1+C_{\Omega}^{2}}\|v\|_{\mathrm{H}^{1}(\Omega)}^{2}
% $$

% We now move to the convective term. Using Green's formula (3.16) yields

% $$
% \begin{aligned}
% \int_{\Omega} v \mathbf{b} \cdot \nabla v d \Omega & =\frac{1}{2} \int_{\Omega} \mathbf{b} \cdot \nabla\left(v^{2}\right) d \Omega=-\frac{1}{2} \int_{\Omega} v^{2} \operatorname{div}(\mathbf{b}) d \Omega+\frac{1}{2} \int_{\partial \Omega} \mathbf{b} \cdot \mathbf{n} v^{2} d \gamma \\
% & =-\frac{1}{2} \int_{\Gamma_N} v^{2} \operatorname{div}(\mathbf{b}) d \Omega
% \end{aligned}
% $$

% as $v=0$ on $\Gamma_D$, whence

% $$
% \int_{\Omega} v \mathbf{b} \cdot \nabla v d \Omega+\int_{\Omega} \sigma v^{2} d \Omega=\int_{\Omega} v^{2}\left(-\frac{1}{2} \operatorname{div}(\mathbf{b})+\sigma\right) d \Omega
% $$

% The last integral is certainly positive if we suppose that:


% \begin{equation}
% -\frac{1}{2} \operatorname{div}(\mathbf{b})+\sigma \geq 0 \quad \text { a.e. in } \Omega 
% \end{equation}

% Then we have another term:
% $$\int_{\Gamma_N} \gamma + \frac{1}{2} \mathbf{b}\cdot \mathbf{n} d\Gamma$$ which is non negative if:
% $\gamma +\frac{1}{2}\mathbf{b}\cdot \mathbf{n} \geq 0 \ a.e. \ \text{in } \Gamma_N.$ \\ \\


% Consequently, the bilinear form $a(\cdot, \cdot)$ is coercive, as


% \begin{equation}
% a(v, v) \geq \alpha\|v\|_{V}^{2} \quad \forall v \in V, \quad \text { with } \quad \alpha=\frac{\mu_{0}}{1+C_{\Omega}^{2}} 
% \end{equation}


% To prove that the bilinear form $a(\cdot, \cdot)$ is continuous, that is it satisfies (2.6 NMDP), we bound the first term on the right-hand side of (5.11) as follows:


% \begin{equation}
% \left|\int_{\Omega} \mu \nabla u \cdot \nabla v d \Omega\right|  \leq\|\mu\|_{\mathrm{L}^{\infty}(\Omega)}\|\nabla u\|_{\mathrm{L}^{2}(\Omega)}\|\nabla v\|_{\mathrm{L}^{2}(\Omega)}  \\
%  \leq\|\mu\|_{\mathrm{L}^{\infty}(\Omega)}\|u\|_{\mathrm{H}^{1}(\Omega)}\|v\|_{\mathrm{H}^{1}(\Omega)}
% \end{equation}


% We have used the Hölder and Cauchy-Schwarz inequalities (see Sect. 2.5), as well as the inequality $\|\nabla w\|_{\mathrm{L}^{2}(\Omega)} \leq\|w\|_{\mathrm{H}^{1}(\Omega)} \forall w \in H^{1}(\Omega)$. For the second term, proceeding in a similar way we find


% \begin{equation}
% \left|\int_{\Omega} v \mathbf{b} \cdot \nabla u d \Omega\right|  \leq\|\mathbf{b}\|_{\mathrm{L}^{\infty}(\Omega)}\|v\|_{\mathrm{L}^{2}(\Omega)}\|\nabla u\|_{\mathrm{L}^{2}(\Omega)}  \\
%  \leq\|\mathbf{b}\|_{\mathrm{L}^{\infty}(\Omega)}\|v\|_{V}\|u\|_{V}
% \end{equation}


% For the third term we have, thanks again to the Cauchy-Schwarz inequality,

% \begin{equation}
% \left|\int_{\Omega} \sigma u v d \Omega\right| \leq C^{2}\|\sigma\|_{\mathrm{L}^{2}(\Omega)}\|u v\|_{\mathrm{L}^{2}(\Omega)} \leq C^{2}\|\sigma\|_{\mathrm{L}^{2}(\Omega)}\|u\|_{V}\|v\|_{V} 
% \end{equation}
% Indeed, $\|u v\|_{\mathrm{L}^{2}(\Omega)} \leq\|u\|_{\mathrm{L}^{4}(\Omega)}\|v\|_{\mathrm{L}^{4}(\Omega)} \leq C^{2}\|u\|_{\mathrm{H}^{1}(\Omega)}\|v\|_{\mathrm{H}^{1}(\Omega)}$, having applied inequality (2.18 NMDP) and exploited inclusions (2.19), with $C$ being the inclusion constant. \\ \\

% Finally, applying Cauchy Scharz on the boundary and the trace inequality we get:
% $$\int_{\Gamma_N} \gamma u v \, d\Gamma \leq |\gamma| C'^2 \|u\|_{V} \|v\|_V $$




% Summing all the 4 terms we obtained, the continuity property (2.6  NMDP) follows by taking, e.g.,

% \begin{equation}
% M=\|\mu\|_{L^{\infty}(\Omega)}+\|\mathbf{b}\|_{L^{\infty}(\Omega)}+C^{2}\|\sigma\|_{L^{2}(\Omega)} + \gamma C'^2
% \end{equation}


% On the other hand, the right-hand side of (5.12) defines a bounded and linear functional thanks to the Cauchy-Schwarz inequality and to the Poincaré inequality (2.13 NMDP).  This must be proved, let's do it:\\
% $$
% \begin{aligned}
% |F(v)| & \leq\|f\|_{L^{2}(\Omega)}\|v\|_{L^{2}(\Omega)}+\left\|g_{N}\right\|_{L^{2}\left(\Gamma_{N}\right)}\|v\|_{L^{2}\left(\Gamma_{N}\right)}+M\left\|R_{g_{D}}\right\|_{V}\|v\|_{V} \\
% & \leq\left(\|f\|_{L^{2}(\Omega)}+C'\left\|g_{N}\right\|_{L^{2}\left(\Gamma_{N}\right)}+M\left\|R_{g_{D}}\right\|_{V}\right)\|v\|_{V} = K \|v\|_{V} .
% \end{aligned}
% $$

% Then, we can state that the solution of the weak formulation of the problem exists and is unique under the hypothesis that  $ \gamma - \frac{1}{2}\mathbf{b} \cdot \mathbf{n} \geq 0$ a.e. on $\Gamma_{N}$ and $- \frac{1}{2}\nabla \cdot \mathbf{b} + \sigma \geq 0$ a.e. in $\Omega$.
% The Galerkin approximation of the problem is:

% \begin{equation}
% \text { find } \tilde{u_{h}} \in V_{0h}: \quad a\left(u_{h}, v_{h}\right)=\left(f, v_{h}\right) \quad \forall v_{h} \in V_{0h} 
% \end{equation}


% where $\left\{V_{h}, h>0\right\}$ is a suitable family of subspaces of $\mathrm{H}_{0}^{1}(\Omega)$. By replicating the proof carried out above for the finite element formulation, the following estimates can be proved:

% $$
% \left\|u_{h}\right\|_{V_h} \leq \frac{1}{\alpha}K, \quad\left\|\nabla u_{h}\right\|_{\mathrm{L}^{2}(\Omega)} \leq \frac{1+C_{\Omega}^2}{\mu_{0}}K
% $$

% These prove, in particular, that the gradient of the discrete solution (as well as that of the weak solution $u$) could be as large as $\mu_{0}$ is small.

% Moreover, the Galerkin error inequality gives:


% \begin{equation}
% \left\|u-u_{h}\right\|_{V} \leq \frac{M}{\alpha} \inf _{v_{h} \in V_{h}}\left\|u-v_{h}\right\|_{V}
% \end{equation}


% By the definitions of $\alpha$ and $M$ (see (5.15) and (5.19)), the upper-bounding constant $M / \alpha$ becomes as large (and, correspondingly, the estimate (5.20) meaningless) as the ratio $\|\mathbf{b}\|_{L^{\infty}(\Omega)} /\|\mu\|_{L^{\infty}(\Omega)}$ (resp. the ratio $\|\sigma\|_{L^{2}(\Omega)} /\|\mu\|_{L^{\infty}(\Omega)}$ ) grows, which happens when the convective (resp. reactive) term dominates over diffusive one.

% In such cases the Galerkin method can give inaccurate solutions, unless - as we will see - an extremely small discretization step $h$ is used.

\section{Complete Case}
In this chapter we consider the following general formulation for an Elliptic Boundary value problem problems. By choosing some coefficient to be null, all the particular cases for the exam can be recovered.

\begin{equation}
\begin{cases}
-\nabla \cdot (\mu \nabla u) + \nabla \cdot (\mathbf{b}u)+\mathbf{c} \cdot \nabla u +  \sigma u = f & \text{in } \Omega, \\
u = g_D & \text{on } \Gamma_D, \\
(\mu \nabla u - \mathbf{b}u) \cdot \mathbf{n} + \gamma u = g_N & \text{on } \Gamma_N,
\end{cases}
\end{equation}
    

where $\mu, \sigma, f$, $\mathbf{b}$  and $\mathbf{c}$ are given functions or constants. In the most general case, we will suppose that $\mu \in \mathrm{L}^{\infty}(\Omega)$ with $\mu(\mathbf{x}) \geq \mu_{0}>0, \sigma \in \mathrm{L}^{\infty}(\Omega)$, $\mathbf{b}, \mathbf{c} \in\left[\mathrm{L}^{\infty}(\Omega)\right]^{2}$ with $\operatorname{div}(\mathbf{b}-\mathbf{c}) \in \mathrm{L}^{\infty}(\Omega)$, and $f \in \mathrm{L}^{2}(\Omega)$. Moreover $\gamma \in L^\infty(\partial \Omega)$ is a constant  and
$g_D \in H^{1/2}(\partial \Omega)$ and $g_N \in L^2(\partial \Omega)$,  in order to have the continuity of the functional on the right-hand side.






\subsection*{Weak problem formulation}

Let $V:0=\mathrm{H}_{0}^{1}(\Omega)$ and $V = \left\{ v \in H^1(\Omega) : v|_{\Gamma_D} = g_d \right\}
$. By introducing the bilinear form $a: V \times V \mapsto \mathbb{R}$:


\begin{equation}
a(u, v)=\int_{\Omega} \mu \nabla u \cdot \nabla v d \Omega-\int_{\Omega} u \mathbf{b} \cdot \nabla v d \Omega + \int_{\Omega} v\mathbf{c} \cdot \nabla u d\Omega  + \int_{\Omega} \sigma u v d \Omega + \int_{\Gamma_N} \gamma u v d\Gamma  
\end{equation}

Introduce the lifting function \( R_{g_D} \) such that \( R_{g_D} \in H^1(\Omega) \) and \( R_{g_D} = g_D \) on \( \Gamma_D \). The new unknown function is \( \tilde{u} = u - R_{g_D} \), which satisfies \( \tilde{u} = 0 \) on \( \Gamma_D \).


The weak form simplifies to:

\[
a(\tilde{u},v) = \int_{\Omega} fv \, d\Omega + \int_{\Gamma_N} g_N v \, d\Gamma \ - a(R_{g_D},v)
\]

where:
\begin{itemize}

\item \( V = H_{\Gamma_D}^1(\Omega) \) as the space of test functions.
\item The bilinear form \( a(u, v) \) and the linear form \( F(v) \) are defined by:
\[
a(\tilde{u}, v)=\int_{\Omega} \mu \nabla \tilde{u} \cdot \nabla v \, d \Omega - \int_{\Omega} \tilde{u} \mathbf{b} \cdot \nabla v \, d \Omega + \int_{\Omega} v\mathbf{c} \cdot \nabla \tilde{u} \, d\Omega + \int_{\Omega} \sigma \tilde{u} v \, d \Omega + \int_{\Gamma_N} \gamma \tilde{u} v \, d\Gamma
\]
  \[
  F(v) = \int_{\Omega} fv \, d\Omega + \int_{\Gamma_N} g_N v \, d\Gamma \ - a(R_{g_D},v).
  \]

  \end{itemize}

The weak formulaton becomes :

Find \( \tilde{u} \in V \) such that:
\[
a(\tilde{u}, v) = F(v) \quad \forall v \in V.
\]

The actual solution \( u \) can be recovered by \( u = \tilde{u} + R_{g_D} \).


In order to prove the existence and uniqueness of the solution of (5.12) we will put ourselves in the condition to apply the Lax-Milgram lemma.

To verify the coercivity of the bilinear form $a(\cdot, \cdot)$, we proceed separately on the single terms:

For the first term we have:
\begin{equation}
\int_{\Omega} \mu \nabla v \cdot \nabla v d \Omega \geq \mu_{0}\|\nabla v\|_{\mathrm{L}^{2}(\Omega)}^{2} 
\end{equation}


As $v \in \mathrm{H}_{0}^{1}(\Omega)$, the Poincaré inequality holds (see (2.13 NMDP)); then

$$
\|v\|_{\mathrm{H}^{1}(\Omega)}^{2}=\|v\|_{\mathrm{L}^{2}(\Omega)}^{2}+\|\nabla v\|_{\mathrm{L}^{2}(\Omega)}^{2} \leq\left(1+C_{\Omega}^{2}\right)\|\nabla v\|_{\mathrm{L}^{2}(\Omega)}^{2}
$$

and therefore it follows that

$$
\int_{\Omega} \mu \nabla v \cdot \nabla v d \Omega \geq \frac{\mu_{0}}{1+C_{\Omega}^{2}}\|v\|_{\mathrm{H}^{1}(\Omega)}^{2}
$$

We now move to the convective term. Using Green's formula (3.16 NMDE) yields

$$
\begin{aligned}
-\int_{\Omega} v (\mathbf{b}-\mathbf{c}) \cdot \nabla v d \Omega & =-\frac{1}{2} \int_{\Omega} (\mathbf{b}-\mathbf{c}) \cdot \nabla\left(v^{2}\right) d \Omega=\frac{1}{2} \int_{\Omega} v^{2} \nabla \cdot(\mathbf{b}-\mathbf{c}) d \Omega-\frac{1}{2} \int_{\Gamma_N} (\mathbf{b}-\mathbf{c}) \cdot \mathbf{n} v^{2} d \gamma \\
\end{aligned}
$$


Then we can conclude another two conditions for the coercitivity, taking into account also the term related to $\sigma$ and $\gamma$: 

\begin{equation}
\frac{1}{2} \nabla \cdot (\mathbf{b}-\mathbf{c})+\sigma \geq 0 \quad \text { a.e. in } \Omega,  \quad \gamma - \frac{1}{2} (\mathbf{b}-\mathbf{c})\cdot \mathbf{n} \geq 0 \quad \text{a.e. in } \Gamma_N
\end{equation}



Consequently, the bilinear form $a(\cdot, \cdot)$ is coercive, as


\begin{equation}
a(v, v) \geq \alpha\|v\|_{V}^{2} \quad \forall v \in V, \quad \text { with } \quad \alpha=\frac{\mu_{0}}{1+C_{\Omega}^{2}} 
\end{equation}


To prove that the bilinear form $a(\cdot, \cdot)$ is continuous, that is it satisfies (2.6 NMDP), we bound the first term on the right-hand side of (5.11) as follows:


\begin{equation}
\left|\int_{\Omega} \mu \nabla u \cdot \nabla v d \Omega\right|  \leq\|\mu\|_{\mathrm{L}^{\infty}(\Omega)}\|\nabla u\|_{\mathrm{L}^{2}(\Omega)}\|\nabla v\|_{\mathrm{L}^{2}(\Omega)}  \\
 \leq\|\mu\|_{\mathrm{L}^{\infty}(\Omega)}\|u\|_{\mathrm{V}}\|v\|_{\mathrm{V}}
\end{equation}


We have used the Hölder and Cauchy-Schwarz inequalities (see Sect. 2.5), as well as the inequality $\|\nabla w\|_{\mathrm{L}^{2}(\Omega)} \leq\|w\|_{\mathrm{V}(\Omega)} \forall w \in V$. For the second term, proceeding in a similar way we find


\begin{equation}
\left|\int_{\Omega} u \mathbf{b} \cdot \nabla v d \Omega\right|  \leq\|\mathbf{b}\|_{\mathrm{L}^{\infty}(\Omega)}\|u\|_{\mathrm{L}^{2}(\Omega)}\|\nabla v\|_{\mathrm{L}^{2}(\Omega)}  \\
 \leq\|\mathbf{b}\|_{\mathrm{L}^{\infty}(\Omega)}\|v\|_{V}\|u\|_{V}
\end{equation}

\begin{equation}
\left|\int_{\Omega} v \mathbf{c} \cdot \nabla u d \Omega\right|  \leq\|\mathbf{c}\|_{\mathrm{L}^{\infty}(\Omega)}\|v\|_{\mathrm{L}^{2}(\Omega)}\|\nabla u\|_{\mathrm{L}^{2}(\Omega)}  \\
 \leq\|\mathbf{c}\|_{\mathrm{L}^{\infty}(\Omega)}\|v\|_{V}\|u\|_{V}
\end{equation}

For the last  term we have, thanks again to the Cauchy-Schwarz inequality,

\begin{equation}
\left|\int_{\Omega} \sigma u v d \Omega\right| \leq C^{2}\|\sigma\|_{\mathrm{L}^{2}(\Omega)}\|u v\|_{\mathrm{L}^{2}(\Omega)} \leq C^{2}\|\sigma\|_{\mathrm{L}^{2}(\Omega)}\|u\|_{V}\|v\|_{V} 
\end{equation}
Indeed, $\|u v\|_{\mathrm{L}^{2}(\Omega)} \leq\|u\|_{\mathrm{L}^{4}(\Omega)}\|v\|_{\mathrm{L}^{4}(\Omega)} \leq C^{2}\|u\|_{\mathrm{H}^{1}(\Omega)}\|v\|_{\mathrm{H}^{1}(\Omega)}$, having applied inequality (2.18 NMDP) and exploited inclusions (2.19), with $C$ being the inclusion constant. \\ \\

Finally, applying Cauchy Scharz on the boundary and the trace inequality we get:
$$\int_{\Gamma_N} \gamma u v \, d\Gamma \leq |\gamma| C'^2 \|u\|_{V} \|v\|_V $$




Summing all the terms we obtained, the continuity property (2.6  NMDP) follows by taking, e.g.,

\begin{equation}
M=\|\mu\|_{L^{\infty}(\Omega)}+\|\mathbf{b}\|_{L^{\infty}(\Omega)}+\|\mathbf{c}\|_{L^{\infty}(\Omega)}+ C^{2}\|\sigma\|_{L^{2}(\Omega)} + \gamma C'^2
\end{equation}


On the other hand, the right-hand side of (5.12) defines a bounded and linear functional thanks to the Cauchy-Schwarz inequality and to the Poincaré inequality (2.13 NMDP).  This must be proved, let's do it:\\
We need $R_{gD}$, the extension of $g_D$ to the whole domain, to be in $H^1(\Omega)$, thus we require that $g_D \in H^{1/2}(\partial \Omega)$, and then we can apply the trace inequality:
$$
\begin{aligned}
|F(v)| & \leq\|f\|_{L^{2}(\Omega)}\|v\|_{L^{2}(\Omega)}+\left\|g_{N}\right\|_{L^{2}\left(\Gamma_{N}\right)}\|v\|_{L^{2}\left(\Gamma_{N}\right)}+M\left\|R_{g_{D}}\right\|_{V}\|v\|_{V} \\
&\leq \left(\|f\|_{L^{2}(\Omega)} + C' \|g_{N}\|_{L^{2}(\Gamma_{N})} + M' \|g_{D}\|_{H^{1/2}(\partial \Omega)}\right) \|v\|_{V} = K \|v\|_{V}.
\end{aligned}
$$


The Galerkin approximation of the problem is:

\begin{equation}
\text { find } \tilde{u_{h}} \in V_{0h}: \quad a\left(u_{h}, v_{h}\right)=\left(f, v_{h}\right) \quad \forall v_{h} \in V_{0h} 
\end{equation}


where $\left\{V_{h}, h>0\right\}$ is a suitable family of subspaces of $\mathrm{H}_{0}^{1}(\Omega)$. By replicating the proof carried out above for the finite element formulation, the following estimates can be proved:

$$
\left\|u_{h}\right\|_{V_h} \leq \frac{1}{\alpha}K, \quad\left\|\nabla u_{h}\right\|_{\mathrm{L}^{2}(\Omega)} \leq \frac{1+C_{\Omega}^2}{\mu_{0}}K
$$

These prove, in particular, that the gradient of the discrete solution (as well as that of the weak solution $u$) could be as large as $\mu_{0}$ is small.

Moreover, the Galerkin error inequality gives:


\begin{equation}
\left\|u-u_{h}\right\|_{V} \leq \frac{M}{\alpha} \inf _{v_{h} \in V_{h}}\left\|u-v_{h}\right\|_{V}
\end{equation}


By the definitions of $\alpha$ and $M$, the upper-bounding constant $M / \alpha$ becomes as large (and, correspondingly, the estimate (5.12) meaningless) as the ratio $\|\mathbf{b}\|_{L^{\infty}(\Omega)} /\|\mu\|_{L^{\infty}(\Omega)}$ (resp. the ratio $\|\sigma\|_{L^{2}(\Omega)} /\|\mu\|_{L^{\infty}(\Omega)}$ ) grows, which happens when the convective (resp. reactive) term dominates over diffusive one.

In such cases the Galerkin method can give inaccurate solutions, unless - as we will see - an extremely small discretization step $h$ is used.
\end{document}